<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorials | ramirodcrego</title>
    <link>//localhost:4321/teaching/</link>
      <atom:link href="//localhost:4321/teaching/index.xml" rel="self" type="application/rss+xml" />
    <description>Tutorials</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 22 Jul 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:4321/media/icon_hu261cb7f7f0462e5b62aedf74fb702d34_367518_512x512_fill_lanczos_center_3.png</url>
      <title>Tutorials</title>
      <link>//localhost:4321/teaching/</link>
    </image>
    
    <item>
      <title>Introduction to spatial ecology with R</title>
      <link>//localhost:4321/teaching/r/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:4321/teaching/r/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction to Google Earth Engine for Ecologists and Practitioners</title>
      <link>//localhost:4321/teaching/gee-2/</link>
      <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:4321/teaching/gee-2/</guid>
      <description>&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;
&lt;p&gt;Google Earth Engine is a cloud-based platform hosted by Google that uses
Google’s high-performance and intrinsically parallel computing system
that subdivides computations across the Google computational
infrastructure, accelerating computational processes (Gorelick et al.,
2017). It has been primarily designed to process and analyze satellite
imagery and geospatial datasets, making computationally demanding
workflows more accessible for research, education and other nonprofit
use at no user cost. Thus, it is totally free.&lt;/p&gt;
&lt;p&gt;Up to 2008, using satellite imagery for data analysis was expensive. In
2008, however, the United States Geological Survey (USGS) opted for a
free and open policy on the use of the most popular satellite, Landsat
(Woodcock et al., 2008). The release of Landsat catalog to be freely use
by the public opened enormously the possibilities to expand earth
research (Woulder et al., 2019). However, with so much data available,
the limitation became data processing rather than data gathering.
Personal computers are generally limited in computation power to analyze
the amount of data that a composite of Landsat images require and data
storage is also limited. Supercomputers can do that job, but access them
is only optional for researchers working in certain institutions. Google
opened the option for all users with internet access to operate on a
high computing infrastructure through the new platform, Google Earth
Engine.&lt;/p&gt;
&lt;p&gt;When the increase of satellite data accessibility is combined with free
access to fast computing processes, researchers and students have the
possibility to carry out extremely high computing demanding projects in
matter of minutes. For ecologists, this has opened the horizons of
research possibilities in unthinkable ways. And while lot of attention
has been put into the use of Google Earth Engine for remote sensing
scientists (Tamimina et al., 2020), the tool has not reached yet the
spatial ecological field in its full capacity. In this book, we explore
the different applications that this platform provides to analyze data
for projects focused on spatial ecology.&lt;/p&gt;
&lt;p&gt;Here I present a tutorial that will guide you on the use of different
model techniques for data analysis. The main goal is that after reading
and practicing with this tutorial, you will be able to analyze your data
using Google Earth Engine interface.&lt;/p&gt;
&lt;p&gt;The tutorial is organized in 3 main chapters. Chapter 1 is an
introduction to Google Earth Engine and the JavaScript language we use
to code. Chapter 2 will guide you on the use of images and geometries,
or raster and vector data. Chapter 3 is an introduction to image
classifications. I have a differnet tutorial for fitting Species
Distribution Models with GEE, that you can check out [here](link to SDM).&lt;/p&gt;
&lt;p&gt;Each chapter will guide you on the use of code and all that code is
available &lt;a href=&#34;https://code.earthengine.google.com/?accept_repo=users/ramirocrego84/IntroToGEE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I assume that the user of this tutorial has a minimal understanding on
remote sensing and geographic information system theory. For instance, I
assume that you are familiar with raster files, image bands, and the
different type of image resolutions. Similarly, I assume you know the
different type of vector data. While not knowing those terms will not
prevent you from using the book, it will definitely make it easier for
you to follow the more advance analyses.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., &amp;amp;
Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial
analysis for everyone. Remote Sensing of Environment, 202, 18–27.
&lt;a href=&#34;https://doi.org/10.1016/j.rse.2017.06.031&#34;&gt;https://doi.org/10.1016/j.rse.2017.06.031&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;C.E. Woodcock, R. Allen, M. Anderson, A. Belward, R. Bindschadler, W.
Cohen, F. Gao, S.N. Goward, D. Helder, E. Helmer, R. Nemani, L.
Oreopoulos, J. Schott, P.S. Thenkabail, E.F. Vermote, J. Vogelmann,
M.A. Wulder, R. Wynne. 2008. Free access to Landsat imagery. Science,
302 (5879), p. 1011&lt;/p&gt;
&lt;p&gt;Michael A. Wulder, Thomas R. Loveland, David P. Roy, Christopher J.
Crawford, Jeffrey G. Masek, Curtis E. Woodcock, Richard G. Allen, Martha
C. Anderson, Alan S. Belward, Warren B. Cohen, John Dwyer, Angela Erb,
Feng Gao, Patrick Griffiths, Dennis Helder, Txomin Hermosilla, James D.
Hipple, Patrick Hostert, M. Joseph Hughes, Justin Huntington, David M.
Johnson, Robert Kennedy, Ayse Kilic, Zhan Li, Leo Lymburner, Joel
McCorkel, Nima Pahlevan, Theodore A. Scambos, Crystal Schaaf, John R.
Schott, Yongwei Sheng, James Storey, Eric Vermote, James Vogelmann,
Joanne C. White, Randolph H. Wynne, Zhe Zhu. 2019. Current status of
Landsat program, science, and applications, Remote Sensing of
Environment, 225, 127-147, &lt;a href=&#34;https://doi.org/10.1016/j.rse.2019.02.015&#34;&gt;https://doi.org/10.1016/j.rse.2019.02.015&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Haifa Tamiminia, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush,
Sarina Adeli, Brian Brisco. 2020. Google Earth Engine for geo-big data
applications: A meta-analysis and systematic review. ISPRS Journal of
Photogrammetry and Remote Sensing, 164, 152-170,
&lt;a href=&#34;https://doi.org/10.1016/j.isprsjprs.2020.04.001&#34;&gt;https://doi.org/10.1016/j.isprsjprs.2020.04.001&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-brief-introduction-to-google-earth-engine&#34;&gt;A brief introduction to Google Earth Engine&lt;/h2&gt;
&lt;p&gt;Google Earth Engine (GEE) is an online Integrated Development
Environment (IDE) for rapid development and visualization of complex
spatial analyses using the JavaScript or Phyton coding languages,
allowing users to conduct data heavy analysis. While computer coding can
sound challenging for ecologists, researchers, students or managers that
have never used code before, it is rather simple and opens the spectrum
of data analysis enormously. The integrated platform that Google Earth
Engine provides allows not only traditional remote sensing scientists,
but also a much broader spectrum of users to run spatial analysis,
without the need to have deep expertise on the use of supercomputers.&lt;/p&gt;
&lt;p&gt;The GEE platform has the main advantage of providing access to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Petabytes of remote sensing imagery and other spatial products;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;High-speed parallel processing and machine learning algorithms;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A library of Application Programming Interfaces (APIs) with
development environments&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three features enable the user to visualize and analyze a bast
range of geospatial data fast and for free, without the need to access a
supercomputers (Gorelick et al., 2017).&lt;/p&gt;
&lt;p&gt;Google Earth Engine Application Programming Interface (API) can be run
in popular coding languages, such as JavaScript and Phyton. In this book
we will focus on the use of JavaScript. In this first chapter we will
explore the basics of Google Earth Engine, from understanding how to set
up an account, to understand the interface to explaining the basics of
JavaScript coding language.&lt;/p&gt;
&lt;p&gt;This video will run you through the materials of this chapter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction to GEE and JavaScript&lt;/strong&gt;:


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/e9QfJ0cIBcs?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;setting-up-a-google-earth-engine-account&#34;&gt;Setting up a Google Earth Engine account:&lt;/h3&gt;
&lt;p&gt;The first step to start using Google Earth Engine is to create an
account in this &lt;a href=&#34;https://signup.earthengine.google.com/#!/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once you have been accepted, you will receive an email with instruction
on how to complete the account setting process.&lt;/p&gt;
&lt;h3 id=&#34;the-google-earth-engine-interface&#34;&gt;The Google Earth Engine Interface&lt;/h3&gt;
&lt;p&gt;Once your Google Earth Engine account is set, you will be able to aces
the Code Editor to start working on it in your browser at
&lt;a href=&#34;https://code.earthengine.google.com/&#34;&gt;https://code.earthengine.google.com/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Google Earth Engine interface is an interactive web-based
environment for data analysis. It has four main panels (Fig. 1):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Code Editor panel&lt;/code&gt; to write JavaScript code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Left panel&lt;/code&gt; for managing and hosting programming scripts,
accessing Google Earth Engine documents and managing and uploading
external data sets such as shapefiles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Right panel&lt;/code&gt; with tabs for printing, queering maps and
exporting products or long-running tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Interactive map panel&lt;/code&gt; for visualizing maps and outputs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additionally, on top of the interface is the &lt;code&gt;search tab&lt;/code&gt;, that allows
you to search and find data sets and places. At the top right is a
&lt;code&gt;help menu&lt;/code&gt; that links to a user guide, help forum and other types of
support (Fig. 1).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure1.png&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 1. Google Earth Engine is a web-based environment that consists on four main panels.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 1. Google Earth Engine is a
web-based environment that consists on four main panels.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Each main panel has a variety of elements that will explore more in
detail (Fig. 2). Among this are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Docs tab&lt;/strong&gt; or documentation tab&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Scripts tab&lt;/strong&gt; or Git-based Script Manager&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Assets tab&lt;/strong&gt; or asset manager&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;search bar&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several buttons in the script panel to &lt;strong&gt;save&lt;/strong&gt;, &lt;strong&gt;run&lt;/strong&gt; and manage
scripts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Inspector tab&lt;/strong&gt; or interactive map query&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Console tab&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Tasks tab&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Geometry drawing tools&lt;/strong&gt; in the interactive map panel&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure2.png&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 2. Google Earth Engine web-based environment.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 2. Google Earth Engine web-based
environment.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Let’s explore the different panels and elements more in detail.&lt;/p&gt;
&lt;h4 id=&#34;the-code-editor-panel&#34;&gt;The code editor panel&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;code editor panel&lt;/strong&gt; is the area where you will write the
JavaScript code to perform all type of analysis. Above the code editor
panel are buttons to save your current script, run it, and clear the
script. The &lt;strong&gt;Get Link button&lt;/strong&gt; (Fig. 4) generates a unique URL for the
script in the address bar that allows you to share the script and/or
open it in a new browser tab. This is a unique characteristic of GEE
that allows you to share your work and make it reproducible by any other
user with a GEE account.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure3.png&#34; style=&#34;width:80.0%&#34;
alt=&#34;Figure 3. The “Get Link” button.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 3. The “Get Link”
button.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;The &lt;strong&gt;drop-down button&lt;/strong&gt; at the right of the &lt;strong&gt;Get Link button&lt;/strong&gt; has the
option to “Manage Links.” Clicking this button opens a new tab that
allows you to access, remove and download previously generated script
links. If you choose to download the script, this will be saved to your
personal computer as a zipped folder containing a .txt file with the
code.&lt;/p&gt;
&lt;p&gt;Next to the &lt;strong&gt;Get Link button&lt;/strong&gt; is the &lt;strong&gt;Save button&lt;/strong&gt;. This button
allows you to save the script as a new script and subsequent updates.&lt;/p&gt;
&lt;p&gt;Next is the &lt;strong&gt;Run button&lt;/strong&gt; that allows you to run the script. The
&lt;strong&gt;drop-down button&lt;/strong&gt; allows you to run the script with profiler (See
Profiler tab below).&lt;/p&gt;
&lt;p&gt;To the right, the &lt;strong&gt;Reset button&lt;/strong&gt; will reset all tabs on the right
panel. The &lt;strong&gt;drop-down button&lt;/strong&gt; will allow you to clear the script to
start a new one.&lt;/p&gt;
&lt;p&gt;Next is the &lt;strong&gt;Apps button&lt;/strong&gt;. This opens the App window display that we
will explore later on the book.&lt;/p&gt;
&lt;p&gt;Finally, there is a &lt;strong&gt;Settings button&lt;/strong&gt; that allows you to activate
options to underline code suggestions and to auto-complete pairs such as
quotation marks, parentheses and brackets. You can activate or
deactivate these two options depending on your coding preferences.&lt;/p&gt;
&lt;h4 id=&#34;the-left-pannel&#34;&gt;The left pannel&lt;/h4&gt;
&lt;h5 id=&#34;the-docs-tab--api-reference&#34;&gt;The Docs tab – API reference&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;Docs tab&lt;/strong&gt; (Fig. 4) contains the entire JavaScript API
documentation developed by Google. Here, you will find all the
information necessary to understand functions. We recommend you to get
familiar with this tab as it gets really useful for writing scripts and
running more complicated analysis. You can use the search tab to filter
methods and find specific functions.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure4.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 4. The Docs tab.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 4. The Docs tab.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;When clicking on any build in function, you will find a description of
the function, the arguments that the function takes, and the what the
function returns.&lt;/p&gt;
&lt;h5 id=&#34;the-scripts-tab--script-manager&#34;&gt;The Scripts tab – Script manager&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;Scripts tab&lt;/strong&gt; is where private, shared and all script examples
hosted by Google are stored. The &lt;strong&gt;Owner level&lt;/strong&gt; stores all your private
scripts, the ones that only you can see and edit. The &lt;strong&gt;Writer level&lt;/strong&gt;
stores scripts that has been shared (access granted) with you by another
user. Sharing repositories and scripts can be really useful to work on
collaborative projects. Once someone has added you to a repository, you
can add new scripts, modify existing scripts, and manage access to the
repository for other users. Of course, you can create and share
repositories and scripts with other users. The &lt;strong&gt;Reader level&lt;/strong&gt; contains
scripts for which you have read access granted by the script owner, but
you cannot edit the scripts. The &lt;strong&gt;Example level&lt;/strong&gt; has script examples
created and managed by Google, that are really good for some basic
operations. Take a look at the scripts available in this folder. The
&lt;strong&gt;Archive level&lt;/strong&gt; is for scripts that users have access to read but have
not been migrated by their owner from an older version of the Script
Manager. The search tab allows you to search for scripts within all
these levels.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure5.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 5. The Scripts tab.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 5. The Scripts tab.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;The &lt;strong&gt;New button&lt;/strong&gt; allows you to create a new repository in the &lt;strong&gt;Owner
folder&lt;/strong&gt; or to create folders and files within existing repositories. It
is a good practice to keep your repositories and scripts organized. Give
folders and scripts proper names, something that you will be able to
recognize after several months of not working on them. Your brain can
forget things pretty quick. Once you have created your repositories and
scripts, they can be renamed and deleted by the &lt;strong&gt;Rename and Delete
buttons&lt;/strong&gt; that appear at the right on the script while positioning the
pointer on top of the script. Scripts can be dragged and dropped into
new folders or copied into new repositories. The &lt;strong&gt;Revision History
button&lt;/strong&gt; allows to compare with previous versions and recover them.
Always important in case that you delete something by mistake.
Similarly, positioning the pointer on top a repository allows to delete
and access the history of the repository. The &lt;strong&gt;Configure button&lt;/strong&gt;
allows you to manage access to the repository. As we said before, you
can grant access to other users to your repository. Repositories can be
configured to be accessed using Git, so you can manage and edit your
scripts outside the Code Editor, or sync them with an external system
like Git Hub (Fig. 6).&lt;/p&gt;
&lt;p&gt;For this book we have created a repository that you can access in the
reader level.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure6.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 6. You can manage repositories to share with other users.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 6. You can manage repositories to
share with other users.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h5 id=&#34;the-asset-tab--asset-manager&#34;&gt;The Asset tab – Asset manager&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;Asset tab&lt;/strong&gt; allows you to upload and manage your own image assets,
shapefiles, and CSV files (Fig. 7). You can click on &lt;strong&gt;NEW&lt;/strong&gt; to upload
new assets from your personal computer. We will learn how to do this
with later examples in the following chapters. For now, it is good to
get familiarized with the different parts of the interface.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure7.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 7. The Asset tab.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 7. The Asset tab.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h4 id=&#34;search-bar-tool&#34;&gt;Search bar tool&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;search bar&lt;/strong&gt; located in the top of the Code Editor that says
‘Search places and data sets…’ allows you to search the GEE data
archive. Here you can type different keywords of data products, sensors
or places, to filter the data base. If you search for a place, clicking
on the place will make it visible in the map. If you search for a data
product, such as Landsat, you will see a detailed description of each
one. Click &lt;strong&gt;Import&lt;/strong&gt; to add the data product into your script as an
import (Fig. 8), which will appear at the top of the script. It is also
possible to import data products using code, so you do not have to
search and click the &lt;strong&gt;Import&lt;/strong&gt; button if you want to reuse the script
for another work. Additionally, you can see the JavaScript code to
import any product by clicking on the copy button next to the code and
pasting it into your script. You can delete imports you have added to
your script by positioning your pointer next to the import and clicking
the delete icon.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure8.png&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 8. Example of product desription on Google Earth Engine.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 8. Example of product desription
on Google Earth Engine.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h4 id=&#34;the-right-panel&#34;&gt;The right panel&lt;/h4&gt;
&lt;h5 id=&#34;inspector-tab&#34;&gt;Inspector tab&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;inspector tab&lt;/strong&gt; allows you to interactively query the different
map layers you have displayed (Fig. 9). When you clicking on the tab,
the pointer becomes a crosshair, meaning the function is activated. You
can click now on any part of the map and the tab will display the
location and values of all layers on the map. This can be useful to
inspect the values of your layers across different locations when
performing an analysis or when inspecting different layers in any area
of interest.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure9.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 9. The inspector, console and task tabs.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 9. The inspector, console and task
tabs.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h5 id=&#34;console-tab&#34;&gt;Console tab&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;console tab&lt;/strong&gt; is located next to the &lt;strong&gt;inspector tab&lt;/strong&gt; (Fig. 9).
This tab is where all the information you print on the code will be
displayed. We will see later how to print information when using
JavaScript. For now, it is enough to remember that this is the tab in
which you will interact with the data and where you can display
information and results from operations. Those can be text, objects or
charts. This console tab is interactive, meaning that you can expand
printed objects to get more details about them. We will start to use
this tab in the next section when you start writing your first
JavaScript code.&lt;/p&gt;
&lt;h5 id=&#34;tasks-tab&#34;&gt;Tasks tab&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;tasks tab&lt;/strong&gt; is located next to the console tab. This tab will
display long running tasks when exporting products from your analyses.
Those could be images or feature collections. Once the export was called
from the code editor, you will need to run it by clicking the run button
next to the exported task. A dialog window will appear, which allows you
to configure your export. Once you have run the task, the status will
display. Tasks can be canceled while running by clicking on the spinning
icon next to the task.&lt;/p&gt;
&lt;h4 id=&#34;profiler&#34;&gt;Profiler&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;profiler&lt;/strong&gt; is a tab that will appear next to the task tab on the
right upper panel when you run the script with the run with profiler
option (or the shortcuts, holding down Alt for PC or Option for Mac, and
clicking run, or pressing Ctrl+Alt+Enter/control+option+return). The tab
displays information about CPU time, memory consumed and other
computation parameters by the algorithms run and the display of all
layers in the map. This can be useful to understand why a script is
running slow or failing due to memory limitations. Not too important at
the beginning, but something to keep in mind for when you start to me
more advanced with Earth Engine analyses. Each row in the profiler tab
corresponds to each algorithm, computation, or asset loaded as described
in the &lt;strong&gt;Description&lt;/strong&gt; column. The count column indicates the
proportional number of times the operation was invoked, and the compute
column indicates the CPU time taken by the operation. The &lt;strong&gt;Peak Mem&lt;/strong&gt;
column indicates the maximum memory used on any single computation node
for the operation. The &lt;strong&gt;Current Mem&lt;/strong&gt; appears only when there is an
error given memory limitation and shows the amount of memory that was in
use on any single compute node when the error occurred.&lt;/p&gt;
&lt;h4 id=&#34;the-interactive-map-panel&#34;&gt;The interactive map panel&lt;/h4&gt;
&lt;h5 id=&#34;layer-manager&#34;&gt;Layer manager&lt;/h5&gt;
&lt;p&gt;At the top-right corner of the interactive map panel is the &lt;strong&gt;layer
manager&lt;/strong&gt; that allows you to adjust the display of all layers you added
to the map. From here you can activate and deactivate layers display and
adjust its transparency with the slider. By clicking the setting icon,
you can adjust the visualization parameters of each individual layer
(Fig. 10). From here you can customize the stretch of pixel value
display, adjust opacity (transparency), adjust gamma display and change
the color palette. You can manually choose different colors. Click Apply
button to apply the new visualization parameters to the current display.
You can also use the Import button to load the new visualization
parameters object as a new variable in the imports section of your
script that you can use to display other layers on the interactive map.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure10.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 10. Visualization parameters for layers displayed on the map.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 10. Visualization parameters for
layers displayed on the map.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;At the right of the &lt;strong&gt;layer manager&lt;/strong&gt; are buttons to choose different
map backgrounds: Google street view, terrain view or satellite view with
the option to activate labels. Another icon allows you to get into a
full-screen view.&lt;/p&gt;
&lt;h5 id=&#34;geometry-tools&#34;&gt;Geometry tools&lt;/h5&gt;
&lt;p&gt;In addition to images, you can import geometries (i.e., vector data in
Earth Engine language) to your script by drawing them on the map. To
create a geometry, use the &lt;strong&gt;geometry drawing tools&lt;/strong&gt; in the upper-left
corner of the interactive map panel. There are four different geometries
you can create: points, lines, polygons, and rectangles. When using the
drawing tool, any geometry created will automatically create a new
geometry layer in the import section of your script. Place the pointer
on the &lt;strong&gt;Geometry Imports&lt;/strong&gt; button and then click the &lt;strong&gt;+new layer&lt;/strong&gt;
link. Then choose the type of geometry you want to draw and start
clicking on the map to draw your geometries. In the case of polylines
and polygons, double click to finalize the drawing. Geometry display on
the map can be activated and deactivated from the &lt;strong&gt;Geometry Imports&lt;/strong&gt;
section. We will learn more about geometries in the second chapter of
this book. You can delete geometries by activating them using the hand
tool, and pressing &lt;strong&gt;Delete&lt;/strong&gt; on your keyboard or by directly clicking
the &lt;strong&gt;delete button&lt;/strong&gt; on the &lt;strong&gt;Imports section&lt;/strong&gt; of the script.&lt;/p&gt;
&lt;p&gt;You can configure the way geometries are imported into the script by
using the settings button that appears when positioning the pointer on
top of the geometry on the &lt;strong&gt;Geometry Imports&lt;/strong&gt; section. After clicking
settings, a dialog box will be displayed (Fig. 11). Note that geometries
can be imported as geometries, feature or feature collections and you
can choose on the button that appears on the box. We will learn more
about this in the next chapter. You can also modify the name and the
color of the layer. Finally, you can click the lock button to lock and
unlock the geometry, what can be useful to prevent edits or deletions by
accident.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh1/Figure11.png&#34; style=&#34;width:50.0%&#34;
alt=&#34;Figure 11. Dialog box for geometries.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 11. Dialog box for
geometries.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h4 id=&#34;the-help-button&#34;&gt;The help button&lt;/h4&gt;
&lt;p&gt;On the upper-right corner of your browser window you will find a help
button. When clicking it, you will find links to the developer user
guide with more detail information about all we just covered
(&lt;a href=&#34;https://developers.google.com/earth-engine/guides/getstarted&#34;&gt;link&lt;/a&gt;,
and also links to forums where users post questions and help each other.
These forums are of great help and you can post your own questions.
Before posting questions make sure to do a good search as there is a
good chance that someone else encounter a similar problem before and the
answer to your problem is already there.&lt;/p&gt;
&lt;h3 id=&#34;the-data-catalog&#34;&gt;The data catalog&lt;/h3&gt;
&lt;p&gt;One of the most notable features of Google Earth Engine is the enormous
amount of data available to be used. The database consists on about a
petabyte-scale archive of publicly available remotely sensed imagery and
other related data sets. It includes products from the most popular
satellites such as Landsat, MODIS, National Oceanographic and
Atmospheric Administration Advanced Very High Resolution Radiometer
(NOAA AVHRR), Sentinel 1, 2, 3 and 5-P; and Advanced Land Observing
Satellite (ALOS), and other data such as precipitation data, sea surface
temperature data, CHIRPS climate data, topography data, and land cover
data (Gorelick et al., 2017). This database increases daily as new
satellite imagery is taken, processes and uploaded. New products are
also being uploaded frequently. You can explore the data sets at the
following &lt;a href=&#34;https://developers.google.com/earth-engine/datasets/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to GEE, there is an online community working collaborating
further to distribute open source data. New products and datasets are
constantly being uploaded and can be find in the following
&lt;a href=&#34;https://samapriya.github.io/awesome-gee-community-datasets/projects/grip/&#34;&gt;link&lt;/a&gt;.
Also, a weekly list of new products updated to GEE can be found in the
following &lt;a href=&#34;https://github.com/samapriya/Earth-Engine-Datasets-List&#34;&gt;GitHub
repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;a-brief-intro-to-javascript-for-google-earth-engine&#34;&gt;A brief intro to JavaScript for Google Earth Engine&lt;/h3&gt;
&lt;p&gt;Now that we have a better understanding of the GEE layout, it is time to
start coding. The API of GEE uses JavaScript coding language. This is an
object-oriented language with first-class functions, and is best known
as the scripting language for Web pages, but it’s used in many
non-browser environments as well. It is a prototype-based,
multi-paradigm scripting language that is dynamic, and supports
object-oriented, imperative, and functional programming styles (xxx).&lt;/p&gt;
&lt;p&gt;In this first section we will learn the basics of JavaScript coding.
There are certain code statements that are proper of GEE. We will later
dive into those specifics.&lt;/p&gt;
&lt;h4 id=&#34;the-basics&#34;&gt;The basics&lt;/h4&gt;
&lt;p&gt;The code editor panel on the interface is probably the one that you will
use the most. If this is the first time you are facing a coding language
to perform data analysis, the recommendation is to take it slow. As you
get familiarized with the code structure and functions, things will get
easier, we promise.&lt;/p&gt;
&lt;p&gt;Let’s start with a simple JavaScript statement. Go to the
&lt;a href=&#34;code.earthengine.google.com&#34;&gt;code.earthengine.google.com&lt;/a&gt; and type into
the code editor panel:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;This is my first code&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After, click &lt;strong&gt;Run&lt;/strong&gt;. The text ‘This is my first code’ will appear on
the &lt;strong&gt;Console tab&lt;/strong&gt;. The parenthesis is used to pass arguments to
functions. Here print is the function that prints the argument on the
console.&lt;/p&gt;
&lt;p&gt;In JavaScript, each statement ends in semicolon (But if you do not write
it, the code will run anyway, but it may help you to keep your code
organized). You will learn to write entire analysis that will be
composed of multiple statements like that one.&lt;/p&gt;
&lt;p&gt;It is really important that you keep your code organized and knit. One
way of doing this is by commenting your code statements. Add &lt;code&gt;//&lt;/code&gt; before
the text you want to add after the statement. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;This is my first code&#39;); //This is my first statement to print text
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;JavaScript will read the first section of the statement, but will omit
all the text that comes after the &lt;code&gt;//&lt;/code&gt;. You can also comment out
statements by placing &lt;code&gt;//&lt;/code&gt; at the beginning of it. This will prevent the
statement from running without having to delete it from the code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//print(&#39;This is my first code&#39;); 
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;A note of writing your script: As you continue working on your script,
you will add new lines of code. In Earth Engine, every time you click
&lt;strong&gt;run&lt;/strong&gt; the entire code will be computed. If there is an error in some
line, the computation will end up there and an error message will be
prompted in the console pointing to the line number that has the error
that stopped the code from continuing processing. You will need to
debug and fix the error for the entire script to run successfully to
the end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;JavaScript works with objects that you can store and later use in other
operations. You can save objects using &lt;code&gt;var&lt;/code&gt; at the beginning of the
statement followed by a name for the objects. There are several types of
data types that JavaScript recognize and that we can use: strings,
numbers, list and dictionaries.&lt;/p&gt;
&lt;p&gt;Strings are text that are defined by single or double quotes (but never
mix them). Generally, it is preferred to use single quotes. You can make
a string and store it as a new object called myfirststring and print it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var myfirststring = &#39;This is my first code&#39;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Numbers can also be saved as objects.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var number = 57;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can pass this object to the function &lt;code&gt;print&lt;/code&gt;. But this time, we
are giving two arguments to the function as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;My favorite number is:&#39;, number);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When looking in the console, now the string and the number have been
printed.&lt;/p&gt;
&lt;p&gt;Another useful data type are lists. Lists are defined using brackets
&lt;code&gt;[]&lt;/code&gt;. For example, we can create a list of numbers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var myfirstlist = [0,1,2,3,4,5,6];

print(&#39;This is my first list of numbers:&#39;, myfirstlist);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or also a list of strings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var list2 = [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;];

print(list2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create arrays, that will add another dimension to the list.
We use brackets to define each dimension. The first brackets define the
columns of first dimension and the second brackets (inside the first
ones) define the rows or second dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var myfirstarray = [[1,2],[3,4],[&#39;a&#39;,&#39;b&#39;]];

print (myfirstarray);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have mixed numbers and strings here.&lt;/p&gt;
&lt;p&gt;Dictionaries are composed of pairs of keys and values and they are made
using curly brackets &lt;code&gt;{}&lt;/code&gt;. When defining a dictionary, you need to
specify the key (a name) and the value (e.g., a number, a string, a
list), separated by a colon. Add comma after the first pair of key and
value if you want to have more than one. For instance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var myfirstdictionary = {
  first: 1,
    second: &#39;a string&#39;,
    third: [1,2,3,5,8,12]
};

print (myfirstdictionary);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important feature of dictionaries is that you can access the
dictionary items suing square brackets or dot notation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(myfirstdictionary[&#39;first&#39;]);

print(myfirstdictionary.first);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dot notations are handy in JavaScript coding architecture. We will use
it more frequently when running different functions in a line of code.&lt;/p&gt;
&lt;h4 id=&#34;functions&#34;&gt;Functions&lt;/h4&gt;
&lt;p&gt;Now that we have seen the different data types, we are going to start
building and using functions. Functions allow to perform a myriad of
operations. While, there are multiple functions that are already built
within Earth Engine (Remember the &lt;strong&gt;Code Editor Docs&lt;/strong&gt; tab to see all
built-in functions), sometimes it is useful to create your owns.
Moreover, understanding the basic components of a function will help you
understand all the built-in ones. Functions need to be stored as
variables as before. Functions are defined with the keyword &lt;code&gt;function&lt;/code&gt;
followed by parentheses within which you can define different parameters
on which the function will operate. After the parentheses, follow the
statements that make up the function and go inside curly brackets. The
last statement is &lt;code&gt;return&lt;/code&gt;, which indicates the output of the function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var myfirstFunction = function(parameter1, parameter2) {
statement1;
statement2;
return statement;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no limit on the number of parameters that a function can take,
and it can also use variables declared outside the function.&lt;/p&gt;
&lt;p&gt;For example, we can build a function that adds 2 to a number. We will
name the function add2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var add2 = function(number) {
  var result = ee.Number(number).add(2);
  return result;
};
print(&#39;Result of add2:&#39;,add2(4)); //Adds 2 to the number 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;engine-objects&#34;&gt;Engine objects&lt;/h4&gt;
&lt;p&gt;So far, we have been using JavaScript code working with our computer
processor, also known as the &lt;em&gt;client side&lt;/em&gt;. However, we are going to be
using Google computing infrastructure to run operations, rather than the
processor of your own computer, or the &lt;em&gt;server side&lt;/em&gt;. Thus, we need to
create objects that can be sent in a form that is understood by Google
servers. These are the &lt;strong&gt;Earth Engine objects&lt;/strong&gt;. These are written in
the form of &lt;code&gt;ee.thing&lt;/code&gt; with thing being a data type, also known as
containers. Maybe, you noticed that in the previous function, we
specified &lt;code&gt;ee.Number&lt;/code&gt; so that Google Server could compute the sum.&lt;/p&gt;
&lt;p&gt;Now on, in order to created variables, we are going to assign those
variables to the corresponding container, which depends on the data
type, so that Google can recognize it and operate on it.&lt;/p&gt;
&lt;p&gt;For strings, we have the &lt;code&gt;ee.String()&lt;/code&gt; container&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var simpleString = &#39;Text to send to server&#39;;

var eeString = ee.String(simpleString);

print(&#39;This is:&#39;, eeString);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the print() function will send the container to Google server to be
evaluated, and then is sent back to your computer, where the container
is opened to be displayed.&lt;/p&gt;
&lt;p&gt;Similarly, for numbers we use &lt;code&gt;ee.Number()&lt;/code&gt;, for lists &lt;code&gt;ee.List()&lt;/code&gt; and
so on. The &lt;code&gt;ee.Thing&lt;/code&gt; are constructors that take arguments and
parameters to be processed by Google servers and returns a container
with a Earth Engine object that you can manipulate, export, print or
plot in your code.&lt;/p&gt;
&lt;p&gt;Note that print, Map, Chart, and other function we will use are on the
&lt;em&gt;client side&lt;/em&gt;. They run on your computer, not on Google servers. WE will
see next how to operate Google objects on the &lt;em&gt;server side&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&#34;working-with-earth-engine-objects&#34;&gt;Working with Earth Engine objects&lt;/h4&gt;
&lt;p&gt;Now we have seen how to create objects that Google servers can read and
process. It is important to incorporate this concept because it will be
critical for developing more complicated codes.&lt;/p&gt;
&lt;p&gt;Once you have created an Earth Engine object, then JavaScript operations
will not work, instead you have to use operations defined for those
Earth Engine objects. Thus, while we are using JavaScript to code, we
are now going to use a specific set of functions created to operate on
Google Earth Engine. On the &lt;strong&gt;Doc tab&lt;/strong&gt; on the top-left panel, you can
find all the methods and functions available to work with Earth Engine
objects.&lt;/p&gt;
&lt;p&gt;Within the built-in function set, you will find a variety of useful
functions that will make your work easier. For example, you can create a
sequence of numbers using &lt;code&gt;ee.List&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var eeList = ee.List([1, 2, 3, 4, 5]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But also you can use the built-in Earth Engine function sequence, to
create the same variable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var sequence = ee.List.sequence(1, 5);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the &lt;code&gt;ee.List&lt;/code&gt; has been saved as a variable, you can use other
Earth Engine functions to interact with this object. This will be the
building block of many functions that we will be using along this book.
One important function is &lt;code&gt;get&lt;/code&gt;, that gets something from the &lt;code&gt;ee.List&lt;/code&gt;
object. For example, you can get the second component of the list.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var value = sequence.get(2);

print(&#39;Value at index 2:&#39;, value);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important aspect to consider when accessing variables created with
Earth Engine functions is the data type. Many times, the object that
returns from a specific function does not come with a data type
assigned. Consequently, when trying to operate on those new variables,
you will get an error stating that the argument is not a function. For
instance, if you try to add a number to the value extracted from the
sequence in the previous step, you will get an error saying &lt;em&gt;value.add
is not a function&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To avoid this problem, you need to reassign the corresponding container
for the specific data type, in this case a &lt;code&gt;ee.Number&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(ee.Number(value).add(5));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This process is known as casting.&lt;/p&gt;
&lt;h4 id=&#34;earth-engine-dictionaries&#34;&gt;Earth Engine dictionaries&lt;/h4&gt;
&lt;p&gt;Dictionaries can be really useful in Earth Engine. Similar to the
JavaScript dictionary, we can create Engine containers for dictionaries
using &lt;code&gt;ee.Dictionary&lt;/code&gt; and specifying the different keys. Lets create a
dictionary containing keys for a species information:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Create an Earth Engine dictionary
var dictionary = ee.Dictionary({
  Common_name: &#39;Tiger&#39;,
  Genus: &#39;Panthera&#39;,
  Species: &#39;Panthera tigris&#39;
});

// Get the species name from the dictionary.
print(&#39;Species:&#39;, dictionary.get(&#39;Species&#39;));

// Get all the keys:
print(&#39;Keys: &#39;, dictionary.keys());
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that with the Earth Engine dictionary, now you cannot access the
elements using JavaScript code as before. You need to use an Earth
Engine function with Earth Engine objects. To extract a key you need to
use the function get. Remember, as we explained before, the returning
objects does not have an data type specified, and you will need to
specify &lt;code&gt;ee.String&lt;/code&gt; to use the object in another Earth Engine function.
Finally, you can also use the function &lt;code&gt;keys&lt;/code&gt; to extract all keys, and
the resulting objects is going to be an &lt;code&gt;ee.List&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;passing-arguments-to-functions-by-name&#34;&gt;Passing arguments to functions by name&lt;/h4&gt;
&lt;p&gt;Something that can be handy to remember before we move to more
complicated operations and data manipulation in Google Earth Engine, is
that you can pass argument to functions by name, using a similar
structure to a dictionary. In this way, you can visualize the name of
the different parameters, what can help understand the function. You can
write the parameters in any order. For instance, the function
&lt;code&gt;Map.setCenter()&lt;/code&gt; centers the map view at a specific zoom level. It then
takes tree arguments, lat, long and zoom. You can run the function as
follow:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.setCenter(99.2, 12.6, 6);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or you can pass the arguments using a dictionary to visualize the name
of the arguments:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.setCenter({lon:99.2, lat:12.6, zoom:6});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The arguments by name can be in any order:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.setCenter({lon:99.2, zoom:6; lat:12.6});
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;dates-in-google-earth-engine&#34;&gt;Dates in Google Earth Engine&lt;/h4&gt;
&lt;p&gt;Many times you will find yourself working with data and imageries from
different times that you will need to filter. To manage time in Google
Earth Engine, you need to use date Earth Engine objects using &lt;code&gt;ee.Date&lt;/code&gt;.
You can construct an &lt;code&gt;ee.Date&lt;/code&gt; object using a string to specify the
date:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define a date in Earth Engine.
var today = ee.Date(&#39;2020-6-9&#39;); //This is a year/month/day format
print(&#39;Today is:&#39;, today);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use another less orthodox method that is using a JavaScript
Date that represents milliseconds since midnight on January 1, 1970.
This seems strange at first, but you will see later on that it becomes
handy for multiple operations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Get current time using JavaScript Date
var now = Date.now();
print(&#39;Milliseconds since January 1, 1970&#39;, now);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Later in this book, we will explore different functions to filter data
sets by date or use data in more complex analysis.&lt;/p&gt;
&lt;h4 id=&#34;final-notes-on-operations&#34;&gt;Final notes on operations&lt;/h4&gt;
&lt;p&gt;We saw how to package objects created in the &lt;em&gt;client side&lt;/em&gt; into
containers to be used on the &lt;em&gt;server side&lt;/em&gt;, the ee.Thing. Sometime,
however, you may need to convert earth engine objects into client-side
objects. In this case, you can open the container to access its
information using &lt;code&gt;.getInfo()&lt;/code&gt;. Another method is using the function
&lt;code&gt;evaluate()&lt;/code&gt;, which retrieves the values inside the earth engine
objects. This is strictly not recommended by Google, so you need to use
only when it is absolutely necessary. The difference with &lt;code&gt;.getInfo()&lt;/code&gt;
is that the later will stop the rest of the code from continuing
processign until all information is retrieved from the Earth Engine
object, whereas &lt;code&gt;evaluate()&lt;/code&gt; will allow other porcesses to continue
operating.&lt;/p&gt;
&lt;p&gt;Retrieving the information in the containers and mixing the client and
server side is not recommended by Google, and you should only use it
when absolutely necessary. It is always better to operate on the server
side. Check the &lt;a href=&#34;https://developers.google.com/earth-engine/guides/client_server&#34;&gt;Google user
guide&lt;/a&gt;
to learn more about this.&lt;/p&gt;
&lt;h4 id=&#34;data-structures-in-google-earth-engine&#34;&gt;Data structures in Google Earth Engine&lt;/h4&gt;
&lt;p&gt;We have seen so far some important data structures in Earth Engine,
&lt;em&gt;String&lt;/em&gt;, &lt;em&gt;Number&lt;/em&gt;, &lt;em&gt;Array&lt;/em&gt;, &lt;em&gt;List&lt;/em&gt;, &lt;em&gt;Dictionary&lt;/em&gt; and &lt;em&gt;Date&lt;/em&gt;. Those are
fundamental blocks of Earth Engine code. However, in Google Earth Engine
we will be working with geospatial data, thus, two other important
geographic data structures are &lt;em&gt;Image&lt;/em&gt; and &lt;em&gt;Feature&lt;/em&gt;. These two data
structures correspond to the raster and vector data types. Images are
composed by a combination of bands and a dictionary with properties and
features by a combination of a geometry and dictionaries with
properties. If you are not familiar with raster and vector data, this is
a good moment to refresh those concepts before you continue with the
next chapter.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this first chapter we have cover an overview of the Google Earth
Engine platform. By now, you should have an account set and be
familiarized with the web-based interface. We have also seen the basics
of JavaScript coding language and how it works within Earth Engine. In
the following chapter we will start using some data and learn how to
work with it.&lt;/p&gt;
&lt;p&gt;For a more in depth description and tutorials to use GEE, remember to
visit the &lt;a href=&#34;https://developers.google.com/earth-engine/guides?hl=de&#34;&gt;Google Earth Engine
guide&lt;/a&gt; and
&lt;a href=&#34;https://developers.google.com/earth-engine/tutorials?hl=de&#34;&gt;tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;references-1&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., &amp;amp;
Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial
analysis for everyone. Remote Sensing of Environment, 202, 18–27.
&lt;a href=&#34;https://doi.org/10.1016/j.rse.2017.06.031&#34;&gt;https://doi.org/10.1016/j.rse.2017.06.031&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;C.E. Woodcock, R. Allen, M. Anderson, A. Belward, R. Bindschadler, W.
Cohen, F. Gao, S.N. Goward, D. Helder, E. Helmer, R. Nemani, L.
Oreopoulos, J. Schott, P.S. Thenkabail, E.F. Vermote, J. Vogelmann,
M.A. Wulder, R. Wynne. 2008. Free access to Landsat imagery. Science,
302 (5879), p. 1011&lt;/p&gt;
&lt;p&gt;Michael A. Wulder, Thomas R. Loveland, David P. Roy, Christopher J.
Crawford, Jeffrey G. Masek, Curtis E. Woodcock, Richard G. Allen, Martha
C. Anderson, Alan S. Belward, Warren B. Cohen, John Dwyer, Angela Erb,
Feng Gao, Patrick Griffiths, Dennis Helder, Txomin Hermosilla, James D.
Hipple, Patrick Hostert, M. Joseph Hughes, Justin Huntington, David M.
Johnson, Robert Kennedy, Ayse Kilic, Zhan Li, Leo Lymburner, Joel
McCorkel, Nima Pahlevan, Theodore A. Scambos, Crystal Schaaf, John R.
Schott, Yongwei Sheng, James Storey, Eric Vermote, James Vogelmann,
Joanne C. White, Randolph H. Wynne, Zhe Zhu. 2019. Current status of
Landsat program, science, and applications, Remote Sensing of
Environment, 225, 127-147, &lt;a href=&#34;https://doi.org/10.1016/j.rse.2019.02.015&#34;&gt;https://doi.org/10.1016/j.rse.2019.02.015&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;data-management-in-google-earth-engine&#34;&gt;Data management in Google Earth Engine&lt;/h2&gt;
&lt;p&gt;In this second chapter we will learn how to import, operate and display
images and features on Google Earth Engine (GEE). This is the basic
building block for any ecological spatial analysis and with time, this
set of operations and functions will become a simple habit.&lt;/p&gt;
&lt;p&gt;In order to learn the basic procedures and functions to work in GEE, we
will develop a simple example. The idea is that after completing this
chapter, you will have a basic understanding on how to change the area
of interest, load and work with different data sets (both, images and
features) and perform some of the most common functions and operations.&lt;/p&gt;
&lt;p&gt;This video will guide you through the code of this chapter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Management in GEE&lt;/strong&gt;:


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/5alN74QJbqI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;
&lt;p&gt;The first step in any project that involves coding is to start and save
a script where you can safely keep your work progress. First, using the
forward slash to comment out code, write a header to your script,
providing general information such as, your name and a title. Something
like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/////////////////////////////////
// My name
// Getting started with GEE
/////////////////////////////////
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also comment chunks of code using a slightly different code:
&lt;code&gt;/* text */&lt;/code&gt; This can be more handy for when you need to activate and
deactivate several lines of code at the same time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/*
My name
Getting started with GEE
*/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can choose the method that is easier for you.&lt;/p&gt;
&lt;p&gt;After you completed the header, click on the &lt;strong&gt;New button&lt;/strong&gt; on the
Scripts tab of the left panel. From the dropping menu, chose &lt;strong&gt;File&lt;/strong&gt;.
This will open a window where you can provide a name and chose the
repository where to save your script. Give scripts proper names. You can
also provide a brief description of your project if desired. Once you
have saved the script for the first time, you just need to click
&lt;strong&gt;Save&lt;/strong&gt; (Above the code editor panel) every time you want to save
updates to the script (Fig. 1).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure1.JPG&#34; style=&#34;width:60.0%&#34;
alt=&#34;Figure 1. Saving a new script.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 1. Saving a new
script.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;As you may recall from the introductory chapter, you can also create new
repositories and folders to keep your work organized and share them with
collaborators.&lt;/p&gt;
&lt;h3 id=&#34;defining-your-study-area&#34;&gt;Defining your study area&lt;/h3&gt;
&lt;p&gt;The study area for our example is in south-east Asia, encompassing the
south of Myanmar and Thailand. We will use this area to learn the basics
of data manipulation in GEE, but you can pick any place on the globe.&lt;/p&gt;
&lt;p&gt;To position the map display on the area of interest, we will use the
function &lt;code&gt;Map.setCenter()&lt;/code&gt;, for which we need to provide coordinates and
a zoom level (higher numbers indicate larger scale or more zoomed in).
You may recall this from the previous chapter.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.setCenter({lon:99.2, lat:12.6, zoom:6});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to define a geometry, in this case a rectangle, that
will represent the study area. Click on the rectangle icon from the
geometry tools and draw a rectangle that covers the area of interest
(see Fig. 2). We need to provide a name to the new geometry. Name it
StudyArea.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hint: You cannot use spaces for names, so we use capital letters or
underscores to differenciate words. At the end, it does not matter
what name you use, but it is good to use names for variables that you
can recognize.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After you finish, the geometry will appear in your imports (top of the
script editor) as a polygon with four vertices. We will discuss more
about geometries later in this chapter.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure2.JPG&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 2. Rectangle geometry demarcating the study area&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 2. Rectangle geometry demarcating
the study area&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;You can rename the geometry and save it as a new object using code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var Bounds = StudyArea;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You have now defined the study area. Later, we will use this polygon to
clip images and restrict the extent of our analysis.&lt;/p&gt;
&lt;h3 id=&#34;working-with-images&#34;&gt;Working with images&lt;/h3&gt;
&lt;p&gt;In GEE, raster data are represented as &lt;strong&gt;Image objects&lt;/strong&gt; and are the
main type of data to work with. All images are composed on one or more
bands, where each band has a name, data type, scale, mask and
projection, as well as metadata stored as a set of properties. Let’s
start working with some simple images.&lt;/p&gt;
&lt;h4 id=&#34;loading-images&#34;&gt;Loading images&lt;/h4&gt;
&lt;p&gt;The first step for our workflow is to import an image. Let’s start
working with a simple image with one band which contains information
about elevation, also known as Digital Elevation Models.&lt;/p&gt;
&lt;p&gt;Remember from previous chapter that you can search for places, images,
image collections, and feature collections on the GEE Data Catalog.&lt;/p&gt;
&lt;p&gt;To find the elevation data, on the search bar type: ‘SRTM Digital
Elevation Data 30m.’ Now, click on it to display the data description
where you can find information about the temporal availability, data
provider and collection ID (Fig. 3). This data set corresponds to a
digital elevation data from the Shuttle Radar Topography Mission (Farr
et al. 2007) and contains information of elevation at 30m spatial
resolution. You can click &lt;strong&gt;import&lt;/strong&gt; to add it to your imports, or you
can use code. We encourage you to use code, so you get more familiarized
with it and also helps you to keep the work organized.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure3.JPG&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 3. SRTM Digital Elevation Data description.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 3. SRTM Digital Elevation Data
description.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;The function &lt;code&gt;ee.Image()&lt;/code&gt; allows you to import image catalogs. To import
the elevation data, we need to provide the directory to the data within
the parenthesis of the function. Remember to assign the image to a new
object using &lt;strong&gt;var&lt;/strong&gt; as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var Elev = ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You have just imported the elevation data for the entire world.&lt;/p&gt;
&lt;p&gt;When working in spatial ecology, frequently, slope, aspect and hillshade
are variables of interest as many species respond positively or
negatively to changes in the terrain surface. Thus, these data sets can
be good predictor variables in several models. We only have information
about elevation, but you can calculate slope, aspect and hillshade and
save them as new objects using other GEE built-in functions.&lt;/p&gt;
&lt;p&gt;To calculate all these terrain variables, we will use the built-in
function &lt;code&gt;ee.Algorithms.Terrains()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s apply the function to the elevation image and save it as a new
object.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var Terrain = ee.Algorithms.Terrain(Elev);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, you should be familiarized with the code syntax to create
objects and use built-in functions in GEE.&lt;/p&gt;
&lt;p&gt;Print both elevation images to display the information on the console.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(Elev);
print(Terrain);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will see that differently from the Elev image that contains one
band, the new image created (Terrain) contains four bands, one for each
variable: elevation, slope, aspect and hillshade. All of these three new
bands where calculated from the elevation data. If you want to know more
about the algorithms used to obtain these results, remember that for all
the Engine built-in functions you can use the API reference on the Docs
tab to see a description and all the parameters used by the function.
The &lt;code&gt;ee.Algorithms&lt;/code&gt; category contains a list of currently supported
algorithms for specialized or domain specific processing.&lt;/p&gt;
&lt;p&gt;Look for the &lt;code&gt;ee.Algorithms.Terrains()&lt;/code&gt; function in the Docs tab (Fig.
4).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure4.JPG&#34; style=&#34;width:70.0%&#34;
alt=&#34;Figure 4. View of the ee.Algorithms.Terrains description in the Docs tab.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 4. View of the
ee.Algorithms.Terrains description in the Docs tab.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;This exercise shows you the versatility and power of conducting spatial
analysis on GEE. You can in matters of seconds, download elevation data
and calculate slope, aspect and hillshade for the entire world!&lt;/p&gt;
&lt;p&gt;In many cases, we are only interested in one specific area and not the
entire planet. We can use the GEE built-in function &lt;code&gt;clip()&lt;/code&gt; to crop the
image to our area of interest, the polygon we crated earlier.&lt;/p&gt;
&lt;p&gt;Using the dot notation, we add the function &lt;code&gt;clip&lt;/code&gt; to the image object
you want to crop, and specifying in between parenthesis the object
representing the area of interest we previously created.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var Terrain = Terrain.clip(Bounds);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You have now an image object cropped to the area of interest.&lt;/p&gt;
&lt;h3 id=&#34;displaying-data-on-the-map&#34;&gt;Displaying data on the map&lt;/h3&gt;
&lt;p&gt;Often, you will be interested in visualizing the images. To display the
elevation data on the map, you can use the function &lt;code&gt;Map.addLayer()&lt;/code&gt;. To
the previous code, now add the function to display the data as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.addLayer(Terrain);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Parameters for functions may have default values, and then, you only
need to specify the parameter if you want to change its default value.
For instance, the previous function displays a gray surface. You need to
change the default parameters for the appearance of the image to be able
to visualize elevation change.&lt;/p&gt;
&lt;p&gt;In the case of the &lt;code&gt;Map.addLayers()&lt;/code&gt; function, it contains five
arguments (Fig. 5). The first argument is the ee.Object to display, in
our case the &lt;strong&gt;Elev object&lt;/strong&gt;. The second argument are the visualization
parameters, which you can pass to the function as a dictionary. Those
include the bands to display (when you have more than one band in the
image, such as the Terrain image created that has 4 bands), the minimum
and maximum values to display, and colors (Remember that you can also
change those parameters from the Layers button in the display). The
third argument is the name of the layer (a string). The fourth argument
is a Boolean number (i.e., 1 for TRUE or 0 for FALSE) and indicates
whether the layer should be displayed by default or be activated by the
user manually by clicking on the layers tab. The last argument in the
opacity, which goes from 0 (transparent) to 1.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure5.JPG&#34; style=&#34;width:70.0%&#34;
alt=&#34;Figure 5. Description of the Map.addLayers function.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 5. Description of the
Map.addLayers function.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;pre&gt;&lt;code&gt;Map.addLayer(Elev, {min: 0, max: 1000}, &#39;DEM&#39;, 1);
Map.addLayer(Terrain, {bands:[&#39;elevation&#39;] ,min: 0, max: 1000}, &#39;Elevation&#39;, 1); //Same as before but from the terrain object
Map.addLayer(Terrain, {bands:[&#39;slope&#39;] ,min: 0, max: 40, palette:&#39;white,red&#39;}, &#39;Slope&#39;, 1); //Display slope and add a color palette
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the difference between an image that was cropped and the one that
was not cropped. Play with the image visualization using the Layers
button in the display.&lt;/p&gt;
&lt;h4 id=&#34;image-collections&#34;&gt;Image collections&lt;/h4&gt;
&lt;p&gt;We have so far worked with a single image. Now, we are going to access
an entire image collection. Image collections can contain hundreds of
images across long periods of time, such as images from satellites.
Similarly to the elevation image, you can search for image collections.
Let’s access the image collection of Landsat 8. Landsat images come with
different levels of pre-processing. Here we will work with T1, that is
the highest level of pre-processing.&lt;/p&gt;
&lt;p&gt;Use the search bar to find the name of the Landsat 8 T1 image collection
and import it using the function &lt;code&gt;ee.ImageCollection()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var landsat8 = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You just imported the entire image collection of Landsat 8 for the
entire Earth land surface. You could try to print the image collection
to see all the images contained in the list. However, because there are
too many images, printing it will either be very slow, time out, or
return an error.&lt;/p&gt;
&lt;h4 id=&#34;filtering-and-sorting&#34;&gt;Filtering and sorting&lt;/h4&gt;
&lt;p&gt;What we need to do to work with image collections is to filter them and
reduce the number of images in the list to the ones that we are
interested in. To do this, we will use filters.&lt;/p&gt;
&lt;p&gt;Filters are a set of functions that allows you to select a set of images
based on certain parameters such as position, time windows, numbers,
properties, depending on the type of object you are trying to filter,
such as image collections, feature collections, lists or geometries.
Lets see how this works for image collections.&lt;/p&gt;
&lt;p&gt;Let’s start by filtering our image collection by space and by time,
specifying the area and the time frame in which we are interested in
(Remember different satellite products contain images for different time
periods and that information is provided in the data description of each
product). We need to use &lt;code&gt;filterBounds()&lt;/code&gt; to select images from our area
of interest (the object Bounds) and &lt;code&gt;filterDate()&lt;/code&gt; to keep only images
from 2019, thus specifying a time window between 2019-01-01 and
2019-12-31. Note how we can add functions to the same line of code using
the dot notation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var landsat8 = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
    .filterBounds(Bounds)
    .filterDate(&#39;2019-01-01&#39;, &#39;2019-12-31&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new object contains only images for which the path overlaps our area
of interest and that were obtained during 2019.&lt;/p&gt;
&lt;p&gt;In addition, we are interested in finding images that are cloud-free.
Here we will use another function to solve this problem. All Landsat
images contain information about cloud cover in the metadata. We can use
this information to sort the image collection from lowest cloud cover to
highest cloud cover using the function &lt;code&gt;sort()&lt;/code&gt;, which operates on the
image metadata. You could apply a new function to the object created
with the previous line of code. You can also simplify things by
concatenating functions in the same line of code, again using the dot
notation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var landsat8 = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
    .filterBounds(Bounds)
    .filterDate(&#39;2019-01-01&#39;, &#39;2019-12-31&#39;)
    .sort(&#39;CLOUD_COVER&#39;, true);
    
print(landsat8) //The image collection contains 240 images
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new image collection is now sorted by the cloud cover percentage,
from lowest to highest. We also see that the collection contains now 240
images.&lt;/p&gt;
&lt;p&gt;You can save the first image (lowest cloud cover) as a new object and
display it. This time, you are displaying an image composed by a series
of bands and the final color display depends on the combination of bands
assigned to the tree main colors, red, green and blue. If you assign the
bands corresponding to the three colors, then you obtain a true color
display. You can also try a false color composition or displaying just
the thermal band.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var first_image = landsat8.first();

// True color
Map.addLayer(first_image, {bands: [&#39;B4&#39;, &#39;B3&#39;, &#39;B2&#39;], min: 7000, max: 10000}, &#39;Landsat8-RGB&#39;, 1);

// False color
Map.addLayer(first_image, {bands: [&#39;B5&#39;, &#39;B4&#39;, &#39;B3&#39;], min: 7000, max: 18000}, &#39;Landsat8-FalseColor&#39;, 1);

// Thermal
Map.addLayer(first_image, {bands: [&#39;B10&#39;], min: 19000, max: 24000, palette: [&#39;blue&#39;, &#39;red&#39;, &#39;orange&#39;, &#39;yellow&#39;]}, &#39;Landsat8-Thermal&#39;, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As always, you can access the complete GEE filtering functionality by
tipping &lt;code&gt;ee.Filter&lt;/code&gt; into the search bar of the &lt;strong&gt;Docs tab&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=&#34;reducing&#34;&gt;Reducing&lt;/h4&gt;
&lt;p&gt;We have so far selected and printed the first image of the list. Now we
want to create a full mosaic of the study area with images that are free
of clouds. The reduce function allows to reduce image collections to a
single image. It is a way to aggregate data over time
(&lt;code&gt;imageCollection.reduce()&lt;/code&gt;), space (&lt;code&gt;image.reduceRegion()&lt;/code&gt;,
&lt;code&gt;image.reduceNeighborhood()&lt;/code&gt;) or bands (&lt;code&gt;image.reduce()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ee.Reducer()&lt;/code&gt; can be a used to operate a simple statistic for
aggregating data (e.g. minimum, maximum, mean, median, standard
deviation, etc.), or for more complicated analysis on the input data
(e.g. histogram, linear regression, list). In all these cases, the
reducer takes an input data set and calculates a single output. When a
single input reducer is applied to a multi-band image, such as Landsat
image, GEE automatically applies the reducer separately to each band,
producing an output image with the same number of bands as the input.&lt;/p&gt;
&lt;p&gt;Continuing with our example, this time we will select all images with
less than 10% cloud cover and estimate the median value for each pixel
and each band in the Landsat 8 collection. To retain only images with
&amp;lt;10% cloud cover we will use the function, &lt;code&gt;filterMetadata()&lt;/code&gt; that
requires three arguments: The name of the metadata property, the name of
a comparison operator (it can be: “equals”, “less_than”,
“greater_than”, “not_equals”, “not_less_than”, “not_greater_than”,
“starts_with”, “ends_with”, “not_starts_with”, “not_ends_with”,
“contains”, or “not_contains”) and a number for the cloud cover
percentage to compare against. We will also increase the time period to
include images from the first day of 2015 to the last day of 2019 to
obtain a five years mosaic of our study area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var l8_mosaic_1 = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
    .filterBounds(Bounds)
    .filterDate(&#39;2015-01-01&#39;, &#39;2019-12-31&#39;)
    .filterMetadata(&#39;CLOUD_COVER&#39;, &#39;less_than&#39;, 10)
    .reduce(ee.Reducer.median());

print(l8_mosaic_1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reducer is going to change the name of the bands. When printing the
image, you can see that the band names have changed (Fig. 6).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure6.JPG&#34; style=&#34;width:60.0%&#34;
alt=&#34;Figure 6. Band names for the mosaic after applying a median reducer.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 6. Band names for the mosaic after
applying a median reducer.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Instead of using the reducer, you can use the built-in function
&lt;code&gt;median()&lt;/code&gt; that will do the same operation on each pixel across the
stack of all matching bands but retain the original band names.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var l8_mosaic_2 = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
    .filterBounds(Bounds)
    .filterDate(&#39;2015-01-01&#39;, &#39;2019-12-31&#39;)
    .filterMetadata(&#39;CLOUD_COVER&#39;, &#39;less_than&#39;, 10)
    .median();

Map.addLayer(l8_mosaic_2, {bands: [&#39;B4&#39;, &#39;B3&#39;, &#39;B2&#39;], min: 7000, max: 12800}, &#39;Landsat8-Mosaic&#39;, 1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we develop more complex analysis along the book, we will learn more
specifics on the reducer functions. You can look into the &lt;code&gt;ee.Reduce&lt;/code&gt;
methods that exist in the API.&lt;/p&gt;
&lt;h4 id=&#34;band-math&#34;&gt;Band math&lt;/h4&gt;
&lt;p&gt;Another powerful functionality is performing mathematical operations on
images or image bands. For example, you can use a combination of bands
to calculate vegetation indexes. One of the most widely used vegetation
indexes is the Normalized Difference Vegetation Index (NDVI). This index
provides an estimation of vegetation productivity, thus, is widely used
in spatial ecology studies. It is calculated using the near infrared and
red bands as follows:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;N&lt;strong&gt;D&lt;/strong&gt;V**I&lt;/em&gt; = (&lt;em&gt;N&lt;strong&gt;I&lt;/strong&gt;F&lt;/em&gt;−&lt;em&gt;R&lt;strong&gt;E&lt;/strong&gt;D&lt;/em&gt;)/(&lt;em&gt;N&lt;strong&gt;I&lt;/strong&gt;R&lt;/em&gt;+&lt;em&gt;R&lt;strong&gt;E&lt;/strong&gt;D&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;To demonstrate the versatility of GEE to perform mathematical operations
on images, we will calculate NDVI using different methods.&lt;/p&gt;
&lt;p&gt;Some simple image math can be performed by using &lt;code&gt;add()&lt;/code&gt;, &lt;code&gt;subtract()&lt;/code&gt;,
&lt;code&gt;divide()&lt;/code&gt;, &lt;code&gt;multiply()&lt;/code&gt;, &lt;code&gt;pow()&lt;/code&gt;, etc. Those operators can be applied
on numbers, images or arrays. We can use these operators to calculate
NDVI.&lt;/p&gt;
&lt;p&gt;First, we need to &lt;code&gt;select&lt;/code&gt; the desired bands from the image and then
apply the mathematical operation. For our example, we need the red band
(band 4) and the near infrared band (band 5):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Method 1 - Applying band operations
var ndvi_1 = l8_mosaic_2.select(&#39;B5&#39;).subtract(l8_mosaic_2.select(&#39;B4&#39;))
  .divide(l8_mosaic_2.select(&#39;B5&#39;).add(l8_mosaic_2.select(&#39;B4&#39;)));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is generally the case that you need to perform more
complicated mathematical operations. For those cases, you can use the
&lt;code&gt;expression()&lt;/code&gt; function, which allows to represent math operation in
text forms.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Method 2 - Applying band opperations using a expression
var ndvi_2 = l8_mosaic_2.expression(&#39;(B5 - B4) / (B5 + B4)&#39;, {
    B5: l8_mosaic_2.select(&#39;B5&#39;),
    B4: l8_mosaic_2.select(&#39;B4&#39;)
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;expression()&lt;/code&gt; returns an integer if two integers are divided,
so that the expression 10 / 20 = 0. To obtain decimal numbers as
results, you need to have decimal numbers in the operations. For
instance, in the previous case, you have to multiply one of the operands
by 1.0: 10 * 1.0 / 20 = 0.5.&lt;/p&gt;
&lt;p&gt;For many basic operations, such as calculating vegetation indexes, often
exists a built-in function in Earth Engine that makes things much
easier. Here, we will use the &lt;code&gt;normilizedDifference()&lt;/code&gt; function. We need
to provide the name of the two bands to use.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Method 3 - Using a built-in function
var ndvi_3 = l8_mosaic_2.normalizedDifference([&#39;B5&#39;, &#39;B4&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, it may be the case that you want to write your own function.
The advantage of writing a function is that you can then apply the
function to any image you want to work with. Here, we are going to write
a function that calculated NDVI (function &lt;code&gt;normilizedDifference()&lt;/code&gt;),
renames the resulting band as NDVI (function &lt;code&gt;rename()&lt;/code&gt;) and adds it to
the image as a new band (function &lt;code&gt;addBands()&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Method 4 - Building your own function to compute NDVI and add it as a new band to the image
var addNDVI = function(image) {
    var NDVI = image.normalizedDifference([&#39;B5&#39;, &#39;B4&#39;]).rename(&#39;NDVI&#39;)
    return image.addBands(NDVI) //with addBands, the new NDVI is added as a new band to the existing bands.
};

// Apply the function to the mosaic
var l8_mosaic_3 = addNDVI(l8_mosaic_2);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the function &lt;code&gt;map()&lt;/code&gt; to apply the addNDVI function you
have created to an entire image collection. As a result, all the images
on the collection will have an additional band called NDVI.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var l8_NDVI = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C02/T1&amp;quot;)
    .filterBounds(Bounds)
    .filterDate(&#39;2015-01-01&#39;, &#39;2019-12-31&#39;)
    .filterMetadata(&#39;CLOUD_COVER&#39;, &#39;less_than&#39;, 10)
    .map(addNDVI);
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;masking&#34;&gt;Masking&lt;/h4&gt;
&lt;p&gt;Another common operation when working with images is masking. Masking
refers to setting certain pixels from an image to no data values (i.e.,
making them transparent) in order to exclude them from analyses. Masking
is usually done to remove pixels with poor data, representing clouds or
any other area that wants to be excluded.&lt;/p&gt;
&lt;p&gt;Every pixel in a band of an &lt;code&gt;ee.Image&lt;/code&gt;, in addition to its value, has a
mask which ranges from 0 (i.e., no data) to 1. In Earth Engine, all
masked pixels (0) are treated as no data. When applying a mask, pixels
with a value of 0 are then excluded from operations. For instance, when
applying &lt;code&gt;image1.mask(image2)&lt;/code&gt;, the values of image2 are taken and used
as a mask of image 1, meaning that pixels in image2 that have the value
0 will be made transparent in image1.&lt;/p&gt;
&lt;p&gt;Continuing with our example, we may be interested in removing all the
ocean water from the NDVI image. We need to create a mask that will
retain only the land pixels (Fig. 7).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure7.JPG&#34; style=&#34;width:80.0%&#34;
alt=&#34;Figure 7. Masking process.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 7. Masking process.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;Using the elevation data, we can use the function &lt;code&gt;gt()&lt;/code&gt; (grater than)
to create a binary image with a value of 1 for all pixels with
elevations greater than 0, and a value of 0 to pixels with values lower
than 0. Then we use the function &lt;code&gt;updateMask()&lt;/code&gt; to retain only NDVI
values from the land (i.e., elevation &amp;gt; 0).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Create a land mask using SRTM elevation data.
var watermask =  ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;).gt(0);

// Update NDVI mask with the land mask.
var maskedndvi_1 = ndvi_1.updateMask(watermask);

// Display the masked result.
Map.addLayer(maskedndvi_1, {min:0, max:1}, &#39;NDVI masked&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Masks can be particularly useful in cases where you cannot find images
that are cloud free and you need to mask those pixels for analysis. We
will explore more of these type of operations using the image metadata
on pixel quality to create cloud masks for different satellites in
following chapters.&lt;/p&gt;
&lt;h3 id=&#34;working-with-features&#34;&gt;Working with features&lt;/h3&gt;
&lt;p&gt;In addition to the raster datasets, we can use vector data in GEE.
Vector data is handled with the Geometry type. Geometries in Earth
Engine can be &lt;strong&gt;Point&lt;/strong&gt; (a list of coordinates in some projection),
&lt;strong&gt;LineString&lt;/strong&gt; (a list of points), &lt;strong&gt;LinearRing&lt;/strong&gt; (a closed LineString),
and &lt;strong&gt;Polygon&lt;/strong&gt; (a list of LinearRings where the first is a shell and
subsequent rings are holes). &lt;strong&gt;MultiPoint&lt;/strong&gt;, &lt;strong&gt;MultiLineString&lt;/strong&gt;, and
&lt;strong&gt;MultiPolygon&lt;/strong&gt; are also supported.&lt;/p&gt;
&lt;h4 id=&#34;geometries-and-features&#34;&gt;Geometries and features&lt;/h4&gt;
&lt;p&gt;You can create Geometries using the Code Editor geometry tools, as you
did at the beginning of the chapter to define your study area. However,
you can also use code to create geometries, by providing a list of
coordinates. The following code will create the polygon representing the
study area defined earlier. All we need is a list of pair of
coordinates, indicating the vertices of the polygon. Check that the
first and last coordinates are the same.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var polygon = ee.Geometry.Polygon([
  [[97.58, 9.32], [100.43, 9.32], [100.43, 14.43], [97.58, 14.43], [97.58, 9.32]]
]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Geometries can then be converted into features. A &lt;strong&gt;Feature&lt;/strong&gt; is an
object with a geometry property storing a &lt;strong&gt;Geometry&lt;/strong&gt; object (or null)
and a &lt;strong&gt;property&lt;/strong&gt; storing a dictionary of other properties. In order to
create a feature, you need to provide a geometry, but also a dictionary
with other properties of interest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var polyFeature = ee.Feature(polygon, {Area: &#39;Tanintharyi&#39;});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Geometry&lt;/strong&gt; and &lt;strong&gt;Feature&lt;/strong&gt; objects can be printed or added to the map
similarly to images using &lt;code&gt;Map.addLayer()&lt;/code&gt;. The default visualization
parameters will display vectors with solid black lines and semi-opaque
black fill. You can change the colors similarly to images.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(polyFeature);
Map.addLayer(polyFeature, {}, &#39;Study area&#39;);
Map.addLayer(polyFeature, {color: &#39;red&#39;}, &#39;Study area - red&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;feature-collections&#34;&gt;Feature collections&lt;/h4&gt;
&lt;p&gt;Similar to image collections, you can create a feature collection, that
is as the name implies, a collection of features. To do this, we need to
apply &lt;code&gt;ee.FeatureCollection()&lt;/code&gt; on a list of features. For instance, we
can create a list of feature points, each of which represents a town
with its coordinate (the geometry) and a dictionary with the name of the
town. We can then combine all the town into a feature collection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Make a list of Features
var feature_list = [
  ee.Feature(ee.Geometry.Point(98.62, 12.44), {name: &#39;Myeik&#39;}),
  ee.Feature(ee.Geometry.Point(98.76, 11.25), {name: &#39;Bokpyin&#39;}),
  ee.Feature(ee.Geometry.Point(98.52, 13.39), {name: &#39;Pe Det&#39;})
];

// Create a FeatureCollection from the list, print it and display it on the map
var Towns = ee.FeatureCollection(feature_list);
print(Towns);
Map.addLayer(Towns, {color: &#39;blue&#39;}, &#39;Towns&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Google Earth Engine offers several &lt;strong&gt;Feature collections&lt;/strong&gt; through the
Data Catalog that can be imported. They are mostly data sets for the USA
but, in addition to the protected areas we used, there are also country
boundaries and ecoregions, among others. This is a much smaller catalog
than the one for image collections, but the amount of data available
grows every day.&lt;/p&gt;
&lt;h4 id=&#34;filtering-feature-collections&#34;&gt;Filtering feature collections&lt;/h4&gt;
&lt;p&gt;Feature collection can be large, and similar to image collections, we
may need to filter them. We will also use filters to do this. Here, we
will load the World Database on Protected Areas and filter it to our
study area, similarly to what we did before to filter the Landsat image
collection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var PA = ee.FeatureCollection(&#39;WCMC/WDPA/current/polygons&#39;);

var PA_filtered = PA.filterBounds(Bounds);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now check how many protected areas are within the study area by
using &lt;code&gt;size()&lt;/code&gt; which returns the number of entries in a dictionary.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(&#39;Count after filter:&#39;, PA_filtered.size());
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s filter the protected areas on our study area that are Marine
National Parks. We can use the &lt;code&gt;ee.Filter.eq()&lt;/code&gt; (eq for equal) function
to retain only those protected areas designated as Marine National Park
and check how many there are. As with the &lt;code&gt;filterMetadata&lt;/code&gt;, you need to
provide the property name of the feature to filter on and the value to
compare against.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var MarineNatParks = PA_filtered.filter(ee.Filter.eq(&#39;DESIG&#39;, &#39;Marine National Park&#39;));

print(&#39;Number of Marine National Parks:&#39;, MarineNatParks.size());
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;mapping-functions-over-feature-collections&#34;&gt;Mapping functions over feature collections&lt;/h3&gt;
&lt;p&gt;Similar to what we learned about mapping functions over image
collections, we can do it on feature collections. Let’s say you are
interested in calculating the area of each Marine National Park. We will
do this as an exercise even though the Protected Area database already
has a property with the area of each feature.&lt;/p&gt;
&lt;p&gt;First, we define a function to apply to each feature. The function
calculates the area using the &lt;code&gt;area()&lt;/code&gt; function. We then divide this by
1000000 to obtain area in square kilometers. The resulting number is set
to each feature as a new property that we called areakm2.&lt;/p&gt;
&lt;p&gt;Second, we map the &lt;code&gt;addArea&lt;/code&gt; function we created across the
&lt;strong&gt;MarineNatParks&lt;/strong&gt; feature collection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Compute the area of each National Park in km2

// Function to compute patch area and perimeter and add it as a property
var addArea = function(feature) {
  return feature.set({areakm2: feature.area().divide(1000 * 1000)});
};

// Map the area using the function over the FeatureCollection
var Area = MarineNatParks.map(addArea);
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;reducing-feature-collections&#34;&gt;Reducing feature collections&lt;/h4&gt;
&lt;p&gt;We can also apply reducers to feature collections. The idea is the same
than for image collections, aggregate data over the collection. Here, we
use the &lt;code&gt;reduceColumns()&lt;/code&gt; function on the Area feature collection
previously created. We need to specify the reducer we want to apply, in
this case the &lt;code&gt;ee.Reducer.mean()&lt;/code&gt;, and the property or list of
properties we want to reduce, in this case &lt;code&gt;areakm2&lt;/code&gt;. When printing this
object we have a new property with the mean area of all National Parks
in the collection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Calculate the mean area across parks
print(&#39;Mean National Park area (km2):&#39;, Area.reduceColumns(ee.Reducer.mean(), [&#39;areakm2&#39;]));
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;displaying-fature-and-feature-collections-on-the-map&#34;&gt;Displaying fature and feature collections on the map&lt;/h4&gt;
&lt;p&gt;Finally, you can display the protected areas and Marine National Parks
on the map. We use the same function than more images,
&lt;code&gt;Map.addLayers()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.addLayer(PA_filtered, {color: &#39;green&#39;}, &#39;Protected Areas&#39;);
Map.addLayer(MarineNatParks, {color: &#39;blue&#39;}, &#39;Marine NP&#39;); 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;exporting-resuts&#34;&gt;Exporting resuts&lt;/h3&gt;
&lt;p&gt;Lastly, at the end of an analysis you may want to export your results
and data products. There are many reasons you may want to do this. The
most obvious is to create figures and maps using another software, such
as &lt;a href=&#34;https://qgis.org/en/site/&#34;&gt;QGIS&lt;/a&gt; or to use raster and vector
products in other analysis in software such as
&lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are different type of products that can be exported from GEE.
These include images, map tiles, features, tables and videos. The data
can be exported to your linked Google Drive account, as a new Earth
Engine asset that will appear on your asset manager, or to Google Cloud
Storage (Note this is a fee-based service). We will not cover Cloud
Storage in this book, but you can learn about Cloud Storage
&lt;a href=&#34;https://developers.google.com/earth-engine/cloud/earthengine_cloud_project_setup?hl=en&#34;&gt;here&lt;/a&gt;
and how to set projects
&lt;a href=&#34;https://developers.google.com/earth-engine/cloud/projects?hl=en&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here, we are going to demonstrate how to export an image and a feature
collection. We will see more about these functions along the different
chapters.&lt;/p&gt;
&lt;h4 id=&#34;exporting-an-image&#34;&gt;Exporting an image&lt;/h4&gt;
&lt;p&gt;Imagine that you want to create a map of your study area displaying the
NDVI values across the region. You can then export the final NDVI image
we created previously &lt;strong&gt;maskedndvi_1&lt;/strong&gt;. We will export the image to
Google Drive, where it can be downloaded to be used in other software.
For this, we will use the function &lt;code&gt;Export.image.toDrive()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All export functions have a series of arguments that we need to
complete. First, we need to specify the &lt;code&gt;image&lt;/code&gt; we want to export, in
our case the &lt;strong&gt;maskedndvi_1&lt;/strong&gt;. Second, we need to provide a
&lt;code&gt;description&lt;/code&gt; that will be the file name when saved in Google Drive.
Then we need a &lt;code&gt;scale&lt;/code&gt; parameter, the spacial resolution or pixel size
in meters, and a &lt;code&gt;region&lt;/code&gt;, a ee.Geometry object that defines your area
of interest. In our example, we have been using &lt;strong&gt;Bounds&lt;/strong&gt;. Another
important argument is the &lt;code&gt;maxPixels&lt;/code&gt;. The default is 1e8 pixels, but
you can increase this number as needed. Note that when the image is too
big, it will be exported in tiles as 2 or more images that then you can
combine. The image default output is &lt;strong&gt;GeoTIFF&lt;/strong&gt;, but the &lt;strong&gt;TFRecord&lt;/strong&gt;
format is also supported.&lt;/p&gt;
&lt;p&gt;There are several other optional arguments for this functions. Here we
show two extra arguments. The &lt;code&gt;folder&lt;/code&gt; within your Google Drive to keep
it organized (By default the image will be saved in the Drive root
directory) and the &lt;code&gt;crs&lt;/code&gt;. The coordinate reference system is optional,
but we include it here because it is important to keep track of the crs
used when exporting data to use in other GIS software.&lt;/p&gt;
&lt;p&gt;Here is the code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Export raster images to Google Drive
Export.image.toDrive({
  image: maskedndvi_1,  // Export the masked NDVI as an example
  region: Bounds,
  description: &#39;NDVI-30m&#39;,
  folder: &#39;GIS&#39;,
  scale: 30,
  fileFormat: &#39;GeoTIFF&#39;,
  maxPixels: 1e10,
  crs: &#39;EPSG:4326&#39;
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other cases, you may be interested in saving an image as an asset, so
you can use it later in other analyses. To do this, we will use the
function &lt;code&gt;Export.image.toAsset()&lt;/code&gt;. The difference with exporting to
Google Drive is that we now need to provide an &lt;code&gt;assetId&lt;/code&gt; as an extra
parameter and that we do not need to specify a file format anymore.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Export raster image to asset
Export.image.toAsset({
  image: maskedndvi_1,  // Export the masked NDVI as an example
  region: Bounds,
  description: &#39;NDVI-30m&#39;,
  assetId: &#39;NDVI30mExport&#39;,
  scale: 30,
  maxPixels: 1e10,
  crs: &#39;EPSG:4326&#39;
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other optional arguments that can be modified. Look into the
Export of the Docs tab to explore other arguments.&lt;/p&gt;
&lt;h3 id=&#34;exporting-a-feature-collection-and-tables&#34;&gt;Exporting a feature collection and tables&lt;/h3&gt;
&lt;p&gt;Similar to images, we can export feature collections or tabular data,
such as a future collection with no geometry, as CSV, ESRI Shapefile
(SHP), GeoJSON, KML, KMZ or TFRecord formats. Again, we can export to
Google Drive using the function &lt;code&gt;Export.table.toDrive()&lt;/code&gt; or as an asset
&lt;code&gt;export.table.toAsset()&lt;/code&gt;. The arguments that we need to specify include
the &lt;code&gt;collection&lt;/code&gt; to export and a &lt;code&gt;description&lt;/code&gt; with the name for the
resulting file. When exporting to Google Drive we also need to provide
the &lt;code&gt;fileFormat&lt;/code&gt; and we will also specify a &lt;code&gt;folder&lt;/code&gt; in Google Drive.
When exporting as an asset, we need to provide the &lt;code&gt;assetId&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here we will export the marine national parks (the object
&lt;strong&gt;MarineNatParks&lt;/strong&gt;) as a shapefile and also save it as an asset.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Export feature to Google Drive
Export.table.toDrive({
  collection: MarineNatParks,  // Export the marine parks as an example
  description: &#39;Marine_Parks_Tanintharyi&#39;,
  fileFormat: &#39;SHP&#39;,
  folder: &#39;GIS&#39;
});

// Export feature to asset
Export.table.toAsset({
  collection: MarineNatParks,  // Export the marine parks as an example
  description: &#39;Marine_Parks_Tanintharyi&#39;,
  assetId: &#39;MarineParksExport&#39;
});

// Export table data
Export.table.toDrive({
  collection: Area,
  description: &#39;exportMarineNationalParkArea&#39;,
  fileFormat: &#39;CSV&#39;,
  folder: &#39;GIS&#39;,
  selectors: [&#39;areakm2&#39;] //A list of properties to include in the export; either a single string with comma-separated names or a list of strings.
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we will export the Marine National Park area as table data in a
CSV format. This object will not have a geometry associated, only the
area we calculated for each feature. For this we use the
&lt;code&gt;Export.table.toDrive()&lt;/code&gt; we used before with the difference that now the
&lt;code&gt;fileFormat&lt;/code&gt; is CSV. We also use the &lt;code&gt;selectors&lt;/code&gt; argument to specify the
property we want to export, NAME and areakm2 of each park. You can add
other properties if wanted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Export table data
Export.table.toDrive({
  collection: Area,
  description: &#39;exportMarineNationalParkArea&#39;,
  fileFormat: &#39;CSV&#39;,
  folder: &#39;GIS&#39;,
  selectors: [&#39;NAME&#39;,&#39;areakm2&#39;] //A list of properties to include in the export; either a single string with comma-separated names or a list of strings.
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that to complete the export process you need to submit the task by
clicking the &lt;code&gt;run&lt;/code&gt; button on the Tasks tab on the right panel (Fig. 8).
When the process is completed, the taks changes to a blue color as shown
in Fig. 8.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure8.png&#34; style=&#34;width:60.0%&#34;
alt=&#34;Figure 8. Exporting images and features.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 8. Exporting images and
features.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;You will see the csv file in your Google Drive (Fig. 9).&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh2/Figure9.png&#34; style=&#34;width:80.0%&#34;
alt=&#34;Figure 9. CSV file exported with name and area of Marine National Parks.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 9. CSV file exported with name and
area of Marine National Parks.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h3 id=&#34;conclusion-1&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;So far, we have seen some common functions to apply on images and
features. There are an enormous amount of possible operations that can
be applied to filter collections, calculate new bands, etc. As we
progress with different types of analyses, you will get more
familiarized with the different possible operations.&lt;/p&gt;
&lt;h2 id=&#34;land-cover-land-use-classification-in-google-earth-engine&#34;&gt;Land-Cover Land-use classification in Google Earth Engine&lt;/h2&gt;
&lt;p&gt;In this chapter we will learn how to perform a Land-Cover Land-use
classification using Google Earth Engine.&lt;/p&gt;
&lt;p&gt;There are two main types of classification, supervised and unsupervised
classifications. In the unsupervised classification, you let the
algorithm define the land cover classes based on the data. In the
supervised classification you will define the land cover classes and
train the algorithm.&lt;/p&gt;
&lt;p&gt;Here, we will work with the supervised classification process.&lt;/p&gt;
&lt;h3 id=&#34;define-your-study-area&#34;&gt;Define your study area&lt;/h3&gt;
&lt;p&gt;We will start with a fresh new script. As before, set up your name, date
and topic of the analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////////////////////
// My name
// Date
// Land cover Land use supervised classification
//////////////////////////////////////////////////
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to define your area of interest or study area. Remember
you can do this using the drawing tools or you can define a polygon
using code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//Specify &#39;Focal Area&#39; for your impact study
var StudyArea = ee.Geometry.Polygon([
  [96.20400705077832,23.942516412253614],
  [96.3197926495332,23.942516412253614],
  [96.3197926495332,24.021486550182036],
  [96.20400705077832,24.021486550182036],
  [96.20400705077832,23.942516412253614]
]);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, center the map into the study area that you just defined.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Map.centerObject(StudyArea);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-the-satellite-imagery&#34;&gt;Loading the satellite imagery&lt;/h3&gt;
&lt;p&gt;The main component of any land cover classification is the satellite
imagery. The information on the different bands of the image is what we
are going to use to classify the different types of land use.&lt;/p&gt;
&lt;p&gt;Here, we will work with Landsat 8. At this point, you should be familiar
with some basic functions regarding this data set, such as, loading the
image collection and filtering the image collection.&lt;/p&gt;
&lt;p&gt;We will get the image cleaning process to another level of complexity.
Before, we removed all images with clouds to avoid having clouds that
will affect the classification process (You want to have a clear image
of the land surface.). This time, we are going to use the metadata of
the Landsat image to mask all the pixels that contain clouds or have a
bad quality.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we are working with Landsat 8. There are other Landsat
products that require some modifications to the filters. We will
examine that later on when classifying images from the past to access
land cover change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What we need to do first is create a function that will mask all pixels
that correspond to clouds or cloud shadows. The information is contained
in a band called &lt;strong&gt;QA_PIXEL&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This function first creates a mask converting all pixels from the
&lt;em&gt;QA_PIXEL&lt;/em&gt; band of the Landsat image (Fig. 1) that correspond to cloud
and cloud shadows
&lt;code&gt;image.select([&#39;QA_PIXEL&#39;]).bitwiseAnd(Math.pow(2,3)).&lt;/code&gt; into zeros
&lt;code&gt;eq(0)&lt;/code&gt;, the mask. It then applies the mask to all the bands on that
image.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh3/Figure1.png&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 1. Landsat 8 Surface Reflectance image.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 1. Landsat 8 Surface Reflectance
image collection.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;The expression &lt;code&gt;Math.pow(2,3)&lt;/code&gt; obtains the bit value that identifies the
pixel with cloud of cloud shadow. You could do the same for snow, water,
etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Function to mask clouds based on QA values from the Landsat Surface Reflectance Code (LaSRC) - for Landsat 8
var lasrcMask = function (image) {
    var mask = image.select([&#39;QA_PIXEL&#39;]).bitwiseAnd(Math.pow(2,3)).eq(0).and(  // Cloud shadow
               image.select([&#39;QA_PIXEL&#39;]).bitwiseAnd(Math.pow(2,5)).eq(0));  // Cloud
    return image.updateMask(mask);
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are also going to create a function to &lt;code&gt;select()&lt;/code&gt; and &lt;code&gt;rename()&lt;/code&gt;
certain bands from the Landsat image. This may not be really useful if
you are working with only one Landsat product, but because different
Landsat satellites have different bands, it became important for more
advanced analysis that needs to integrate images from different
satellites (Landsat 4 and Landsat 8 for instance.).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Function to rename Landsat 8 bands for cross-Landsat compatibility &amp;amp; rescale
var renameBandsL8 = function(image) {
    var imgNewBands = image.select([&#39;SR_B2&#39;, &#39;SR_B3&#39;, &#39;SR_B4&#39;, &#39;SR_B5&#39;, &#39;SR_B6&#39;]).rename([&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;nir&#39;, &#39;swir1&#39;]);
    return imgNewBands.copyProperties(image,[&#39;system:time_start&#39;]);
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we will create a function to compute NDVI. This one should be
more familiar. Note that we want to add the new NDVI band to the rest of
the bands in each image, thus, we add &lt;code&gt;add.Bands(ndvi)&lt;/code&gt; to the return of
the function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Function to compute NDVI
var addNDVI = function(image) {
    var ndvi = image.normalizedDifference([&#39;nir&#39;, &#39;red&#39;]).rename(&#39;ndvi&#39;);
    return image.addBands(ndvi);
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the functions defined, we can load the Landsat 8
Surface Reflectance collection.&lt;/p&gt;
&lt;p&gt;We will apply a couple of filters. First &lt;code&gt;filterBounds()&lt;/code&gt; to keep images
that intersect the study area. We will also retain images that have less
than 20% of cloud coverage. Finally, we will work with images from 2019
to create a year composite.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Load Landsat surface reflectance images from 2016-2018
var l8sr = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;)
                .filterBounds(StudyArea)
                .filterMetadata(&#39;CLOUD_COVER&#39;, &#39;less_than&#39;, 20)
                .filterDate(&#39;2019-01-01&#39;, &#39;2019-12-31&#39;);
print(l8sr);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you print the image collection to the console, you will see that you
have retained 15 images:&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;FigCh3/Figure2.png&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 2. Landsat 8 Surface Reflectance filtered images.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 2. Landsat 8 Surface Reflectance
filtered images.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;We now can apply the functions to mask cloud pixels, rename bands and
add NDVI by using the function &lt;code&gt;map()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, we will create a median composite by calculating the &lt;code&gt;median()&lt;/code&gt;
value for each pixel in each band across all available images and
&lt;code&gt;clip()&lt;/code&gt; the final image to the study area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Apply functions
var l8sr8nocld = l8sr.map(lasrcMask)
                      .map(renameBandsL8)
                      .map(addNDVI);
                    
  
// Create a median composite image (takes the median value from each band across all available images)
var l8srcomp = l8sr8nocld.median().clip(StudyArea);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now display some band combinations into the map. We will use the
final composite together with the high resolution image provided by
Google to create training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Display image
Map.addLayer(l8srcomp, {bands: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;], gamma: 1, max: 14139, min: 8505, opacity: 1}, &#39;L8 SR True color&#39;);
Map.addLayer(l8srcomp, {bands: [&#39;nir&#39;, &#39;red&#39;, &#39;green&#39;], gamma: 1, max: 18376, min: 9196, opacity: 1}, &#39;L8 SR False color&#39;);
Map.addLayer(l8srcomp, {bands: [&#39;ndvi&#39;], palette: [&#39;white&#39;,&#39;green&#39;], max: 1, min: 0, opacity: 1}, &#39;NDVI&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;idenifying-land-cover-classes&#34;&gt;Idenifying land cover classes&lt;/h3&gt;
&lt;p&gt;The next step in the supervised classification process is to define the
classes that you will work with. This is an important step. Complete….&lt;/p&gt;
&lt;p&gt;For this example, we will work with four classes: 1. Forest 2. Water 3.
Sand 4. Agriculture&lt;/p&gt;
&lt;p&gt;We need to train a model to identify these four land cover classes
across the image composite. Thus, we need to indicate to the algorithm
where those areas are. For this, we will use the drawing tools to draw
polygons on top of the Landsat image composite where the land cover is
distinctive. You can use the high-resolution image provided by Google to
help you find clear areas. If your polygons are not well defined, then
the algorithm will confuse classes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that some areas are highly dynamic, such as the sand banks along
the river. Make sure your polygons match the Landsat image, as this is
the image you are using to train the algorithm.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You need to create a new geometry for each class. Then you can draw as
many polygons as you want for each class. Start with at least 10
polygons for each class. Try to account for all possible spectral
variation across the class.&lt;/p&gt;
&lt;p&gt;Choose a proper color to represent each class.&lt;/p&gt;
&lt;p&gt;Here is a demonstration on how to crate and name the different
geometries for the four classes.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;FigCh3/TrainingData.gif&#34; style=&#34;width:90.0%&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Use the hand tool to select and edit any polygons or delete them.&lt;/p&gt;
&lt;h3 id=&#34;creating-training-and-validation-data-sets&#34;&gt;Creating training and validation data sets&lt;/h3&gt;
&lt;p&gt;The next step is to create random points in each polygon to extract the
spectral information on those pixels. This is going to be the data set
to train and validate your classification model.&lt;/p&gt;
&lt;p&gt;The first step is to combine all geometries using the &lt;code&gt;merge()&lt;/code&gt; funtion.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Merge training polygons for land cover map
var LandCovers = [Forest,Agriculture,Sand,Water];
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next steps are going to be more challenging. Take your time to
understand every step in the code.&lt;/p&gt;
&lt;p&gt;Now that we have all geometries combined, we need to convert them into a
feature and add a numeric class property. Instead of doing this one by
one, we can create a loop.&lt;/p&gt;
&lt;p&gt;In a loop we start with the command &lt;code&gt;for()&lt;/code&gt;. The &lt;code&gt;for()&lt;/code&gt; takes 3
arguments. The first one is the variable to loop over and the value at
which to start (We call this variable i and start at 0). The second
argument is where to end the loop. Here instead of hard coding 3
(Remember that in java script the first element is in position 0, so the
fourth element is in position 3), we will use the function
&lt;code&gt;LandCovers.length&lt;/code&gt; to get the number of classes and use the symbol &lt;code&gt;&amp;lt;&lt;/code&gt;
to get one minus the total number of classes. The advantage of using
code is that you can later add more classes, and the loop will still
work as it will automatically adjust for the new classes. The final
argument increases the value each time the code in the loop is executed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Convert training data geometries to features and add numeric &#39;Class&#39; property
for (var i = 0; i &amp;lt; LandCovers.length; i++){
    LandCovers[i] = ee.Feature(LandCovers[i]).set(&#39;Class&#39;, i);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember that we need to use the ee.Thing notation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is ideal to have ground independent data to validate your model
predictions. However, many times it is not possible to gather such
information. One solution is to split your data set into a percentage
for training and the rest for validating. We will use 70% for training
and 30% for validation.&lt;/p&gt;
&lt;p&gt;We also need to define the number of random points that we want to
sample per class. A rule of thumb is to have for &lt;em&gt;n&lt;/em&gt; bands of data, at
least 10&lt;sup&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sup&gt; pixels for each class. For this example we are
going to work with 6 bands (blue, green, red, nir, swir1, ndvi) and 4
classes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// SAMPLE TRAINING AND VALIDATION PIXELS FROM EACH CLASS
var split = 0.7;   // What proportion of your data are used to train the model?
var N = 500;       // Define the number of pixels for each class to be randomly sampled from training polygons
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to create random points for each class to extract the pixel
values and split the data set into training and validation as previously
defined.&lt;/p&gt;
&lt;p&gt;To do this, we will have to loop over all polygons created.&lt;/p&gt;
&lt;p&gt;We first need to define two functions that we will need inside the loop.
One function to transform each point into a feature and add a property
class. And a second one to reassign the class after processing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var addClass = function(poly){
  return ee.Feature(ee.Geometry(poly)).set(&#39;Class&#39;,i);   // convert to feature and set Class
};
  
var ptsClass = function(f) {
  // Set class value for sampled points
    return f.set({
      Class: thisClass
    });
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now define two empty variables to store the training and validation data
sets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define empty variables
var trainVals;
var testVals;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now is the big loop. Here is the entire code, and then we will walk
trough line by line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for (var i = 0; i &amp;lt; LandCovers.length; i++){
  var geometries = LandCovers[i].geometry().geometries();   // extract individual geometries
  var extractpolys = ee.FeatureCollection(geometries.map(addClass));
  var extractpolys = extractpolys.randomColumn();  // add random number to each feature as property
    
  // Split into training/testing based on random number property
  var trainingPartition = extractpolys.filter(ee.Filter.lt(&#39;random&#39;, split));
  var testingPartition = extractpolys.filter(ee.Filter.gte(&#39;random&#39;, split));   
    
  // Sample random pixels from both sets of polygons
  var trainpts = ee.FeatureCollection.randomPoints(trainingPartition.geometry(), (N*split));
  var testpts = ee.FeatureCollection.randomPoints(testingPartition.geometry(), (N-N*split));
    
  // Extract values of current class from training data
  var thisClass = ee.Feature(LandCovers[i]).get(&#39;Class&#39;);
  
  // Create a function to the class property
  trainpts = trainpts.map(ptsClass);
  testpts = testpts.map(ptsClass);
  
  // Extract pixel values from composite at sampled points
  var trainPixelVals = l8srcomp.sampleRegions(trainpts, [&#39;Class&#39;, &#39;label&#39;], 30);
  var testPixelVals = l8srcomp.sampleRegions(testpts, [&#39;Class&#39;, &#39;label&#39;], 30);
  
  // Combine points across classes
  if (i === 0) {
     trainVals = trainPixelVals;
     testVals = testPixelVals;
  } else {
    trainVals = trainVals.merge(trainPixelVals);
    testVals = testVals.merge(testPixelVals);
  } 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at each line:&lt;/p&gt;
&lt;p&gt;We first define the arguments for the loop as before, to loop over each
land cover category:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;for (var i = 0; i &amp;lt; LandCovers.length; i++){&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The second line extracts the individual geometry of each polygon for the
first class &lt;code&gt;i&lt;/code&gt; of the loop:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var geometries = LandCovers[i].geometry().geometries();&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The third line applies the function &lt;code&gt;addClass()&lt;/code&gt; to each individual
geometry:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var extractpolys = ee.FeatureCollection(geometries.map(addClass));&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And the next line adds a random number as a new property to the feature.
We will use this random number to split the data set into training and
validation:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;extractpolys = extractpolys.randomColumn();&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Using the filter &lt;code&gt;ee.Filter.lt()&lt;/code&gt; (less than) and &lt;code&gt;ee.Filter.gte()&lt;/code&gt;
(greater or equal than) we split the polygons into training and
validation using the random number we created in the previous line.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var trainingPartition = extractpolys.filter(ee.Filter.lt(&#39;random&#39;, split));&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var testingPartition = extractpolys.filter(ee.Filter.gte(&#39;random&#39;, split));&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now we will create random points inside those polygons, first the ones
for training and then the ones for testing. For this, we use the
function &lt;code&gt;randomPoints()&lt;/code&gt; that takes the polygon geometries as the first
argument and the number of points to create as the second. We will
divide the number of points we previously specified into 70 and 30%.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var trainpts = ee.FeatureCollection.randomPoints(trainingPartition.geometry(), (N*split));&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var testpts = ee.FeatureCollection.randomPoints(testingPartition.geometry(), (N-N*split));&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The created random points do not have the class assigned. The next three
lines add the class to the point depending on which polygon they come
from that corresponds to the cycle of the loop. Remember that in the lop
we first perform all these functions to class 1 and then 2 and so on.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var thisClass = ee.Feature(LandCovers[i]).get(&#39;Class&#39;);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;trainpts = trainpts.map(ptsClass);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;testpts = testpts.map(ptsClass);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So far we just have created the random points for validation and testing
on the class. Now the important part, get the pixel values from the
image composite. For this, we use the function &lt;code&gt;sampleRegions&lt;/code&gt;. The
resulting variable will be the points with all the pixel values for each
band and the corresponding class as properties. The number 30 is the
spatial resolution of the Landsat image.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var trainPixelVals = l8srcomp.sampleRegions(trainpts, [&#39;Class&#39;], 30);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;var testPixelVals = l8srcomp.sampleRegions(testpts, [&#39;Class&#39;], 30);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Finally, we have a conditional statement in order to combine all the
points created for the class. If the loop is going trough the first
class, then it is saved into the previously created variables (trainVals
or testVals). If the loop is going trough the second, third, etc. cycle,
then the points are merged to the variables. This is needed because you
cannot merge points into an empty variable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if (i === 0) {      trainVals = trainPixelVals;      testVals = testPixelVals;   } else {     trainVals = trainVals.merge(trainPixelVals);     testVals = testVals.merge(testPixelVals); }  }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now, lets print and inspect the trainVals variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(trainVals)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;applying-the-classifier&#34;&gt;Applying the classifier&lt;/h3&gt;
&lt;p&gt;Now that we have the training and validation data sets, we can move to
the next step, training the classifier algorithm. There are many
different type of classifiers. You can look at them typing ee.Classifier
into the search bar of the Docs tab.&lt;/p&gt;
&lt;p&gt;For this tutorial, we are going to use the random forest classifier.&lt;/p&gt;
&lt;p&gt;The first thing we need to do is to define the bands that we are going
to use for the classification.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Indicate input bands for classifier
var bands = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;nir&#39;, &#39;swir1&#39;, &#39;ndvi&#39;]; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will apply the random forest classifier using the function
&lt;code&gt;ee.Classifier.smileRandomForest()&lt;/code&gt;. The argument is the number of
decision trees to create. We will use 100 for fast computation, but you
should use at least 500. The &lt;code&gt;train()&lt;/code&gt; function specifies the collection
of features to rain the classifier, using the specified numeric
properties of each feature as training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Classify using Random Forest
var rfClassifier = ee.Classifier.smileRandomForest(100).train(trainVals, &#39;Class&#39;, bands);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have trained the classifier, we can classify the image composite
(l8srcomp) using the &lt;code&gt;classify()&lt;/code&gt; function and providing the trained
classifier as the argument. Note that we only select the bands from the
image that we used to train the classifier.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var rfClassified = l8srcomp.select(bands).classify(rfClassifier);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now you have a classified image.&lt;/p&gt;
&lt;p&gt;The final step is to display the classification on the map.&lt;/p&gt;
&lt;p&gt;Use a pallet with colors that are representative of the land cover
classes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//Generate pallet   
var colors = [&#39;darkgreen&#39;,&#39;orange&#39;,&#39;yellow&#39;,&#39;darkblue&#39;]

// Add classified image to the map
Map.addLayer(rfClassified.clip(StudyArea), {
  palette: colors, 
  min: 0, 
  max: (LandCovers.length-1) // #classes-1
}, &#39;Land Cover Map&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;model-validation&#34;&gt;Model validation&lt;/h3&gt;
&lt;p&gt;The last task in a classification is to test the accuracy of the model.
Thus, how well the classifier actually classified the image. If the
classification is not really accurate, you need to modify the training
data and repeat the process until you get an accuracy value that
satisfies your work.&lt;/p&gt;
&lt;p&gt;The first step is a familiar one now, use the &lt;code&gt;classify()&lt;/code&gt; function
again, but this time on the 30% validation data set.&lt;/p&gt;
&lt;p&gt;Next, you will apply the &lt;code&gt;errorMatrix()&lt;/code&gt; function that computes an error
matrix by comparing the actual values of the training data with those
predicted by the classifier. You need to specify in the arguments the
name of the property containing the actual value and the name of the
property containing the predicted value. You can print the error matrix
in the console.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Check the accuracy of your classification
var rfTest = testVals.classify(rfClassifier);
var rfAccuracy = rfTest.errorMatrix(&#39;Class&#39;, &#39;classification&#39;);
print(&#39;RF Error Matrix: &#39;, rfAccuracy);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use the confusion matrix to estimate the accuracy of each band,
and with that, see for which classes the model is performing better or
worse. Here, we will use the function &lt;code&gt;accuracy()&lt;/code&gt; to get the overall
accuracy of the classification, defined as correct classified pixels
over the total.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var frOvAccuracy = rfAccuracy.accuracy().getInfo();
print(&#39;RF Overall Accuracy: &#39;, frOvAccuracy);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;conclussion&#34;&gt;Conclussion&lt;/h3&gt;
&lt;p&gt;This concludes the supervised classification exercise. In the next
chapter, we will see how to perform classifications using multiple
Landsat products in order to access land cover change trough time.&lt;/p&gt;
&lt;h2 id=&#34;species-distribution-models&#34;&gt;Species Distribution Models&lt;/h2&gt;
&lt;p&gt;The tutorial for SDMs can be found &lt;a href=&#34;https://ramirodcrego.com/teaching/gee/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementation of species distribution models in Google Earth Engine</title>
      <link>//localhost:4321/teaching/gee/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>//localhost:4321/teaching/gee/</guid>
      <description>&lt;p&gt;Ramiro D. Crego 1,2, Jared A. Stabach 1 and Grant Connette 1,2&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Smithsonian National Zoo and Conservation Biology Institute,
Conservation Ecology Center, 1500 Remount Rd, Front Royal, VA 22630,
USA.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Working Land and Seascapes, Conservation Commons, Smithsonian
Institution, Washington, DC 20013, USA.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;This is a guide for modeling species distributions and habitat
suitability in Google Earth Engine. This guide is intended to explain
the details of the Earth Engine code developed for this manuscript.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/ddi.13491&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;./Figures/26.png&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;We first cover the basics for importing data and setting the main
arguments used in different functions, such as, grid size and the area
of interest. We then expand on different modelling workflows using three
different case studies to demonstrate how to adapt the code workflow for
different goals.&lt;/p&gt;
&lt;p&gt;For information on how to set up a Google Earth Engine account as well
as user guidelines and tutorials visit:
&lt;a href=&#34;https://developers.google.com/earth-engine/&#34;&gt;https://developers.google.com/earth-engine/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The code found below can also be accessed through the GEE repository for
this study:
&lt;a href=&#34;https://code.earthengine.google.com/?accept_repo=users/ramirocrego84/SDM_Manuscript&#34;&gt;https://code.earthengine.google.com/?accept_repo=users/ramirocrego84/SDM_Manuscript&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are new to GEE, I recommend you starting with the tutorial An Introduction to Google Earth Engine for Ecologists and Practitioners.&lt;/p&gt;
&lt;p&gt;This a video tutorial explaining step by step the code forfitting species distribution models in GEE.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Species Distribution Models in GEE&lt;/strong&gt;:


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/54PPKkblAks?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h2 id=&#34;general-settings-for-running-sdms-in-google-earth-engine&#34;&gt;General settings for running SDMs in Google Earth Engine&lt;/h2&gt;
&lt;h3 id=&#34;importing-species-location-data-as-an-asset&#34;&gt;Importing species location data as an asset&lt;/h3&gt;
&lt;p&gt;Datasets need to be uploaded as assets in Google Earth Engine. The
easiest way to do this is by creating a csv file with spatial
coordinates and any other desired attribute information. Note that you
can also upload an ESRI Shapefile with the species location data.&lt;/p&gt;
&lt;p&gt;Below is an example for uploading the &lt;em&gt;Bradypus variegatus&lt;/em&gt; data set
from a &lt;code&gt;csv&lt;/code&gt; file. Prepare a &lt;code&gt;csv&lt;/code&gt; file with coordinates in latitude and
longitude (EPSG:4326). To include a column with date use format
Year-Month-Day (e.g., 2000-01-30).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig1.jpg&#34;
alt=&#34;Figure 1. Steps for uploading assets to Google Earth Engine. 1) Click ‘New’ under the Assets tab and then select ‘CSV file (.csv)’. 2) Click ‘SELECT’. 3) Browse and select the file from your computer. 4) Provide a name for the asset and the names of the columns containing coordinates in degrees.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 1. Steps for uploading assets to
Google Earth Engine. 1) Click ‘New’ under the Assets tab and then select
‘CSV file (.csv)’. 2) Click ‘SELECT’. 3) Browse and select the file from
your computer. 4) Provide a name for the asset and the names of the
columns containing coordinates in degrees.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;loading-and-cleaning-your-species-data&#34;&gt;Loading and cleaning your species data&lt;/h3&gt;
&lt;p&gt;To import the asset into your active script you can click on the forward
arrow icon on your asset manager or you can use code to programmatically
load the data as a new object. We recommend using code to import data.
To import the asset with your species presence data, use the
&lt;code&gt;ee.FeatureCollection()&lt;/code&gt; function and provide the asset ID. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var Data = ee.FeatureCollection(&#39;users/yourfolder/yourdata&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important step in modeling species distributions is to limit the
potential effect of geographic sampling bias on the model output due to
data aggregation resulting from multiple nearby observations.&lt;/p&gt;
&lt;p&gt;We thin the location data to one randomly selected occurrence record per
pixel at the chosen spatial resolution (the raster pixel or grain size
of the analysis).&lt;/p&gt;
&lt;p&gt;Here, we will apply a function to remove all points that lay within the
same raster cell at a given grain size. For this, we first need to
define the spatial resolution of our study.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define spatial resolution to work with (m)
var GrainSize = 10000; // e.g. 10 km
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can define a function to remove duplicates and apply it to the
species data set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function RemoveDuplicates(data){
  var randomraster = ee.Image.random().reproject(&#39;EPSG:4326&#39;, null, GrainSize);
  var randpointvals = randomraster.sampleRegions({collection:ee.FeatureCollection(data), scale: 10, geometries: true});
  return randpointvals.distinct(&#39;random&#39;);
}

var Data = RemoveDuplicates(Data);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following figure exemplifies how points are rarefied at a 1 km grain
size.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig2.png&#34;
alt=&#34;Figure 2. Example of presence point filtering. A) Original dataset; B) Final dataset with only one presence point retained per pixel.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 2. Example of presence point
filtering. A) Original dataset; B) Final dataset with only one presence
point retained per pixel.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;You can evaluate the number of points before and after removing
duplicates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;print(ee.FeatureCollection(&#39;users/yourfolder/yourimage&#39;).size())
print(Data.size())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;define-your-area-of-interest-for-modeling&#34;&gt;Define your area of interest for modeling&lt;/h3&gt;
&lt;p&gt;The extent of the analysis should be carefully selected and constrained
to a realistic realm of the species of study, avoiding unrealistic
extents that can hamper model accuracy and predictions (Guisan et al.,
2017; Leroy et al., 2018; Sillero et al., 2021).&lt;/p&gt;
&lt;p&gt;There are different ways you can define your area of interest. You can
directly draw a polygon using the drawing tools in GEE or manually set
the polygon (e.g., Case Study 2 in this tutorial). Here, we present two
methods for automating this process.&lt;/p&gt;
&lt;p&gt;If you are interested in working with a specific country or continent,
you can use the Large Scale International Boundary Polygons data set
available in GEE catalog.&lt;/p&gt;
&lt;p&gt;Here an example to select Kenya:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Load region boundary from data catalog if working at a larger scale
var AOI = ee.FeatureCollection(&#39;USDOS/LSIB_SIMPLE/2017&#39;).filter(ee.Filter.eq(&#39;country_co&#39;, &#39;KE&#39;));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the list of country codes at:
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_FIPS_country_codes&#34;&gt;https://en.wikipedia.org/wiki/List_of_FIPS_country_codes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you are interested in working within the entire African continent,
you can use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Load country boundary from data catalog if working at a country scale
var AOI = ee.FeatureCollection(&#39;USDOS/LSIB_SIMPLE/2017&#39;).filter(ee.Filter.eq(&#39;wld_rgn&#39;, &#39;Africa&#39;));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option is to select a bounding box around your species location
data. For example, we can define a bounding box using the function
&lt;code&gt;bounds()&lt;/code&gt; and add a buffer of 50 km.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define the area of interest
var AOI = Data.geometry().bounds().buffer(50000);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To display the study area on the map use the following code and assign
the map layer the name ‘AOI’:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Add AOI to the map
Map.addLayer(AOI, {}, &#39;AOI&#39;, 1); // The number 1 indicates the zoom level. Higher numbers increases zoom level.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;selecting-predictor-variables&#34;&gt;Selecting predictor variables&lt;/h3&gt;
&lt;p&gt;One of the main advantages of implementing SDMs in Google Earth Engine
is to make use of the large number of datasets available as predictor
variables. This includes not only the bioclimatic variables from Hijmans
et al. (2005), but elevation data and derivatives (slope, aspect,
hillside, etc.), diverse vegetation indices, human modification indices,
nighttime light images, water bodies, hourly climatic data, land cover
classifications, roads or other infrastructure and even the raw pixel
values of satellite data. Depending on your area of interest, certain
regions have greater data availability. GEE also offers the opportunity
to directly include user-derived datasets in your analysis, such as
processed satellite imagery (e.g., a land cover classification that you
previously developed for your area of interest).&lt;/p&gt;
&lt;p&gt;Selecting predictor variables is a step in which the researcher needs to
rely on existing knowledge of the study species, such as the variables
that may affect its distribution, etc.&lt;/p&gt;
&lt;p&gt;To find spatial data sets, you can use the search bar. All information
related to each spatial dataset is available by clicking on the name of
the product. The code necessary to import the dataset is available as
shown in the following figure.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig3.JPG&#34;
alt=&#34;Figure 3. SRTM Digital Elevation Data description with code to import the dataset&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 3. SRTM Digital Elevation Data
description with code to import the dataset&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;// Example of code to import the SRTM Digital Elevation Data 30m
var Elev = ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the GEE function &lt;code&gt;ee.Algorithms.Terrain()&lt;/code&gt; allows you to
calculate slope, aspect, and hillshade from the elevation dataset.&lt;/p&gt;
&lt;p&gt;We will demonstrate different ways to import and manipulate spatial data
sets with the specific examples below.&lt;/p&gt;
&lt;h3 id=&#34;defining-an-area-to-create-pseudo-absences&#34;&gt;Defining an area to create pseudo-absences&lt;/h3&gt;
&lt;p&gt;When using presence only data, the most common methodology when using
data from online databases such as, GBIF, it is important to define the
area to create pseudo-absences. Choosing the proper method is a critical
step as it can affect model performance (Barbet-Massin, Jiguet, Albert,
&amp;amp; Thuiller, 2012). Here we show how to implement three different
methodologies. In all three, we first create a mask of the location of
the presence data to avoid randomly generating pseudo-absences at the
same pixels as presences.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Note: The mask to remove presence point locations from the area to
create pseudo-absences has a code problem that is now fixed. Also
see Study Case 1 on how to mask out water if your area of interest
includes areas over water.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Generate random pseudo-absences across the entire study area.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;// Make an image out of the presence locations. The pixels where we have presence locations will be removed from the area to generate pseudo-absences.
// This will prevent having presence and pseudo-absences in the same pixel. 
var mask = Data
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(GrainSize)).mask().neq(1).selfMask();

var AreaForPA = mask.clip(AOI);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;Generate pseudo-absences within a specified distance from presence
locations to limit pseudo-absences to areas potentially accessible
to the species (Araújo et al., 2019) and to account for the
potential geographical or environmental sampling bias of presence
records by creating pseudo-absences with a similar sampling bias
(Phillips et al., 2009). The &lt;code&gt;buffer()&lt;/code&gt; function determines the area
available for generating pseudo-absences, assuming that these areas
have the same sampling bias than the records and represent areas
where animals can disperse. The &lt;code&gt;buffer()&lt;/code&gt; function has two
arguments, the distance in meters for the buffer and the maximum
amount of error tolerated when approximating the buffer circle.
Larger maximum errors improve computing efficiency.&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- --&gt;
&lt;pre&gt;&lt;code&gt;// Make an image out of the presence locations. The pixels where we have presence locations will be removed from the area to generate pseudo-absences.
// This will prevent having presence and pseudo-absences in the same pixel. 
var mask = Data
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(GrainSize)).mask().neq(1).selfMask();

// Option 2: Spatially constrained pseudo-absence selection to a buffer around presence points.
var buffer = 500000; // Distance in meters.
var AreaForPA = Data.geometry().buffer(buffer, 1000);
var AreaForPA = mask.clip(AreaForPA).clip(AOI);
right.addLayer(AreaForPA, {},&#39;Area to create pseudo-absences&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;Limit the area to select pseudo-absences by implementing an
environmental profiling technique, masking out the known
environmentally suitable locations, similar to other two-steps
methods for generating pseudo-absences (Chefaoui &amp;amp; Lobo, 2008;
Senay, Worner, &amp;amp; Ikeda, 2013).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This method requires fitting a clustering function to the occurrence
data. When you have a large presence dataset, you can randomly choose a
subset of the data with this line of code:
&lt;code&gt;Data.randomColumn().sort(&#39;random&#39;).limit(200)&lt;/code&gt; where the number in
&lt;code&gt;limit()&lt;/code&gt; specifies the number of random presence points used to train
the clustering algorithm. A critical step is to display the cluster
results and identify the correct cluster, zero or one, to use for
creation of pseudo-absences. The cluster ID is assigned in an
inconsistent manner and can change even when changing the order of the
same data input. We wrote a line of code to automatically identify the
cluster ID to define the area to create pseudo-absences. But it is
important to display the cluster results and confirm that the clustering
worked properly and that the correct cluster ID was identified before
creating the mask:
&lt;code&gt;var mask2 = Clresult.select([&#39;cluster&#39;]).eq(clustID)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Make an image out of the presence locations. The pixels where we have presence locations will be removed from the area to generate pseudo-absences.
// This will prevent having presence and pseudo-absences in the same pixel. 
var mask = Data
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(GrainSize)).mask().neq(1).selfMask();

//Option 3: Environmental pseudo-absences selection (environmental profiling)
// Extract environmental values for a random subset of presence data
var PixelVals = predictors.sampleRegions({collection: Data.randomColumn().sort(&#39;random&#39;).limit(200), properties: [], tileScale: 16, scale: GrainSize});
// Perform k-means clustering based on Euclidean distance.
var clusterer = ee.Clusterer.wekaKMeans({nClusters:2, distanceFunction:&amp;quot;Euclidean&amp;quot;}).train(PixelVals);
// Assign pixels to clusters using the trained clusterer.
var Clresult = predictors.cluster(clusterer);
// Display cluster results and identify the cluster IDs for pixels similar and dissimilar to the presence data
Map.addLayer(Clresult.randomVisualizer(), {}, &#39;Clusters&#39;, 0);
// Mask pixels that are dissimilar to the presence data.
// Obtain the ID of the cluster similar to the presence data and use the opposite cluster to define the allowable area to for creating pseudo-absences.
var clustID = Clresult.sampleRegions({collection: Data.randomColumn().sort(&#39;random&#39;).limit(200), properties: [], tileScale: 16, scale: GrainSize});
clustID = ee.FeatureCollection(clustID).reduceColumns(ee.Reducer.mode(),[&#39;cluster&#39;]);
clustID = ee.Number(clustID.get(&#39;mode&#39;)).subtract(1).abs();
var mask2 = Clresult.select([&#39;cluster&#39;]).eq(clustID);
var AreaForPA = mask.updateMask(mask2).clip(AOI);

// Display area for creation of pseudo-absence
Map.addLayer(AreaForPA, {},&#39;Area to create pseudo-absences&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;case-study-1-modeling-bradypus-variegatus-habitat-suitability-and-predicted-distribution-using-presence-only-data&#34;&gt;Case Study 1: Modeling &lt;em&gt;Bradypus variegatus&lt;/em&gt; habitat suitability and predicted distribution using presence-only data&lt;/h2&gt;
&lt;p&gt;To demonstrate the basic code to conduct SDMs in Google Earth Engine, we
will use &lt;em&gt;Bradypus variegatus&lt;/em&gt; as a case study. This species has been
widely used to present other SDM software and R packages (Hijmans et
al., 2017; Kindt, 2018; Phillips et al., 2017, 2006) and allows us to
compare GEE outputs with other tools. We obtained occurrence data from
GBIF (GBIF.org &lt;/p&gt;
\[20 January 2021\]
&lt;p&gt; GBIF Occurrence Download
&lt;a href=&#34;https://doi.org/10.15468/dl.jxcv7e&#34;&gt;https://doi.org/10.15468/dl.jxcv7e&lt;/a&gt;). We filtered data to the period
from 2000 to 2020, retaining only georeferenced records with a
coordinate uncertainty &amp;lt; 250 m. We further cleaned the data set by
removing all locations that fall on top of buildings or water bodies
assuming they had incorrect coordinates.&lt;/p&gt;
&lt;h3 id=&#34;loading-species-location-data&#34;&gt;Loading species location data&lt;/h3&gt;
&lt;p&gt;We upload the presence data set, specify the spatial scale to work with
and randomly select one occurrence location per 1km grid cell.&lt;/p&gt;
&lt;p&gt;Note that the following code modifies the &lt;code&gt;ui.root&lt;/code&gt; to display two maps
on the map panel of the user interface, one for the habitat suitability
map and one for the potential distribution map.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////
// Section 1 - Species data
///////////////////////////////

// Load presence data
var Data = ee.FeatureCollection(&#39;users/ramirocrego84/BradypusVariegatus&#39;);
print(&#39;Original data size:&#39;, Data.size());

// Define spatial resolution to work with (m)
var GrainSize = 1000;

function RemoveDuplicates(data){
  var randomraster = ee.Image.random().reproject(&#39;EPSG:4326&#39;, null, GrainSize);
  var randpointvals = randomraster.sampleRegions({collection:ee.FeatureCollection(data), scale: 10, geometries: true});
  return randpointvals.distinct(&#39;random&#39;);
}

var Data = RemoveDuplicates(Data);
print(&#39;Final data size:&#39;, Data.size());

// Add two maps to the screen.
var left = ui.Map();
var right = ui.Map();
ui.root.clear();
ui.root.add(left);
ui.root.add(right);

// Link maps, so when you drag one map, the other will be moved in sync.
ui.Map.Linker([left, right], &#39;change-bounds&#39;);

// Visualize presence points on the map
//right.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);
//left.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;In JavaScript you can activate or inactivate lines of code by using
//. JavaScript will omit all the text that comes after the //.
Commenting out code that prints objects or adds elements to the map is
a good practice to keep the code clean and efficient. Sometimes it can
help reduce the chance of reaching memory limits as we reduce the
number of processes being called. Another option is to use /* code
*/. All code in between will be inactivated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig4.png&#34;
alt=&#34;Figure 4. Presence data for Bradypus variegatus. Data were obtained from GBIF.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 4. Presence data for Bradypus
variegatus. Data were obtained from GBIF.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;defining-the-area-of-interest&#34;&gt;Defining the area of interest&lt;/h3&gt;
&lt;p&gt;The next step is to define the extent of the area of interest. Here we
defined a 100 km buffer around the bounding box containing all presence
data. The argument for the buffer distance is in meters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////////////
// Section 2 - Define Area of Interest
////////////////////////////////////////////

// Define the AOI
var AOI = Data.geometry().bounds().buffer({distance:50000, maxError:1000});

// Add border of study area to the map
var outline = ee.Image().byte().paint({
  featureCollection: AOI, color: 1, width: 3});
right.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);
left.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);

// Center each map to the area of interest
right.centerObject(AOI, 4); //Number indicates the zoom level
left.centerObject(AOI, 4); //Number indicates the zoom level
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that in the code we are using right and left instead of the
default Map statement now that we have divided the interactive map
into two elements, ‘right’ and ‘left’.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig5.png&#34;
alt=&#34;Figure 5. This figure shows the area of interest, which was defined as a 100 km buffer around the bounding box containing all presence locations.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 5. This figure shows the area of
interest, which was defined as a 100 km buffer around the bounding box
containing all presence locations.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;loading-predictor-variables&#34;&gt;Loading predictor variables&lt;/h3&gt;
&lt;p&gt;For this example, we selected a combination of climatic predictor
variables (temperature seasonality, maximum temperature of warmest
month, minimum temperature of coldest month and annual precipitation)
obtained from (Hijmans et al. 2005), elevation (Farr et al. 2007) and
percentage tree cover at 250 m resolution obtained from the Terra MODIS
Vegetation Continuous Fields (VCF) product. The VCF product is generated
yearly and produced using monthly composites of Terra MODIS Land Surface
Reflectance data. We estimated mean percentage tree cover for the period
of the occurrence data, 2003 to 2020. All predictor variables ultimately
need to be combined as bands into a single multi-band image. We also
mask oceans from the multi-band predictor image.&lt;/p&gt;
&lt;p&gt;The calculation of the median percentage tree cover for each pixel
across the annual MODIS images shows how powerful Google Earth Engine
can be for raster processing when creating predictor variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////////////////
// Section 3 - Selecting Predictor Variables
////////////////////////////////////////////////

// Load WorldClim BIO Variables (a multiband image) from the data catalog
var BIO = ee.Image(&amp;quot;WORLDCLIM/V1/BIO&amp;quot;);

// Load elevation data from the data catalog and calculate slope, aspect, and a simple hillshade from the terrain Digital Elevation Model.
var Terrain = ee.Algorithms.Terrain(ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;));

// Load NDVI 250 m collection and estimate median annual tree cover value per pixel
var MODIS = ee.ImageCollection(&amp;quot;MODIS/006/MOD44B&amp;quot;);
var MedianPTC = MODIS.filterDate(&#39;2003-01-01&#39;, &#39;2020-12-31&#39;).select([&#39;Percent_Tree_Cover&#39;]).median();

// Combine bands into a single multi-band image
var predictors = BIO.addBands(Terrain).addBands(MedianPTC);

// Mask out ocean pixels from the predictor variable image
var watermask =  Terrain.select(&#39;elevation&#39;).gt(0); //Create a water mask
var predictors = predictors.updateMask(watermask).clip(AOI);

// Select subset of bands to keep for habitat suitability modeling
var bands = [&#39;bio04&#39;,&#39;bio05&#39;,&#39;bio06&#39;,&#39;bio12&#39;,&#39;elevation&#39;,&#39;Percent_Tree_Cover&#39;];
var predictors = predictors.select(bands);

// Display layers on the map
right.addLayer(predictors, {bands:[&#39;elevation&#39;], min: 0, max: 5000,  palette: [&#39;000000&#39;,&#39;006600&#39;, &#39;009900&#39;,&#39;33CC00&#39;,&#39;996600&#39;,&#39;CC9900&#39;,&#39;CC9966&#39;,&#39;FFFFFF&#39;,]}, &#39;Elevation (m)&#39;, 0);
right.addLayer(predictors, {bands:[&#39;bio05&#39;], min: 190, max: 400, palette:&#39;white,red&#39;}, &#39;Temperature seasonality&#39;, 0); 
right.addLayer(predictors, {bands:[&#39;bio12&#39;], min: 0, max: 4000, palette:&#39;white,blue&#39;}, &#39;Annual Mean Precipitation (mm)&#39;, 0); 
right.addLayer(predictors, {bands:[&#39;Percent_Tree_Cover&#39;], min: 1, max: 100, palette:&#39;white,yellow,green&#39;}, &#39;Percent_Tree_Cover&#39;, 0); 
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig6.png&#34;
alt=&#34;Figure 6. Examples of predictor variables. A) Elevation; B) Temperature seasonality; C) Annual precipitation; D) Median percentage of tree cover between 2003 and 2020.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 6. Examples of predictor
variables. A) Elevation; B) Temperature seasonality; C) Annual
precipitation; D) Median percentage of tree cover between 2003 and
2020.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is important to make sure that there is no significant correlation
among predictor variables that can cause collinearity. We account for
this by estimating the Spearman correlation among predictor variable
values at 5000 random locations. Highly correlated predictor variables
should not be included in the same model.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Estimate correlation among predictor variables

// Extract local covariate values from multi-band predictor image at 5000 random points
var DataCor = predictors.sample({scale: GrainSize, numPixels: 5000, geometries: true}); //Generate 5000 random points
var PixelVals = predictors.sampleRegions({collection: DataCor, scale: GrainSize, tileScale: 16}); //Extract covariate values

// To check all pairwise correlations we need to map the reduceColumns function across all pairwise combinations of predictors
var CorrAll = predictors.bandNames().map(function(i){
    var tmp1 = predictors.bandNames().map(function(j){
      var tmp2 = PixelVals.reduceColumns({
        reducer: ee.Reducer.spearmansCorrelation(),
        selectors: [i, j]
      });
    return tmp2.get(&#39;correlation&#39;);
    });
    return tmp1;
  });
print(&#39;Variables correlation matrix&#39;,CorrAll);
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a function that requires a lot of memory in GEE as it is
working with a large feature collection. Once you have run the
correlation code and selected the final set of covariables to use, it
is recommended to comment out the print function so the correlations
between predictor variables are not run repeatedly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;creating-pseudo-absences&#34;&gt;Creating pseudo-absences&lt;/h3&gt;
&lt;p&gt;In this example, we use presence only data, the most common methodology
when using data from online databases such as, GBIF. We will generate
pseudo-absences to fit the model. But first, we need to define the area
in which random pseudo-absences can be generated.&lt;/p&gt;
&lt;p&gt;We used a two-step environmental profiling approach to restrict the area
for the creation of pseudo-absences. We first performed a k-means
clustering based on Euclidean distance for the presence data and then
created random pseudo-absences within the pixels classified as being
more dissimilar to the presence data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 4 - Defining area for pseudo-absences and spatial blocks for model fitting and cross validation
////////////////////////////////////////////////////////////////////////////////////////////////////////

// Make an image out of the presence locations. The pixels where we have presence locations will be removed from the area to generate pseudo-absences.
// This will prevent having presence and pseudo-absences in the same pixel. 
var mask = Data
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(GrainSize)).mask().neq(1).selfMask();

// Option 1: Simple random pseudo-absence selection across the entire area of interest.
// var AreaForPA = mask.updateMask(watermask).clip(AOI);

// Option 2: Spatially constrained pseudo-absence selection to a buffer around presence points.
//var buffer = 500000; // Distance in meters.
//var AreaForPA = Data.geometry().buffer({distance:buffer, maxError:1000});
//var AreaForPA = mask.clip(AreaForPA).updateMask(watermask).clip(AOI);
//right.addLayer(AreaForPA, {},&#39;Area to create pseudo-absences&#39;, 0);

//Option 3: Environmental pseudo-absences selection (environmental profiling)
// Extract environmental values for the a random subset of presence data
var PixelVals = predictors.sampleRegions({collection: Data.randomColumn().sort(&#39;random&#39;).limit(200), properties: [], tileScale: 16, scale: GrainSize});
// Perform k-means clusteringthe clusterer and train it using based on Eeuclidean distance.
var clusterer = ee.Clusterer.wekaKMeans({nClusters:2, distanceFunction:&amp;quot;Euclidean&amp;quot;}).train(PixelVals);
// Assign pixels to clusters using the trained clusterer
var Clresult = predictors.cluster(clusterer);
// Display cluster results and identify the cluster IDs for pixels similar and dissimilar to the presence data
right.addLayer(Clresult.randomVisualizer(), {}, &#39;Clusters&#39;, 0);
// Mask out pixels that are dissimilar to presence data.
// Obtain the ID of the cluster similar to the presence data and use the opposite cluster to define the allowable area to for creatinge pseudo-absences
var clustID = Clresult.sampleRegions({collection: Data.randomColumn().sort(&#39;random&#39;).limit(200), properties: [], tileScale: 16, scale: GrainSize});
clustID = ee.FeatureCollection(clustID).reduceColumns(ee.Reducer.mode(),[&#39;cluster&#39;]);
clustID = ee.Number(clustID.get(&#39;mode&#39;)).subtract(1).abs();
var mask2 = Clresult.select([&#39;cluster&#39;]).eq(clustID);
var AreaForPA = mask.updateMask(mask2).clip(AOI);

// Display area for creation of pseudo-absence
right.addLayer(AreaForPA, {palette: &#39;black&#39;},&#39;Area to create pseudo-absences&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig7a.png&#34;
alt=&#34;Figure 7a. Results from cluster analysis.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 7a. Results from cluster analysis.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig7b.png&#34;
alt=&#34;Figure 7b. Area to create pseudo-absences.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 7b. Area to create pseudo-absences.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;For this case study, we implement a block repeated split-sample
cross-validation technique to randomly partition data for model training
and validation (Roberts et al., 2017; Valavi, Elith, Lahoz-Monfort, &amp;amp;
Guillera-Arroita, 2019). We then run multiple model iterations with
random block splits to create training and validation data sets.&lt;/p&gt;
&lt;p&gt;The argument &lt;code&gt;scale&lt;/code&gt; determines the range in m for each block. In this
case, we are using 200 km.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define a function to create a grid over AOI
function makeGrid(geometry, scale) {
  // pixelLonLat returns an image with each pixel labeled with longitude and
  // latitude values.
  var lonLat = ee.Image.pixelLonLat();
  // Select the longitude and latitude bands, multiply by a large number then
  // truncate them to integers.
  var lonGrid = lonLat
    .select(&#39;longitude&#39;)
    .multiply(100000)
    .toInt();
  var latGrid = lonLat
    .select(&#39;latitude&#39;)
    .multiply(100000)
    .toInt();
  return lonGrid
    .multiply(latGrid)
    .reduceToVectors({
      geometry: geometry.buffer({distance:20000,maxError:1000}), //The buffer allows you to make sure the grid includes the borders of the AOI.
      scale: scale,
      geometryType: &#39;polygon&#39;,
    });
}
// Create grid and remove cells outside AOI
var Scale = 200000; // Set range in m to create spatial blocks
var grid = makeGrid(AOI, Scale);
var Grid = watermask.reduceRegions({collection: grid, reducer: ee.Reducer.mean()}).filter(ee.Filter.neq(&#39;mean&#39;,null));
right.addLayer(Grid, {},&#39;Grid for spatil block cross validation&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig8.png&#34;
alt=&#34;Figure 8. Visualization of blocks created for cross validation.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 8. Visualization of blocks
created for cross validation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;model-fit-model-validation-and-model-predictions&#34;&gt;Model fit, model validation and model predictions&lt;/h3&gt;
&lt;p&gt;We can now fit the models. There are several functions that need to be
defined.&lt;/p&gt;
&lt;p&gt;The first function allows us to create random seeds for splitting
spatial blocks and generating pseudo-absences at each iteration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////
// Section 5 - Fitting SDM models
//////////////////////////////////

// Define function to generate a vector of random numbers between 1 and 1000
function runif(length) {
    return Array.apply(null, Array(length)).map(function() {
        return Math.round(Math.random() * (1000 - 1) + 1)
    });
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to define a function to fit models.&lt;/p&gt;
&lt;p&gt;There are several non-parametric classification algorithms available in
GEE that can be implemented. These include random forest, support vector
machine, classification and regression trees, maximum entropy, and
gradient boosting.&lt;/p&gt;
&lt;p&gt;We implement a 10 block repeated split-sample cross-validation
technique. The number of pseudo-absences is balanced with the number of
presences for the training and validation data sets, as this practice is
recommended for machine learning classifiers (Evans et al., 2011;
Barbet-Massin et al. 2012). In this case, we will use random forest. But
note that in the &lt;code&gt;SDM&lt;/code&gt; function, we also provide the code to fit
gradient boosting classifiers. To change the classifier, you need to
comment out the random forest function call using two forward slashes
(&lt;code&gt;//&lt;/code&gt;) and activate the gradient boosting classifier.&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;setOutputMode()&lt;/code&gt; function to obtain results as &lt;code&gt;PROBABILITY&lt;/code&gt;
and as &lt;code&gt;CLASSIFICATION&lt;/code&gt;. This allows us to obtain a binary output (i.e.,
predicted presence) to quickly visualize in the interactive map. Later
we will show how to define a threshold to transform the probability
output into a binary map.&lt;/p&gt;
&lt;p&gt;At each iteration, the spatial blocks will be randomly split into 70%
for model fitting and 30% for model validation, respectively.
Consequently, each of the 10 runs will have a different set of presence
and pseudo-absence points for model fitting and validation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define SDM function
// Activate the desired classifier, random forest or gradient boosting. 
// Note that other algorithms are available in GEE. See ee.Classifiers on the documentation for more information.

function SDM(x) {
    var Seed = ee.Number(x);
    
    // Randomly split blocks for training and validation
    var GRID = ee.FeatureCollection(Grid).randomColumn({seed:Seed}).sort(&#39;random&#39;);
    var TrainingGrid = GRID.filter(ee.Filter.lt(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TestingGrid = GRID.filter(ee.Filter.gte(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    // Presence
    var PresencePoints = ee.FeatureCollection(Data);
    PresencePoints = PresencePoints.map(function(feature){return feature.set(&#39;PresAbs&#39;, 1)});
    var TrPresencePoints = PresencePoints.filter(ee.Filter.bounds(TrainingGrid));  // Filter presence points for training 
    var TePresencePoints = PresencePoints.filter(ee.Filter.bounds(TestingGrid));  // Filter presence points for testing
    
    // Pseudo-absences
    var TrPseudoAbsPoints = AreaForPA.sample({region: TrainingGrid, scale: GrainSize, numPixels: TrPresencePoints.size().add(300), seed:Seed, geometries: true}); // We add extra points to account for those points that land in masked areas of the raster and are discarded. This ensures a balanced presence/pseudo-absence data set
    TrPseudoAbsPoints = TrPseudoAbsPoints.randomColumn().sort(&#39;random&#39;).limit(ee.Number(TrPresencePoints.size())); //Randomly retain the same number of pseudo-absences as presence data 
    TrPseudoAbsPoints = TrPseudoAbsPoints.map(function(feature){
        return feature.set(&#39;PresAbs&#39;, 0);
        });
    
    var TePseudoAbsPoints = AreaForPA.sample({region: TestingGrid, scale: GrainSize, numPixels: TePresencePoints.size().add(100), seed:Seed, geometries: true}); // We add extra points to account for those points that land in masked areas of the raster and are discarded. This ensures a balanced presence/pseudo-absence data set
    TePseudoAbsPoints = TePseudoAbsPoints.randomColumn().sort(&#39;random&#39;).limit(ee.Number(TePresencePoints.size())); //Randomly retain the same number of pseudo-absences as presence data 
    TePseudoAbsPoints = TePseudoAbsPoints.map(function(feature){
        return feature.set(&#39;PresAbs&#39;, 0);
        });

    // Merge presence and pseudo-absencepoints
    var trainingPartition = TrPresencePoints.merge(TrPseudoAbsPoints);
    var testingPartition = TePresencePoints.merge(TePseudoAbsPoints);

    // Extract local covariate values from multiband predictor image at training points
    var trainPixelVals = predictors.sampleRegions({collection: trainingPartition, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16, geometries: true});

    // Classify using random forest
    var Classifier = ee.Classifier.smileRandomForest({
       numberOfTrees: 500, //The number of decision trees to create.
       variablesPerSplit: null, //The number of variables per split. If unspecified, uses the square root of the number of variables.
       minLeafPopulation: 10,//Only create nodes whose training set contains at least this many points. Integer, default: 1
       bagFraction: 0.5,//The fraction of input to bag per tree. Default: 0.5.
       maxNodes: null,//The maximum number of leaf nodes in each tree. If unspecified, defaults to no limit.
       seed: Seed//The randomization seed.
      });
    
    // Classify using a gradient boosting
    // var ClassifierPr = ee.Classifier.smileGradientTreeBoost({
    //   numberOfTrees:500, //The number of decision trees to create.
    //   shrinkage: 0.005, //The shrinkage parameter in (0, 1) controls the learning rate of procedure. Default: 0.005
    //   samplingRate: 0.7, //The sampling rate for stochastic tree boosting. Default 0.07
    //   maxNodes: null, //The maximum number of leaf nodes in each tree. If unspecified, defaults to no limit.
    //   loss: &amp;quot;LeastAbsoluteDeviation&amp;quot;, //Loss function for regression. One of: LeastSquares, LeastAbsoluteDeviation, Huber.
    //   seed:Seed //The randomization seed.
    // });
  
    // Presence probability 
    var ClassifierPr = Classifier.setOutputMode(&#39;PROBABILITY&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    var ClassifiedImgPr = predictors.select(bands).classify(ClassifierPr);
    
    // Binary presence/absence map
    var ClassifierBin = Classifier.setOutputMode(&#39;CLASSIFICATION&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    var ClassifiedImgBin = predictors.select(bands).classify(ClassifierBin);
   
    return ee.List([ClassifiedImgPr, ClassifiedImgBin, trainingPartition, testingPartition]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to define some parameters before executing the SDM function.
A variable with the percentage for data split, and a variable with the
number of iterations to run.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define partition for training and testing data
var split = 0.70;  // // The proportion of the blocks used to select training data

// Define number of repetitions
var numiter = 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now map the model function. Instead of generating random numbers,
we will manually set 10 random numbers for reproducibility of results.
The length of the list of random seeds determines the number of
iterations of model fitting and validation to run, with each iteration
having a different set of presence and pseudo-absence points.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Fit SDM 
//var RanSeeds = runif(numiter)
//var results = ee.List(RanSeeds).map(SDM)

// While the runif function can be used to generate random seeds, we map the SDM function over random created numbers for reproducibility of results
var results = ee.List([35,68,43,54,17,46,76,88,24,12]).map(SDM);

// Extract results from list
var results = results.flatten();
//print(results); //Activate this line to visualize all elements
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can extract the model predictions and display them on the maps.
Note that we will also create some legends for each map.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////
// Section 6 - Extracting and displaying model prediction results
///////////////////////////////////////////////////////////////////

// Habitat suitability

// Set visualization parameters
var visParams = {
  min: 0,
  max: 1,
  palette: [&amp;quot;#440154FF&amp;quot;,&amp;quot;#482677FF&amp;quot;,&amp;quot;#404788FF&amp;quot;,&amp;quot;#33638DFF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,
  &amp;quot;#1F968BFF&amp;quot;,&amp;quot;#29AF7FFF&amp;quot;,&amp;quot;#55C667FF&amp;quot;,&amp;quot;#95D840FF&amp;quot;,&amp;quot;#DCE319FF&amp;quot;],
};

// Extract all model predictions
var images = ee.List.sequence(0,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// You can add all the individual model predictions to the map. The number of layers to add will depend on how many iterations you selected.

// left.addLayer(ee.Image(images.get(0)), visParams, &#39;Run1&#39;);
// left.addLayer(ee.Image(image.get(1)), visParams, &#39;Run2&#39;);

// Calculate mean of all individual model runs
var ModelAverage = ee.ImageCollection.fromImages(images).mean();

// Add final habitat suitability layer and presence locations to the map
left.addLayer(ModelAverage, visParams, &#39;Habitat Suitability&#39;);
left.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);

// Create legend for habitat suitability map.
var legend = ui.Panel({style: {position: &#39;bottom-left&#39;, padding: &#39;8px 15px&#39;}});

legend.add(ui.Label({
  value: &amp;quot;Habitat suitability&amp;quot;,
  style: {fontWeight: &#39;bold&#39;, fontSize: &#39;18px&#39;, margin: &#39;0 0 4px 0&#39;, padding: &#39;0px&#39;}
}));

legend.add(ui.Thumbnail({
  image: ee.Image.pixelLonLat().select(0),
  params: {
    bbox: [0,0,1,0.1],
    dimensions: &#39;200x20&#39;,
    format: &#39;png&#39;,
    min: 0,
    max: 1,
    palette: [&amp;quot;#440154FF&amp;quot;,&amp;quot;#482677FF&amp;quot;,&amp;quot;#404788FF&amp;quot;,&amp;quot;#33638DFF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,
  &amp;quot;#1F968BFF&amp;quot;,&amp;quot;#29AF7FFF&amp;quot;,&amp;quot;#55C667FF&amp;quot;,&amp;quot;#95D840FF&amp;quot;,&amp;quot;#DCE319FF&amp;quot;]
  },
  style: {stretch: &#39;horizontal&#39;, margin: &#39;8px 8px&#39;, maxHeight: &#39;40px&#39;},
}));

legend.add(ui.Panel({
  widgets: [
    ui.Label(&#39;Low&#39;, {margin: &#39;0px 0px&#39;, textAlign: &#39;left&#39;, stretch: &#39;horizontal&#39;}),
    ui.Label(&#39;Medium&#39;, {margin: &#39;0px 0px&#39;, textAlign: &#39;center&#39;, stretch: &#39;horizontal&#39;}),
    ui.Label(&#39;High&#39;, {margin: &#39;0px 0px&#39;, textAlign: &#39;right&#39;, stretch: &#39;horizontal&#39;}),
    ],layout: ui.Panel.Layout.Flow(&#39;horizontal&#39;)
}));

legend.add(ui.Panel(
  [ui.Label({value: &amp;quot;Presence locations&amp;quot;,style: {fontWeight: &#39;bold&#39;, fontSize: &#39;16px&#39;, margin: &#39;4px 0 4px 0&#39;}}),
   ui.Label({style:{color:&amp;quot;red&amp;quot;,margin: &#39;4px 0 0 4px&#39;}, value:&#39;◉&#39;})],
  ui.Panel.Layout.Flow(&#39;horizontal&#39;)));

left.add(legend);


// Distribution map

// Extract all model predictions
var images2 = ee.List.sequence(1,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// Calculate mean of all indivudual model runs
var DistributionMap = ee.ImageCollection.fromImages(images2).mode();

// Add final distribution map and presence locations to the map
right.addLayer(DistributionMap, 
  {palette: [&amp;quot;white&amp;quot;, &amp;quot;green&amp;quot;], min: 0, max: 1}, 
  &#39;Potential distribution&#39;);
right.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);

// Create legend for distribution map
var legend2 = ui.Panel({style: {position: &#39;bottom-left&#39;,padding: &#39;8px 15px&#39;}});
legend2.add(ui.Label({
  value: &amp;quot;Potential distribution map&amp;quot;,
  style: {fontWeight: &#39;bold&#39;,fontSize: &#39;18px&#39;,margin: &#39;0 0 4px 0&#39;,padding: &#39;0px&#39;}
}));

var colors2 = [&amp;quot;green&amp;quot;,&amp;quot;white&amp;quot;];
var names2 = [&#39;Presence&#39;, &#39;Absence&#39;];
var entry2;
for (var x = 0; x&amp;lt;2; x++){
  entry2 = [
    ui.Label({style:{color:colors2[x],margin: &#39;4px 0 4px 0&#39;}, value:&#39;██&#39;}),
    ui.Label({value: names2[x],style: {margin: &#39;4px 0 4px 4px&#39;}})
  ];
  legend2.add(ui.Panel(entry2, ui.Panel.Layout.Flow(&#39;horizontal&#39;)));
}

legend2.add(ui.Panel(
  [ui.Label({value: &amp;quot;Presence locations&amp;quot;,style: {fontWeight: &#39;bold&#39;, fontSize: &#39;16px&#39;, margin: &#39;0 0 4px 0&#39;}}),
   ui.Label({style:{color:&amp;quot;red&amp;quot;,margin: &#39;0 0 4px 4px&#39;}, value:&#39;◉&#39;})],
  ui.Panel.Layout.Flow(&#39;horizontal&#39;)));

right.add(legend2);
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig9.png&#34;
alt=&#34;Figure 9. Visualization of predicted habitat suitability and potential distribution of Bradypus variegatus.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 9. Visualization of predicted
habitat suitability and potential distribution of &lt;em&gt;Bradypus
variegatus&lt;/em&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;It is important to understand that GEE does a resampling on the fly
for displaying maps. The resolution of the model output will change
with the zoom level. To set the visualization at the resolution of the
analysis defined with the grain size, you need to specify the
resolution of the image using the function &lt;code&gt;reproject()&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the next section, we will calculate the Area Under the Curve of the
Receiver Operator Characteristic (AUC-ROC)(Fielding and Bell, 1997) and
the Area Under the Precision-Recall Curve (AUC-PR; Sofaer, Hoeting, &amp;amp;
Jarnevich, 2019) for each run using the validation data sets. We will
then calculate the mean AUC-ROC and AUC-PR for the &lt;strong&gt;n&lt;/strong&gt; iterations.&lt;/p&gt;
&lt;p&gt;It is important to check that you have a sufficient number of points for
model validation at each run. Because the final number of points depends
on the random split of spatial blocks, you want to make sure there are
enough presence and pseudo-absence points for model validation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/////////////////////////////////////
// Section 7 - Accuracy assessment
/////////////////////////////////////

// Extract testing/validation data sets
var TestingDatasets = ee.List.sequence(3,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
                      return results.get(x)});

// Double check that you have a satisfactory number of points for model validation
print(&#39;Number of presence and pseudo-absence points for model validation&#39;, ee.List.sequence(0,ee.Number(numiter).subtract(1),1)
.map(function(x){
  return ee.List([ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,1)).size(),
         ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,0)).size()]);
})
);

// Define functions to estimate sensitivity, specificity and precision at different thresholds.
function getAcc(img,TP){
  var Pr_Prob_Vals = img.sampleRegions({collection: TP, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16});
  var seq = ee.List.sequence({start: 0, end: 1, count: 25});
  return ee.FeatureCollection(seq.map(function(cutoff) {
  var Pres = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,1);
  // true-positive and true-positive rate, sensitivity  
  var TP =  ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var TPR = TP.divide(Pres.size());
  var Abs = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,0);
  // false-negative
  var FN = ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  // true-negative and true-negative rate, specificity  
  var TN = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  var TNR = TN.divide(Abs.size());
  // false-positive and false-positive rate
  var FP = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var FPR = FP.divide(Abs.size());
  // precision
  var Precision = TP.divide(TP.add(FP));
  // sum of sensitivity and specificity
  var SUMSS = TPR.add(TNR);
  return ee.Feature(null,{cutoff: cutoff, TP:TP, TN:TN, FP:FP, FN:FN, TPR:TPR, TNR:TNR, FPR:FPR, Precision:Precision, SUMSS:SUMSS});
  }));
}

// Calculate AUC of the Receiver Operator Characteristic
function getAUCROC(x){
  var X = ee.Array(x.aggregate_array(&#39;FPR&#39;));
  var Y = ee.Array(x.aggregate_array(&#39;TPR&#39;)); 
  var X1 = X.slice(0,1).subtract(X.slice(0,0,-1));
  var Y1 = Y.slice(0,1).add(Y.slice(0,0,-1));
  return X1.multiply(Y1).multiply(0.5).reduce(&#39;sum&#39;,[0]).abs().toList().get(0);
}

function AUCROCaccuracy(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return getAUCROC(Acc);
}


var AUCROCs = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(AUCROCaccuracy);
print(&#39;AUC-ROC:&#39;, AUCROCs);
print(&#39;Mean AUC-ROC&#39;, AUCROCs.reduce(ee.Reducer.mean()));


// Calculate AUC of Precision Recall Curve
function getAUCPR(roc){
  var X = ee.Array(roc.aggregate_array(&#39;TPR&#39;));
  var Y = ee.Array(roc.aggregate_array(&#39;Precision&#39;)); 
  var X1 = X.slice(0,1).subtract(X.slice(0,0,-1));
  var Y1 = Y.slice(0,1).add(Y.slice(0,0,-1));
  return X1.multiply(Y1).multiply(0.5).reduce(&#39;sum&#39;,[0]).abs().toList().get(0);
}

function AUCPRaccuracy(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return getAUCPR(Acc);
}

var AUCPRs = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(AUCPRaccuracy);
print(&#39;AUC-PR:&#39;, AUCPRs);
print(&#39;Mean AUC-PR&#39;, AUCPRs.reduce(ee.Reducer.mean()));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accuracy assessment results are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;AUC-ROC&lt;/th&gt;
&lt;th&gt;AUC-PR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 1&lt;/td&gt;
&lt;td&gt;0.95&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 2&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 3&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 4&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 5&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 6&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 7&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 8&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 9&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 10&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;Mean&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.91&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.83&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the next section we will extract sensitivity (correct predictions of
the occurrence) and specificity (correct predictions of the absence;
Fielding &amp;amp; Bell, 1997). These metrics require defining a threshold. For
each iteration, we use the threshold that maximizes the sum of
sensitivity and specificity. This threshold has been shown to perform
well with presence-only data (Liu, Newell, &amp;amp; White, 2016).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Function to extract other metrics
function getMetrics(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return Acc.sort({property:&#39;SUMSS&#39;,ascending:false}).first();
}

// Extract sensitivity, specificity and mean threshold values
var Metrics = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(getMetrics);
print(&#39;Sensitivity:&#39;, ee.FeatureCollection(Metrics).aggregate_array(&amp;quot;TPR&amp;quot;));
print(&#39;Specificity:&#39;, ee.FeatureCollection(Metrics).aggregate_array(&amp;quot;TNR&amp;quot;));

var MeanThresh = ee.Number(ee.FeatureCollection(Metrics).aggregate_array(&amp;quot;cutoff&amp;quot;).reduce(ee.Reducer.mean()));
print(&#39;Mean threshold:&#39;, MeanThresh);
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Sensitivity&lt;/th&gt;
&lt;th&gt;Specificity&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 1&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 2&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 3&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 4&lt;/td&gt;
&lt;td&gt;0.93&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 5&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 6&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 7&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 8&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 9&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 10&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;Mean&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.90&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.81&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Finally, we can create a binary potential distribution map using the
mean threshold from the 10 model iterations.This operation is more
computationally demanding and is likely to give a memory limit error if
trying to add the resulting binary image directly to the interactive
map. If memory limits are reached, it is then necessary to use the batch
mode on GEE, directly exporting the output to Google Drive or Google
Cloud Storage.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////////////////////////////////////////////////
// Section 8 - Create a custom binary distribution map based on best threshold
////////////////////////////////////////////////////////////////////////////////

// Calculating the potential distribution map based on the threshold 
// that maximizes the sum of sensitivity and specificity is computationally intensive and
// for large number of iterations may need to be executed using batch mode.
// In batch mode, the final image needs to exported to Google Drive and opened in 
// another software for visualization (or imported to GEE as an asset for visualization.
// Transform probability model output into a binary map using the defined threshold and set NA into -9999
// Transform probability model output into a binary map using the defined threshold and set NA into -9999
var DistributionMap2 = ModelAverage.gte(MeanThresh);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next section we will show how to export results to Google Drive
to then display it on a third party software.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig10.png&#34;
alt=&#34;Figure 10. Potential distribution of Bradypus variegatus calculated using a custom threshold. The final map was exported to Google Drive and displayed using QGIS.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 10. Potential distribution of
&lt;em&gt;Bradypus variegatus&lt;/em&gt; calculated using a custom threshold. The
final map was exported to Google Drive and displayed using
QGIS.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;exporting-results&#34;&gt;Exporting results&lt;/h3&gt;
&lt;p&gt;Here, we present code to export the final probability maps as well as
the accuracy metrics, training and validation data sets used for each
model. Note that there are other options to export data in GEE (see the
&lt;a href=&#34;https://developers.google.com/earth-engine/guides/exporting/&#34;&gt;https://developers.google.com/earth-engine/guides/exporting/&lt;/a&gt;(user
guide) for other ways to export data).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////////////////////////
// Section 9 - Export outputs
//////////////////////////////////////////////////////

// Export final model predictions to drive

// Averaged habitat suitability
Export.image.toDrive({
  image: ModelAverage, //Object to export
  description: &#39;HSI&#39;, //Name of the file
  scale: GrainSize, //Spatial resolution of the exported raster
  maxPixels: 1e10,
  region: AOI //Area of interest
});

// Export final binary model based on a mayority vote
Export.image.toDrive({
  image: DistributionMap, //Object to export
  description: &#39;PotentialDistribution&#39;, //Name of the file
  scale: GrainSize, //Spatial resolution of the exported raster
  maxPixels: 1e10,
  region: AOI //Area of interest
});

// Export final binary model based on the threshold that maximises the sum of specificity and sensitivity
Export.image.toDrive({
  image: DistributionMap2.unmask(-9999),
  description: &#39;PotentialDistributionThreshold&#39;,
  scale: GrainSize,
  maxPixels: 1e10,
  region: AOI
});


// Export Accuracy Assessment Metrics

Export.table.toDrive({
  collection: ee.FeatureCollection(AUCROCs
                        .map(function(element){
                        return ee.Feature(null,{AUCROC:element})})),
  description: &#39;AUCROC&#39;,
  fileFormat: &#39;CSV&#39;,
});

Export.table.toDrive({
  collection: ee.FeatureCollection(AUCPRs
                        .map(function(element){
                        return ee.Feature(null,{AUCPR:element})})),
  description: &#39;AUCPR&#39;,
  fileFormat: &#39;CSV&#39;,
});

Export.table.toDrive({
  collection: ee.FeatureCollection(Metrics),
  description: &#39;Metrics&#39;,
  fileFormat: &#39;CSV&#39;,
});

// Export training and validation data sets

// Extract training datasets
var TrainingDatasets = ee.List.sequence(1,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// If you are interested in exporting any of the training or testing datasets used for modeling,
// you need to extract the feature collections from the SDM output list and export them.
// Here is an example for exporting the training and validation data sets from the first iteration. 
// For other iterations you need to change the number in the get function. In JavaScript the first element of the list is indexed by 0.

Export.table.toDrive({
  collection: TrainingDatasets.get(0),
  description: &#39;TestingDataRun1&#39;,
  fileFormat: &#39;CSV&#39;,
});

Export.table.toDrive({
  collection: TestingDatasets.get(0),
  description: &#39;TestingDataRun1&#39;,
  fileFormat: &#39;CSV&#39;,
});
/*
*/
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;case-study-2-accounting-for-temporal-resolution-in-species-distribution-models&#34;&gt;Case Study 2: Accounting for temporal resolution in species distribution models&lt;/h2&gt;
&lt;p&gt;One main limitation in many SDMs is the lack of consideration for
temporal resolution when modeling habitat suitability and species
distributions (Araújo et al., 2019). We used &lt;em&gt;Cebus capucinus&lt;/em&gt; as an
example to demonstrate a framework that takes advantage of GEE to match
the observation date for each presence record to the raster image from a
collection that is closest in time. For example, we could extract the
normalized difference vegetation index (NDVI) at an occurrence location
from the satellite image that is closest in time to that species
observation.&lt;/p&gt;
&lt;h3 id=&#34;species-data-and-aoi&#34;&gt;Species data and AOI&lt;/h3&gt;
&lt;p&gt;We obtained occurrence data from GBIF (GBIF.org (27 January 2021)
&lt;a href=&#34;https://doi.org/10.15468/dl.qus4ha&#34;&gt;https://doi.org/10.15468/dl.qus4ha&lt;/a&gt;). We retained only georeferenced
records with a coordinate uncertainty &amp;lt; 250 m.&lt;/p&gt;
&lt;p&gt;For this example we will model &lt;em&gt;Cebus capucinus&lt;/em&gt; distribution in Panama
and Costa Rica, where most occurrence records are.&lt;/p&gt;
&lt;p&gt;As a note, some authors consider the subspecies &lt;em&gt;Cebus capucinus
imitator&lt;/em&gt; and &lt;em&gt;Cebus capucinus capucinus&lt;/em&gt; two distinct species which
distribution split in central Panama (Mittermeier et al., 2013). In this
study, we consider &lt;em&gt;Cebus capucinus&lt;/em&gt; as one taxon.&lt;/p&gt;
&lt;p&gt;For this example, we will manually define a study area geometry
encompassing the countries of Costa Rica and Panama.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////
// Section 1 - Species data and AOI
////////////////////////////////////

var Data = ee.FeatureCollection(&#39;users/ramirocrego84/CebusCapucinus&#39;);

var AOI = ee.Geometry.Polygon([
  [-86.25662272529605,6.799166493750054],
  [-77.15994303779605,6.799166493750054],
  [-77.15994303779605,11.171211677305884],
  [-86.25662272529605,11.171211677305884],
  [-86.25662272529605,6.799166493750054]
]);

print(&#39;Original data size:&#39;, Data.size());
var Data = Data.filter(ee.Filter.bounds(AOI));

// Add border of study area to the map
var outline = ee.Image().byte().paint({
  featureCollection: AOI, color: 1, width: 3});
Map.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);

// Center map to the area of interest
Map.centerObject(AOI, 6); //Number indicates the zoom level
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig11.png&#34;
alt=&#34;Figure 11. Defined area of interest to study change in Cebus capucinus habitat suitability.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 11. Defined area of interest to
study change in Cebus capucinus habitat suitability.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We will use data between 2003 and 2018 for model fitting, after
filtering the data to retain a single record per year from each 250 m
pixel. To do so, we need to run the &lt;code&gt;removeduplicate()&lt;/code&gt; function for
each year at the specified grain size and then merge filtered data from
all years back together.&lt;/p&gt;
&lt;h3 id=&#34;define-spatial-resolution-and-remove-duplicates&#34;&gt;Define spatial resolution and remove duplicates&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////////////////////////////////
// Section 2 - Define spatial resolution and remove duplicates
//////////////////////////////////////////////////////////////

// Define spatial resolution to work with (m)
var GrainSize = 250;

function RemoveDuplicates(data){
  var randomraster = ee.Image.random().reproject(&#39;EPSG:4326&#39;, null, GrainSize);
  var randpointvals = randomraster.sampleRegions({collection:ee.FeatureCollection(data), scale: 10, geometries: true});
  return randpointvals.distinct(&#39;random&#39;);
}

// Filter by year and eliminate points withing the same pixel at each year
var Data03 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2003-01-01&#39;, &#39;2003-12-31&#39;)));
var Data04 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2004-01-01&#39;, &#39;2004-12-31&#39;)));
var Data05 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2005-01-01&#39;, &#39;2005-12-31&#39;)));
var Data06 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2006-01-01&#39;, &#39;2006-12-31&#39;)));
var Data07 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2007-01-01&#39;, &#39;2007-12-31&#39;)));
var Data08 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2008-01-01&#39;, &#39;2008-12-31&#39;)));
var Data09 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2009-01-01&#39;, &#39;2009-12-31&#39;)));
var Data10 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2010-01-01&#39;, &#39;2010-12-31&#39;)));
var Data11 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2011-01-01&#39;, &#39;2011-12-31&#39;)));
var Data12 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2012-01-01&#39;, &#39;2012-12-31&#39;)));
var Data13 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2013-01-01&#39;, &#39;2013-12-31&#39;)));
var Data14 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2014-01-01&#39;, &#39;2014-12-31&#39;)));
var Data15 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2015-01-01&#39;, &#39;2015-12-31&#39;)));
var Data16 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2016-01-01&#39;, &#39;2016-12-31&#39;)));
var Data17 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2017-01-01&#39;, &#39;2017-12-31&#39;)));
var Data18 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2018-01-01&#39;, &#39;2018-12-31&#39;)));

// Combine all datasets
var Data2 = Data03.merge(Data04).merge(Data05).merge(Data06).merge(Data07)
            .merge(Data08).merge(Data09).merge(Data10).merge(Data11).merge(Data12)
            .merge(Data13).merge(Data14).merge(Data15).merge(Data16).merge(Data17).merge(Data18);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;predictor-variables&#34;&gt;Predictor Variables&lt;/h3&gt;
&lt;p&gt;We will use mean annual temperature, annual precipitation (Hijmans et
al. 2005), elevation (Farr et al. 2007) and percentage tree cover (Terra
MODIS VCF, 250 m resolution) as predictor variables. We set the grain
size of the analysis at 250 m resolution to match the MODIS data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////////////////
// Section 3 - Selecting Predictor Variables
//////////////////////////////////////////////

// Load bioclimatic data set
var BIO = ee.Image(&amp;quot;WORLDCLIM/V1/BIO&amp;quot;);

// Load elevation data 
var Elevation = ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;);

// Combine bands into a single image
var predictors = BIO.addBands(Elevation);

// Load MODIS surface reflectance
var start = ee.Date(&#39;2003-01-01&#39;);
var end = ee.Date(&#39;2020-01-01&#39;);
var MODIS = ee.ImageCollection(&amp;quot;MODIS/006/MOD44B&amp;quot;)
             .filterDate(start, end);

// Mask ocean from predictor variables
var watermask =  Elevation.gt(0); //Create a water mask
var predictors = predictors.updateMask(watermask).clip(AOI);
var bands = [&#39;bio01&#39;,&#39;bio12&#39;,&#39;elevation&#39;,&#39;Percent_Tree_Cover&#39;];

Map.addLayer(predictors, {bands:[&#39;elevation&#39;], min: 0, max: 5000,  palette: [&#39;000000&#39;,&#39;006600&#39;, &#39;009900&#39;,&#39;33CC00&#39;,&#39;996600&#39;,&#39;CC9900&#39;,&#39;CC9966&#39;,&#39;FFFFFF&#39;,]}, &#39;Elevation (m)&#39;, 0);
Map.addLayer(MODIS.first(), {bands:[&#39;Percent_Tree_Cover&#39;], min: 0, max: 100, palette:&#39;white,yellow,green&#39;}, &#39;Percent_Tree_Cover&#39;, 0); 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each data location, we need to identify the closest recorded MODIS
image in time and extract the percentage tree cover value. We need to
define a series of function to do this. In this case, because the data
product is produced yearly, we define a max difference of 360 days.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/////////////////////////////////////////////////////////////////////////////////
// Section 4 - Match each point to the closest image in time and extract the pixel value
/////////////////////////////////////////////////////////////////////////////////

// Function to add property with time in milliseconds to the data
var add_date = function(feature) {
  return feature.set({date_millis: ee.Date(ee.String(feature.get(&amp;quot;Date&amp;quot;))).millis()});
};
var Data2 = Data2.map(add_date);

// Join Image and Points based on a maxDifference Filter within a day
var tempwin = 360;  // set time window (days)

var maxDiffFilter = ee.Filter.maxDifference({
  difference: tempwin * 24 * 60 * 60 * 1000,  // 8 day * hr * min * sec * milliseconds
  leftField: &#39;date_millis&#39;, //date data was collected
  rightField: &#39;system:time_start&#39; // image date
});

// Define the join.
var saveBestJoin = ee.Join.saveBest({
  matchKey: &#39;bestImage&#39;,
  measureKey: &#39;timeDiff&#39;
});

// Apply the join
var Data_match = saveBestJoin.apply(Data2, MODIS, maxDiffFilter);
//print(Data_match.limit(2)) //Activate to visualize results

// Function to add property with Percent Tree Cover value from the matched MODIS image
var add_value = function(feature) {
   var img1 = ee.Image(feature.get(&#39;bestImage&#39;)).select(&#39;Percent_Tree_Cover&#39;);
   var point = feature.geometry();
   var pixel_Value = img1.sample({region: point, scale: 10, tileScale: 15, dropNulls: false});
   return feature.set({Percent_Tree_Cover: pixel_Value.first().get(&#39;Percent_Tree_Cover&#39;)});
};

var DataFinal = Data_match.map(add_value);

// Remove points that were outside the MODIS image footprint (e.g., in the ocean)
var DataFinal = DataFinal.filter(ee.Filter.neq(&#39;Percent_Tree_Cover&#39;, null))

// Check the final number of presence locations for analysis
print(&#39;Presence data size:&#39;, DataFinal.size());
//print(DataFinal.limit(2)) //Activate to visualize results
Map.addLayer(DataFinal, {color:&#39;red&#39;}, &#39;Presence&#39;, 1)  //Add points to the map
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig12.png&#34;
alt=&#34;Figure 12. Cebus capucinus presence locations.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 12. &lt;em&gt;Cebus capucinus&lt;/em&gt;
presence locations.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After all this process, we end up with 330 occurrence records.&lt;/p&gt;
&lt;h3 id=&#34;model-fit&#34;&gt;Model fit&lt;/h3&gt;
&lt;p&gt;The next step is to define an area to create pseudo-absences. We will
first create an image where presence records are marked to avoid
creating pseudo-absences in the same locations where we have known
presences.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////
// Section 5 - Defining area for creation of pseudo-absence points
///////////////////////////////////////////////////////////////////

// Make an image out of the presence locations to mask from the area to generate pseudo-absences. This will impede having presence and pseudo-absences in a 1km around the presence location.
var mask = DataFinal
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(1000)).mask().neq(1).selfMask();

var AreaForPA = mask.updateMask(watermask).clip(AOI);
Map.addLayer(AreaForPA)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a matching pseudo-absence location for each occurrence
location, we generated a random pseudo-absence point within a 100 km
buffer, extracting the percent tree cover from the VCF image that
corresponded to the time period of the occurrence point. We repeated
this process five times, resulting in five balanced datasets for each
iteration, each with a different set of pseudo-absences.&lt;/p&gt;
&lt;p&gt;To do this, we defined a function that creates a random point within a
100 km buffer and extracts the pixel value of the percent tree cover
image. We then merge the presence data with the pseudo-absences.
Finally, we extract the value for the other predictors, elevation, mean
annual temperature, annual precipitation. Each of the 5 resulting
training data sets is used to fit a random forest classifier.&lt;/p&gt;
&lt;p&gt;We pack all this into a function that we can map across a list of random
seeds for each iteration of model fitting and validation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////
// Section 6 - Model fit
///////////////////////////

// Define SDM function
function SDM(x) {
    // Presence points
    var PresencePoints = DataFinal.map(function(feature){return feature.set(&#39;PresAbs&#39;, 1)});
    var PresencePoints = predictors.sampleRegions({collection: PresencePoints, properties: [&#39;PresAbs&#39;, &#39;Percent_Tree_Cover&#39;], scale: 250, tileScale: 4});
    var npoints = PresencePoints.size();
    
    // Pseudoabsences
    var PseudoAbs = DataFinal.map(function(feature){
                        var img1 = ee.Image(feature.get(&#39;bestImage&#39;)).select(&#39;Percent_Tree_Cover&#39;);
                        var pointbuff = feature.geometry().buffer(100000);
                        var randpoints = AreaForPA.sample({region: pointbuff, scale: 10, numPixels: 30, seed:x, geometries: true, tileScale: 15, dropNulls: true}); // If error appears on drop null, increase the number of pixels
                        var PTC = img1.sampleRegions({collection: randpoints, scale: 10, tileScale: 16, geometries: true});
                        return PTC.first();
                      });
    var PseudoAbs = PseudoAbs.map(function(feature){return feature.set(&#39;PresAbs&#39;, 0)});
    var PseudoAbsPoints = predictors.sampleRegions({collection: PseudoAbs, properties: [&#39;PresAbs&#39;, &#39;Percent_Tree_Cover&#39;], scale: 250, tileScale: 4, geometries: true});

    // Merge points
    var trainingData = PresencePoints.merge(PseudoAbsPoints);

    // Classify using Random Forest
    var rfClassifier = ee.Classifier.smileRandomForest(500).setOutputMode(&#39;PROBABILITY&#39;).train(trainingData, &#39;PresAbs&#39;, bands); 
   
    return ee.List([rfClassifier, trainingData]);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now fit the models. We use a pre-defined list of random numbers
for reproducibility of results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define number of repetitions
var numiter = 5;

// Fit SDM
var results = ee.List([81,96,57,22,2]).map(SDM);

// Extract results from list
var results = results.flatten();
//print(results) //Activate this line to visualize all elements
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;In this example we are using 5 iterations. Adding more iterations
could cause a memory limit issue. If more iterations are desired, then
instead of adding resulting predictions to the interactive map, you
can directly export results to Google Drive (batch mode) to prevent
the computation from reaching the memory limit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;accuracy-assessment&#34;&gt;Accuracy assessment&lt;/h3&gt;
&lt;p&gt;Because model predictions will vary each year due to changes in
underlying predictor variables, we withheld data from 2019 for model
validation. We use 2019 for out-of-sample model validation as this year
had a large number of occurrence records and was the last year the MODIS
VCF was available in GEE. We assume that if the model predicts well for
withheld data in 2019, then the model likely performed well in other
years and is useful for making predictions and studying change over
time. We used 125 occurrence records and a set of 125 pseudo-absences
randomly created across the study area (within a 100 km buffer around
each occurrence location) to estimate the AUC-RP for each of the five
individual model predictions using the percentage of tree cover for
2019, together with mean annual temperature, annual precipitation and
elevation as the predictor variables.&lt;/p&gt;
&lt;p&gt;We need to define the AUC-RP functions as we did in the previous
example.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////
// Section 7 - Accuracy assessment
////////////////////////////////////

// Define functions to estimate sensitivity, specificity and precision.
function getAcc(img,TP){
  var Pr_Prob_Vals = img.sampleRegions({collection: TP, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16});
  var seq = ee.List.sequence({start: 0, end: 1, count: 25});
  return ee.FeatureCollection(seq.map(function(cutoff) {
  var Pres = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,1);
  // true-positive and true-positive rate, sensitivity  
  var TP =  ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var TPR = TP.divide(Pres.size());
  var Abs = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,0);
  // true-negative rate, specificity  
  var TNR = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size()).divide(Abs.size());
  // false-positive and false-positive rate
  var FP = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var FPR = FP.divide(Abs.size());
  // precision
  var Precision = TP.divide(TP.add(FP));
  return ee.Feature(null,{TPR:TPR, FPR:FPR, Precision:Precision});
  }));
}

// Calculate AUC of Precision Recall Curve
function getAUCPR(x){
  var X = ee.Array(x.aggregate_array(&#39;TPR&#39;));
  var Y = ee.Array(x.aggregate_array(&#39;Precision&#39;)); 
  var X1 = X.slice(0,1).subtract(X.slice(0,0,-1));
  var Y1 = Y.slice(0,1).add(Y.slice(0,0,-1));
  return X1.multiply(Y1).multiply(0.5).reduce(&#39;sum&#39;,[0]).abs().toList().get(0);
}

// Extract all model classifiers
var classifiers = ee.List.sequence(0,ee.Number(numiter).multiply(2).subtract(1),2)
                  .map(function(x){return results.get(x)});
                  
// We will use 2019 data to validate the model
var Data19 = RemoveDuplicates(Data.filter(ee.Filter.rangeContains(&#39;Date&#39;, &#39;2019-01-01&#39;, &#39;2019-12-31&#39;)));
var Presence19 = Data19.map(function(feature){return feature.set(&#39;PresAbs&#39;, 1)});
Map.addLayer(Presence19, {color:&#39;red&#39;}, &#39;Presence 2019&#39;, 1)  //Add points to the map

// Make an image out of the presence locations to mask from the area to generate pseudoabsences. This will impede having presence and pseudoabsences near the same pixel.
var mask2 = Presence19
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(1000));

// Limit pseudo-absences to a buffer around presence points. 
var buffer = 100000; // E.g., 100 km.
var AreaForPA2 = Data19.geometry().buffer(buffer);
var AreaForPA2 = mask2.mask().clip(AreaForPA2).updateMask(watermask).clip(AOI);

// Create random pseudo-absences
var Abs19 = AreaForPA2.sample({region: AOI, scale: GrainSize, numPixels: 1000, geometries: true}); //Because many points will on the ocean, we need to create more than needed.
var Abs19 = Abs19.randomColumn().sort(&#39;random&#39;).limit(Presence19.size()); // We keep the same amount of pseudoabsences than presences
var Abs19 = Abs19.map(function(feature){
    return feature.set(&#39;PresAbs&#39;, 0);
    });
print(&#39;Presence 2019&#39;, Presence19.size());
print(&#39;Pseudo-absences 2019&#39;, Abs19.size());

// Merge presence and pseudo-absences
var testingdata2019 = Presence19.merge(Abs19);

// Create the predictor variables for 2019
var mod2019 = MODIS.filterDate(&#39;2019-01-01&#39;, &#39;2019-12-31&#39;).select([&#39;Percent_Tree_Cover&#39;]).first();
var pred19 = predictors.addBands(mod2019);

// Predict HSI for 2019 and estimate ROC-AUC
function accuracy(x){
  var Classifier = classifiers.get(x);
  var HSM = pred19.classify(Classifier);
  var Acc = getAcc(HSM, testingdata2019);
  return getAUCPR(Acc);
}

var AUCPRs = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(accuracy);
print(&#39;AUC of the precision-recall:&#39;, AUCPRs);
print(&#39;Mean AUC of the precision-recall&#39;, AUCPRs.reduce(ee.Reducer.mean()));

// Function to extract other metrics
function getMetrics(x){
  var Classifier = classifiers.get(x);
  var HSM = pred19.classify(Classifier);
  var Acc = getAcc(HSM, testingdata2019);
  return Acc.sort({property:&#39;SUMSS&#39;,ascending:false}).first();
}

// Extract threshold values
var Metrics = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(getMetrics);
print(&#39;Sensitivity:&#39;, ee.FeatureCollection(Metrics).aggregate_array(&amp;quot;TPR&amp;quot;));
print(&#39;Specificity:&#39;, ee.FeatureCollection(Metrics).aggregate_array(&amp;quot;TNR&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig13.png&#34;
alt=&#34;Figure 13. 2019 presence data used for model validation.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 13. 2019 presence data used for
model validation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;The individual model accuracy is:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Random Forest AUC-PR&lt;/th&gt;
&lt;th&gt;Sensitivity&lt;/th&gt;
&lt;th&gt;Specificity&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 1&lt;/td&gt;
&lt;td&gt;0.89&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 2&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.76&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 3&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Run 4&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Run 5&lt;/td&gt;
&lt;td&gt;0.72&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;strong&gt;Mean&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.81&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.78&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.84&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;model-predictions&#34;&gt;Model predictions&lt;/h3&gt;
&lt;p&gt;After validating our models, we can predict suitable habitat across all
years to reflect changes over time in the Terra MODIS VCF image
composite. We need to define a function that adds the Terra MODIS VCF of
each year to the mean annual temperature, annual precipitation and
elevations variables and predicts the habitat suitability for each
single model. We then obtain the median habitat suitability per pixel as
the final prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////
// Section 8 - Predictions
///////////////////////////

var PTC = ee.ImageCollection(&amp;quot;MODIS/006/MOD44B&amp;quot;).select([&#39;Percent_Tree_Cover&#39;]);
var HSI = PTC.map(function(img){
  var predimg = predictors.addBands(img); 
  return ee.ImageCollection.fromImages(ee.List.sequence(0,ee.Number(numiter).subtract(1),1)
  .map(function prediction(x){
        var Classifier = classifiers.get(x);
        return predimg.classify(Classifier)}))
  .mean().copyProperties(img, [&#39;system:time_start&#39;]);
  });

//print(HSI);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot some outputs. We need to convert the resulting image
collection into a list to display each year. Here we display years 2000
and 2019.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define visualization parameters.
var visParams = {
  min: 0,
  max: 0.9,
  palette: [&amp;quot;#440154FF&amp;quot;,&amp;quot;#482677FF&amp;quot;,&amp;quot;#404788FF&amp;quot;,&amp;quot;#33638DFF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,
  &amp;quot;#1F968BFF&amp;quot;,&amp;quot;#29AF7FFF&amp;quot;,&amp;quot;#55C667FF&amp;quot;,&amp;quot;#95D840FF&amp;quot;,&amp;quot;#DCE319FF&amp;quot;],
};

var HSIlist = HSI.toList(20);

// Add final habitat suitability layer to the map. Use the function get to select specific years. 0 is the first element in the list.
Map.addLayer(ee.Image(HSIlist.get(0)), visParams, &#39;Habitat Suitability - 2000&#39;);
Map.addLayer(ee.Image(HSIlist.get(19)), visParams, &#39;Habitat Suitability - 2019&#39;);

// Create legend for habitat suitability map.
var legend = ui.Panel({style: {position: &#39;bottom-left&#39;, padding: &#39;8px 15px&#39;}});

legend.add(ui.Label({
  value: &amp;quot;Habitat suitability&amp;quot;,
  style: {fontWeight: &#39;bold&#39;, fontSize: &#39;18px&#39;, margin: &#39;0 0 4px 0&#39;, padding: &#39;0px&#39;}
}));

var colors = [&amp;quot;#DCE319FF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,&amp;quot;#440154FF&amp;quot;];
var names = [&#39;High&#39;, &#39;Medium&#39;,&#39;Low&#39;];
var entry;
for (var x = 0; x&amp;lt;3; x++){
  entry = [
    ui.Label({style:{color:colors[x],margin: &#39;0 0 4px 0&#39;}, value:&#39;██&#39;}),
    ui.Label({value: names[x],style: {margin: &#39;0 0 4px 4px&#39;}})
  ];
  legend.add(ui.Panel(entry, ui.Panel.Layout.Flow(&#39;horizontal&#39;)));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig14.png&#34;
alt=&#34;Figure 14. Predicted Cebus capucinus habitat suitability for 2000.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 14. Predicted &lt;em&gt;Cebus
capucinus&lt;/em&gt; habitat suitability for 2000.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig15.png&#34;
alt=&#34;Figure 15. Predicted Cebus capucinus habitat suitability for 2019.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 15. Predicted &lt;em&gt;Cebus
capucinus&lt;/em&gt; habitat suitability for 2019.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;habitat-suitability-change-assessment&#34;&gt;Habitat suitability change assessment&lt;/h2&gt;
&lt;p&gt;To assess habitat suitability change across time, we fit a pixel-based
linear regression. We applied the &lt;code&gt;formaTrend()&lt;/code&gt; function to the image
collection containing habitat suitability predictions for each year of
our 20-year study. This function fits a pixel-based linear regression to
identify areas where habitat suitability increased or decreased across
the 20-year period. The output is an image with four bands, two of which
are of particular interest, 1) the slope of the linear regression and 2)
a t-test statistic on the significance of the slope. Finally, we create
an output map showing the slope of the linear regression at each pixel
while using the t-test statistic to mask out any pixels with
non-significant trends. Positive values indicate areas that increased in
habitat suitability over time and negative values indicate areas that
decreased in habitat suitability.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;////////////////////////////////////////////////////////////////////////////
// Section 9 - Fit linear regression to 20 years of HSI values at each pixel
////////////////////////////////////////////////////////////////////////////

// Use the formaTrend function to fit a linear regression to the habitat suitability collection.
var TempTrend = HSI.formaTrend();
//print(TempTrend);

// Display pixels with significant trends at an alpha = 0.05.
// To mask out pixels with non-significant trends, we need to find those pixels with 
// a two tailed t-test statistic larger or lower than the threshold value for the specific degrees of freedom.
// In our case we have 20 years, so 19 degrees of freedom, at an alpha of 0.05.  This gives us critical values for the t-test statistic of 2.093 and -2.093

var negative = TempTrend.select(&#39;long-tstat&#39;).lt(-2.093);
var possitive = TempTrend.select(&#39;long-tstat&#39;).gt(2.093);
var sign = negative.add(possitive);

Map.addLayer(TempTrend.select(&#39;long-trend&#39;).updateMask(sign), {
  min: -0.02,
  max: 0.02,
  palette: [&#39;ff0000&#39;,&#39;e96666&#39;,&#39;d6aeae&#39;,&#39;f1f1f1&#39;,&#39;c8ccff&#39;,&#39;6e8dff&#39;,&#39;000dad&#39;]
}, &#39;HSI long-trend&#39;);

// Add regression slope legend to the map
legend.add(ui.Label({
  value: &amp;quot;Regression slope&amp;quot;,
  style: {fontWeight: &#39;bold&#39;, fontSize: &#39;18px&#39;, margin: &#39;0 0 4px 0&#39;, padding: &#39;0px&#39;}
}));

var colors = [&#39;ff0000&#39;,&#39;f1f1f1&#39;,&#39;000dad&#39;];
var names = [&#39;-0.02&#39;, &#39;0&#39;,&#39;0.02&#39;];
var entry;
for (var x = 0; x&amp;lt;3; x++){
  entry = [
    ui.Label({style:{color:colors[x],margin: &#39;0 0 4px 0&#39;}, value:&#39;██&#39;}),
    ui.Label({value: names[x],style: {margin: &#39;0 0 4px 4px&#39;}})
  ];
  legend.add(ui.Panel(entry, ui.Panel.Layout.Flow(&#39;horizontal&#39;)));
}

Map.add(legend);
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig16.png&#34;
alt=&#34;Figure 16. Regression slope values show areas with positive (blue) to negative (red) trends in habitat suitability change between 2000 and 2019 for Cebus capucinus. Only pixels with significant trends are shown.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 16. Regression slope values show
areas with positive (blue) to negative (red) trends in habitat
suitability change between 2000 and 2019 for Cebus capucinus. Only
pixels with significant trends are shown.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is also possible to create a gif animation showing the change of
habitat suitability across years.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Create RGB visualization images for use as animation frames.
var rgbVis = HSM.map(function(img) {
  var scale = 250;
  return img.visualize(visParams);
});

// Define GIF visualization parameters.
var gifParams = {
  &#39;region&#39;: geometry2,
  &#39;dimensions&#39;: 500,
  &#39;crs&#39;: &#39;EPSG:3857&#39;,
  &#39;framesPerSecond&#39;: 2
};

//Print the GIF URL to the console.
print(rgbVis.getVideoThumbURL(gifParams));
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&#34;./Figures/Fig17.gif&#34;
alt=&#34;Figure 17. Cebus capucinus habitat suitability change across years.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 17. &lt;em&gt;Cebus capucinus&lt;/em&gt;
habitat suitability change across years.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As before, we can export results to Google Drive.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//////////////////////////////////////////////////////
// Section 10 - Export final map
//////////////////////////////////////////////////////

// Export final model to drive
Export.image.toDrive({
   image: TempTrend, //Image to export
   description: &#39;Cebuscapucinus&#39;, //File name
   scale: GrainSize, // Spatial resolution
   maxPixels: 1e10,
   region: AOI //Area of interest
 });
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;case-study-3-modelling-species-distribution-at-high-spatial-resolution-using-unclassified-satellite-images-as-predictor-variables&#34;&gt;Case Study 3: Modelling species distribution at high spatial resolution using unclassified satellite images as predictor variables&lt;/h2&gt;
&lt;p&gt;In this third case study, we demonstrate the implementation of the SDM
workflow to predict habitat suitability based on presence records and
unclassified satellite data. The code workflow is very similar to case
study 1, and differs primarily in the predictor variables used and the
fact that this analysis has to be run on batch mode to avoid memory
limits. Therefore, all results have to be exported to Google Drive. Note
that it is possible to run a couple of iterations to quickly visualize
results on the interactive map before exporting the model with a larger
number of iterations.&lt;/p&gt;
&lt;h3 id=&#34;loading-data-and-defining-grid-size-and-extent&#34;&gt;Loading data and defining grid size and extent&lt;/h3&gt;
&lt;p&gt;For this example, we obtained the eBird observation dataset for
&lt;em&gt;Hylocichla mustelina&lt;/em&gt; from GBIF (GBIF.org (12 November 2021);
&lt;a href=&#34;https://doi.org/10.15468/dl.hpjfup&#34;&gt;https://doi.org/10.15468/dl.hpjfup&lt;/a&gt;) for the month of June (middle of
breeding season when observation of migrants is more unlikely) for the
years 2018, 2019 and 2020. The dataset was first ingested into GEE as an
asset.&lt;/p&gt;
&lt;p&gt;In the first section we load the data and set the spatial resolution of
the analysis to 90 m.&lt;/p&gt;
&lt;p&gt;We define the extent of the analysis to be the eastern continental USA
(4,606,284 km2), limiting the extent to the westernmost observation in
the dataset (longitude: -103 degrees). To do this we used the Large
Scale International Boundary (LSIB) dataset and filtered out the USA
boundary. We used the &lt;code&gt;intersection()&lt;/code&gt; function to limit the geometry to
the -103 degrees of longitude.&lt;/p&gt;
&lt;p&gt;We rarified the original 99,939 observations to keep one per pixel,
resulting in 34,880 observations for modelling.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 1 - Load species data, AOI, and remove duplicates
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Load presence data
var DataRaw = ee.FeatureCollection(&#39;users/ramirocrego84/WoodThrush&#39;);
//print(&#39;Original data size:&#39;, DataRaw.size());

//Define the AOI
var USA = ee.FeatureCollection(&amp;quot;USDOS/LSIB_SIMPLE/2017&amp;quot;).filter(ee.Filter.eq(&#39;country_co&#39;,&#39;US&#39;));
var AOI = USA.filter(ee.Filter.eq(&#39;country_na&#39;,&#39;United States&#39;)).limit(2).union();
var AOI = ee.Feature(AOI.first()).geometry();
var AOI = AOI.intersection(ee.Geometry.Polygon(
        [[[-103, 49],
          [-103, 23],
          [-64, 23],
          [-64, 49]]]),1000)
          
//var Area = AOI.area()
//var AreaSqKm = ee.Number(Area).divide(1e6).round() //This calculates the area of the AOI in km2
//print(AreaSqKm)

// Define spatial resolution to work with (m)
var GrainSize = 90;

function RemoveDuplicates(data){
  var randomraster = ee.Image.random().reproject(&#39;EPSG:4326&#39;, null, GrainSize);
  var randpointvals = randomraster.sampleRegions({collection:ee.FeatureCollection(data), scale: 10, geometries: true});
  return randpointvals.distinct(&#39;random&#39;);
}

DataRaw = DataRaw.filter(ee.Filter.bounds(AOI));
var Data = RemoveDuplicates(DataRaw)
print(&#39;Final data size:&#39;, Data.size());

// Add border of study area to the map
var outline = ee.Image().byte().paint({
  featureCollection: AOI, color: 1, width: 3});
Map.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);

// Center map to the center of the screen
Map.centerObject(AOI,3); //Number indicates the zoom level

// Visualize presence points on the map
Map.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;predictor-variables-1&#34;&gt;Predictor variables&lt;/h3&gt;
&lt;p&gt;In the next step we prepare the predictor variables for analysis.&lt;/p&gt;
&lt;p&gt;We modeled &lt;em&gt;Hylocichla mustelina&lt;/em&gt; habitat suitability using
atmospherically corrected Landsat 8 surface reflectance (SR) collection
2, Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band
Synthetic Aperture Radar (SAR) HH and HV polarization datasets, and
temperature products as predictor variables.&lt;/p&gt;
&lt;p&gt;We first loaded and filtered the Landsat 8 SR product to keep only
images between the months of April and August and for 2018, 2019, and
2020. We included those months to obtain a cloud-free mosaic for the
entire study area.&lt;/p&gt;
&lt;p&gt;For each image, we masked bad quality pixels with clouds, cloud shadows
and saturated pixels and rescaled pixel values with the appropriate
scaling factors using a predefined &lt;code&gt;maskL8sr()&lt;/code&gt; mask function available
in GEE.&lt;/p&gt;
&lt;p&gt;We selected the blue, red, green, near-infrared and shortwave infrared 1
bands for analysis (30 m spatial resolution).&lt;/p&gt;
&lt;p&gt;For each image we also calculated NDVI using the
&lt;code&gt;normalizedDifference()&lt;/code&gt; function available in GEE.&lt;/p&gt;
&lt;p&gt;We finally created a mosaic of the entire study area by calculating the
median value from the time series for each pixel across all Landsat
image bands.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 2 - Selecting Predictor Variables
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Load and process landsat data
var l8sr = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;)
                .filterBounds(AOI)
                //.filterMetadata(&#39;CLOUD_COVER&#39;, &#39;less_than&#39;, 100)
                .filterDate(&#39;2018-01-01&#39;, &#39;2020-12-31&#39;)
                .filter(ee.Filter.calendarRange({start:5, end:8, field:&#39;month&#39;}));
                

// Function to mask clouds
function maskL8sr(image) {
  // Bit 0 - Fill
  // Bit 1 - Dilated Cloud
  // Bit 2 - Cirrus
  // Bit 3 - Cloud
  // Bit 4 - Cloud Shadow
  var qaMask = image.select(&#39;QA_PIXEL&#39;).bitwiseAnd(parseInt(&#39;11111&#39;, 2)).eq(0);
  var saturationMask = image.select(&#39;QA_RADSAT&#39;).eq(0);

  // Apply the scaling factors to the appropriate bands.
  var opticalBands = image.select(&#39;SR_B.&#39;).multiply(0.0000275).add(-0.2);
  var thermalBands = image.select(&#39;ST_B.*&#39;).multiply(0.00341802).add(149.0);

  // Replace the original bands with the scaled ones and apply the masks.
  return image.addBands(opticalBands, null, true)
      .addBands(thermalBands, null, true)
      .updateMask(qaMask)
      .updateMask(saturationMask);
}

// Function to rename Landsat 8 bands for cross-Landsat compatibility &amp;amp; rescale
var renameBandsL8 = function(image) {
    var imgNewBands = image.select([&#39;SR_B2&#39;, &#39;SR_B3&#39;, &#39;SR_B4&#39;, &#39;SR_B5&#39;, &#39;SR_B6&#39;]).rename([&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;nir&#39;, &#39;swir1&#39;]);
    return imgNewBands.copyProperties(image,[&#39;system:time_start&#39;]);
};

// Function to compute NDVI
var addNDVI = function(image) {
    var ndvi = image.normalizedDifference([&#39;nir&#39;, &#39;red&#39;]).rename(&#39;ndvi&#39;);
    return image.addBands(ndvi);
};

// Apply functions
var l8sr8nocld = l8sr.map(maskL8sr)
                      .map(renameBandsL8)
                      .map(addNDVI);

// Create a median composite image (takes the median value from each band across all available images)
var l8srcompmedian = l8sr8nocld.median();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also used the 2018, 2019, and 2020 global mosaics of Advanced Land
Observing Satellite (ALOS) Phased Arrayed L-band Synthetic Aperture
Radar (SAR) HH and HV polarization datasets (25 m spatial resolution).
For each pixel of the two bands, we also obtained the median value among
the three years.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Load Radar Data
var radar = ee.ImageCollection(&#39;JAXA/ALOS/PALSAR/YEARLY/SAR&#39;)
                  .filterDate(&#39;2018-01-01&#39;, &#39;2020-12-31&#39;);
                
var radar = radar.select([&#39;HH&#39;,&#39;HV&#39;]).median();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, to account for the variation in temperature across the breeding
range we included the mean temperature for the month of June (1 km
spatial resolution).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Load WorldClim BIO Variables (a multiband image) from the data catalog
var juneTemp = ee.ImageCollection(&#39;WORLDCLIM/V1/MONTHLY&#39;).filter(ee.Filter.eq(&#39;month&#39;,6)).first().select(&#39;tavg&#39;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then combined all predictor variables into one multiband image.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Combine bands into a single multi-band image
var predictors = l8srcompmedian.addBands(radar).addBands(juneTemp);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we masked out all pixels containing permanent water using the
global water surface product.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Mask water pixels from the predictor variable image collection
var watermask = ee.Image(&amp;quot;JRC/GSW1_3/GlobalSurfaceWater&amp;quot;).select(&#39;max_extent&#39;).eq(0);
var predictors = predictors.updateMask(watermask).clip(AOI);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You need to display the resulting images to make sure you have a
complete mosaic for the study area with minimal gaps due to clouds or
bad quality pixels. In our case study, we had to modify the filter
parameters until the complete Lansdat mosaic was obtained without
missing data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Get band names
var bands = predictors.bandNames();

// Display layers on the map
Map.addLayer(predictors, {bands:[&#39;ndvi&#39;], min: 0, max: 1,  palette: [&#39;white&#39;,&#39;yellow&#39;,&#39;green&#39;]}, &#39;NDVI&#39;, 0);
Map.addLayer(predictors, {bands: [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;], gamma: 1, min: 0, max: 0.2, opacity: 1}, &#39;L8 SR True color&#39;,0);
Map.addLayer(predictors, {bands:[&#39;tavg&#39;], min: 0, max: 300,  palette: [&#39;blue&#39;, &#39;purple&#39;, &#39;cyan&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;]}, &#39;Mean T June&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;defining-spatial-blocks-for-model-fitting-and-cross-validation&#34;&gt;Defining spatial blocks for model fitting and cross validation&lt;/h3&gt;
&lt;p&gt;For this analysis, we implemented a two-step environmental profiling
technique to generate pseudo-absences. We performed a k-means clustering
based on Euclidean distance for a subset of 1,000 randomly selected
occurrences to restrict the area for the creation of pseudo-absences to
pixels more dissimilar to the environmental profile of the occurrence
data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 3 - Defining spatial blocks for model fitting and cross validation
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Make an image out of the presence locations. The pixels where we have presence locations will be removed from the area to generate pseudo-absences.
// This will prevent having presence and pseudo-absences in the same pixel. 
var mask = Data
  .reduceToImage({
    properties: [&#39;random&#39;],
    reducer: ee.Reducer.first()
}).reproject(&#39;EPSG:4326&#39;, null, ee.Number(GrainSize)).mask().neq(1).selfMask();

// Extract local covariate values from multiband predictor image at presence points
var PixelVals = predictors.sampleRegions({collection: Data.randomColumn({seed:5}).sort(&#39;random&#39;).limit(1000), properties: [], scale: 30, tileScale: 8});
// Instantiate the clusterer and train it.
var clusterer = ee.Clusterer.wekaKMeans({nClusters:2, distanceFunction:&amp;quot;Euclidean&amp;quot;, fast: true}).train(PixelVals);
// Cluster the input using the trained clusterer.
var ClResult = predictors.cluster(clusterer);

// Retain cluster class mode dissimilar to occurrence data
var ClMask = ClResult.select([&#39;cluster&#39;]).eq(1);
          
var AreaForPA =  mask.updateMask(ClMask);

Map.addLayer(AreaForPA, {},&#39;Area to create pseudo-absences&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We implemented a repeated (5-times) spatial block cross validation
technique, randomly partitioning 200 x 200 km spatial blocks for model
training (70%) and validation (30%) while randomly generating
pseudo-absences at each iteration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Define a function to create a grid over AOI
function makeGrid(Geometry, scale) {
  // pixelLonLat returns an image with each pixel labeled with longitude and
  // latitude values.
  var lonLat = ee.Image.pixelLonLat();
  // Select the longitude and latitude bands, multiply by a large number then
  // truncate them to integers.
  var lonGrid = lonLat
    .select(&#39;longitude&#39;)
    .multiply(100000)
    .toInt();
  var latGrid = lonLat
    .select(&#39;latitude&#39;)
    .multiply(100000)
    .toInt();
  return lonGrid
    .multiply(latGrid)
    .reduceToVectors({
      geometry: Geometry,
      scale: scale,
      geometryType: &#39;polygon&#39;,
    });
}
// Create grid and remove cells outside AOI
var Scale = 200000; // Set range in m to create spatial blocks
var Grid = makeGrid(AOI, Scale);
Map.addLayer(Grid, {},&#39;Grid for spatail block cross validation&#39;, 0);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;fitting-sdm-models&#34;&gt;Fitting SDM models&lt;/h3&gt;
&lt;p&gt;For the analysis, we generated an equal number of pseudo-absences as
occurrence data for each of the 5 datasets. To ensure an equal number of
pseudo-absences, we first generated an arbitrarily large sample of 50000
random points to allow for pixels to be discarded that fell outside land
area (withing the ocean or lakes) within the spatial blocks selected for
model training. After dropping these masked pixels, we then limited the
number of pseudo-absence points to match the number of presence points
for a given dataset. We then fit a random forest model (500 trees) to
each individual training dataset and then averaged model outputs to
calculate the mean habitat suitability of the five iterations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 4 - Fitting SDM models
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Define SDM function
// Activate the desired classifier, random forest or gradient boosting. 
// Note that other algorithms are available in GEE. See ee.Classifiers on the documentation for more information.

function SDM(x) {
    var Seed = ee.Number(x);
    // Randomly split blocks for training and validation
    var GRID = ee.FeatureCollection(Grid).randomColumn({seed:Seed}).sort(&#39;random&#39;);
    var TrainingGrid = GRID.filter(ee.Filter.lt(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TestingGrid = GRID.filter(ee.Filter.gte(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    // Presence
    var PresencePoints = ee.FeatureCollection(Data);
    PresencePoints = PresencePoints.map(function(feature){return feature.set(&#39;PresAbs&#39;, 1)});
    var TrPresencePoints = PresencePoints.filter(ee.Filter.bounds(TrainingGrid));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TePresencePoints = PresencePoints.filter(ee.Filter.bounds(TestingGrid));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    // Pseudo-absences
    var TrPseudoAbsPoints = AreaForPA.sample({region: TrainingGrid, scale: GrainSize, numPixels: 50000, seed:Seed, geometries: true, tileScale: 16}); // We add extra points to account for those points that land in masked areas of the raster and are discarded. This ensures a balanced presence/pseudo-absence data set
    TrPseudoAbsPoints = TrPseudoAbsPoints.randomColumn().sort(&#39;random&#39;).limit(ee.Number(TrPresencePoints.size())); //Randomly retain the same number of pseudo-absences as presence data 
    TrPseudoAbsPoints = TrPseudoAbsPoints.map(function(feature){
        return feature.set(&#39;PresAbs&#39;, 0);
        });
 
    var TePseudoAbsPoints = AreaForPA.sample({region: TestingGrid, scale: GrainSize, numPixels: TePresencePoints.size(), seed:Seed, geometries: true, tileScale: 16}); // We add extra points to account for those points that land in masked areas of the raster and are discarded. This ensures a balanced presence/pseudo-absence data set
    TePseudoAbsPoints = TePseudoAbsPoints.randomColumn().sort(&#39;random&#39;).limit(ee.Number(TePresencePoints.size())); //Randomly retain the same number of pseudo-absences as presence data 
    TePseudoAbsPoints = TePseudoAbsPoints.map(function(feature){
        return feature.set(&#39;PresAbs&#39;, 0);
        });

    // Merge points
    var trainingPartition = TrPresencePoints.merge(TrPseudoAbsPoints);
    var testingPartition = TePresencePoints.merge(TePseudoAbsPoints);

    // Extract local covariate values from multiband predictor image at training points
    var trainPixelVals = predictors.sampleRegions({collection: trainingPartition, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16, geometries: false});

    // Classify using random forest
    var Classifier = ee.Classifier.smileRandomForest({
       numberOfTrees: 500, //The number of decision trees to create.
       variablesPerSplit: null, //The number of variables per split. If unspecified, uses the square root of the number of variables.
       minLeafPopulation: 10,//Only create nodes whose training set contains at least this many points. Integer, default: 1
       bagFraction: 0.5,//The fraction of input to bag per tree. Default: 0.5.
       maxNodes: null,//The maximum number of leaf nodes in each tree. If unspecified, defaults to no limit.
       seed: Seed//The randomization seed.
      });
  
    // Presence probability 
    var ClassifierPr = Classifier.setOutputMode(&#39;PROBABILITY&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    var ClassifiedImgPr = predictors.select(bands).classify(ClassifierPr);

    // Binary presence/absence map
    //var ClassifierBin = Classifier.setOutputMode(&#39;CLASSIFICATION&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    //var ClassifiedImgBin = predictors.select(bands).classify(ClassifierBin);
   
    return ee.List([ClassifiedImgPr, testingPartition]);
}


// Define partition for training and testing data
var split = 0.70;  // The proportion of the blocks used to select training data

// Define number of repetitions
var numiter = 5;

// Fit SDM 
var results = ee.List([55,7,25,65,23]).map(SDM);

// Extract results from list
var results = results.flatten();

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 5 - Extracting and displaying model prediction results
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Habitat suitability

// Extract all model predictions
var images = ee.List.sequence(0,ee.Number(numiter).multiply(2).subtract(1),2).map(function(x){
  return results.get(x)});

// Calculate mean of all individual model runs
var ModelAverage = ee.ImageCollection.fromImages(images).mean();
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;accuracy-assessment-1&#34;&gt;Accuracy assessment&lt;/h3&gt;
&lt;p&gt;We assessed model accuracy by calculating the AUC-PR, sensitivity, and
specificity on the validation dataset for each model iteration. To
obtain a binary potential distribution map, we used the mean threshold
that maximized the sum of sensitivity and specificity of each of the
individual model predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 6 - Accuracy assessment
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Extract testing/validation datasets
var TestingDatasets = ee.List.sequence(1,ee.Number(numiter).multiply(2).subtract(1),2).map(function(x){
                      return results.get(x)});

// Double check that you have a satisfactory number of points for model validation
// print(&#39;Number of presence and pseudo-absence points for model validation&#39;, ee.List.sequence(0,ee.Number(numiter).subtract(1),1)
// .map(function(x){
//   return ee.List([ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,1)).size(),
//         ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,0)).size()]);
// })
// );

// Define functions to estimate sensitivity, specificity and precision.
function getAcc(img,TP){
  var Pr_Prob_Vals = img.sampleRegions({collection: TP, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16});
  var seq = ee.List.sequence({start: 0, end: 1, count: 25});
  return ee.FeatureCollection(seq.map(function(cutoff) {
  var Pres = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,1);
  // true-positive and true-positive rate, sensitivity  
  var TP =  ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var TPR = TP.divide(Pres.size());
  var Abs = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,0);
  // false-negative
  var FN = ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  // true-negative and true-negative rate, specificity  
  var TN = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  var TNR = TN.divide(Abs.size());
  // false-positive and false-positive rate
  var FP = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var FPR = FP.divide(Abs.size());
  // precision
  var Precision = TP.divide(TP.add(FP));
  // sum of sensitivity and specificity
  var SUMSS = TPR.add(TNR);
  return ee.Feature(null,{cutoff: cutoff, TP:TP, TN:TN, FP:FP, FN:FN, TPR:TPR, TNR:TNR, FPR:FPR, Precision:Precision, SUMSS:SUMSS});
  }));
}


// Calculate AUC of Precision Recall Curve

function getAUCPR(roc){
  var X = ee.Array(roc.aggregate_array(&#39;TPR&#39;));
  var Y = ee.Array(roc.aggregate_array(&#39;Precision&#39;)); 
  var X1 = X.slice(0,1).subtract(X.slice(0,0,-1));
  var Y1 = Y.slice(0,1).add(Y.slice(0,0,-1));
  return X1.multiply(Y1).multiply(0.5).reduce(&#39;sum&#39;,[0]).abs().toList().get(0);
}

function AUCPRaccuracy(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return getAUCPR(Acc);
}

var AUCPRs = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(AUCPRaccuracy);

// Function to extract other metrics
function getMetrics(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return Acc.sort({property:&#39;SUMSS&#39;,ascending:false}).first();
}

// Extract sensitivity, specificity and mean threshold values
var Metrics = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(getMetrics);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;exporting-model-outputs&#34;&gt;Exporting model outputs&lt;/h3&gt;
&lt;p&gt;We ran the model in batch mode, exporting the results to Google Drive.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 7 - Export outputs
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Export final outputs to Google Drive

Export.image.toDrive({
  image: ModelAverage, //Object to export
  description: &#39;HSI&#39;, //Name of the file
  scale: GrainSize, //Spatial resolution of the exported raster
  maxPixels: 1e10,
  region: AOI //Area of interest
});

Export.table.toDrive({
  collection: ee.FeatureCollection(AUCPRs
                        .map(function(element){
                        return ee.Feature(null,{AUCPR:element})})),
  description: &#39;AUCPR&#39;,
  fileFormat: &#39;CSV&#39;,
});

Export.table.toDrive({
  collection: ee.FeatureCollection(Metrics),
  description: &#39;Metrics&#39;,
  fileFormat: &#39;CSV&#39;,
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because exporting results for this analysis takes a long time, instead
of directly exporting the binary presence-absence map, we manually
calculated the average threshold from the exported accuracy metrics and
set the threshold on the visualization parameters in QGIS.&lt;/p&gt;
&lt;p&gt;This is the resulting figure we produced for the publication.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&#34;Figures/Fig18.jpg&#34; style=&#34;width:90.0%&#34;
alt=&#34;Figure 18. Habitat suitability prediction for Hylocichla mustelina across the eastern continental USA (4,606,284 km2) at 90 m spatial resolution. This analysis used a combination of Landsat 8 surface reflectance collection 2 and Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band Synthetic Aperture Radar (SAR) mosaics and averaged temperature datasets. The zoom-in boxes show details of habitat suitability predictions at 90 m spatial resolution. Red dots represent 2,000 presence locations of Hylocichla mustelina, randomly selected from the 34,880 observations used for modelling. The potential distribution is also shown in the bottom left corner of the figure.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 18. Habitat suitability prediction for &lt;em&gt;Hylocichla mustelina&lt;em&gt; across the eastern continental USA (4,606,284 km2) at 90 m spatial resolution. This analysis used a combination of Landsat 8 surface reflectance collection 2 and Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band Synthetic Aperture Radar (SAR) mosaics and averaged temperature datasets. The zoom-in boxes show details of habitat suitability predictions at 90 m spatial resolution. Red dots represent 2,000 presence locations of &lt;em&gt;Hylocichla mustelina&lt;em&gt;, randomly selected from the 34,880 observations used for modelling. The potential distribution is also shown in the bottom left corner of the figure.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h2 id=&#34;code-to-model-presence-absence-data&#34;&gt;Code to model presence-absence data&lt;/h2&gt;
&lt;p&gt;Using presence and absence data is always recommended for SDM analysis.&lt;/p&gt;
&lt;p&gt;Here we present code that will allow you to fit an SDM using presence
and absence data. Because there is no need to create pseudo-absences,
the code has some modifications from the previous examples. However, the
work flow is very similar.&lt;/p&gt;
&lt;p&gt;It is important that the data set that is uploaded into GEE as an asset
contains a field with 1 indicating presence and 0 indicating absence for
each location.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// Presence absence model

///////////////////////////////
// Section 1 - Species data
///////////////////////////////

// Load presence absence data
//var Data = ee.FeatureCollection(&#39;users/yourdata&#39;);
var Data = table
print(&#39;Data size:&#39;, Data.size());

// Define spatial resolution to work with (m)
var GrainSize = 1000;


// Add two maps to the screen.
var left = ui.Map();
var right = ui.Map();
ui.root.clear();
ui.root.add(left);
ui.root.add(right);

// Link maps, so when you drag one map, the other will be moved in sync.
ui.Map.Linker([left, right], &#39;change-bounds&#39;);

// Add presence points to the map
// Visualize presence points on the map
//right.addLayer(Data, {color:&#39;red&#39;}, &#39;Data&#39;, 1);
//left.addLayer(Data, {color:&#39;red&#39;}, &#39;Data&#39;, 1);

////////////////////////////////////////
// Section 2 - Define Area of Interest
////////////////////////////////////////

// Define the AOI
var AOI = Data.geometry().bounds().buffer(10000);

// Add border of study area to the map
var outline = ee.Image().byte().paint({
  featureCollection: AOI, color: 1, width: 3});
right.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);
left.addLayer(outline, {palette: &#39;FF0000&#39;}, &amp;quot;Study Area&amp;quot;);

// Center map to the area of interest
right.centerObject(AOI, 9); //Number indicates the zoom level
left.centerObject(AOI, 9); //Number indicates the zoom level


//////////////////////////////////////////////
// Section 3 - Selecting Predictor Variables
//////////////////////////////////////////////

// Here as an example we are using elevation data.
// Load elevation data from the data catalog and calculate slope, aspect, and a simple hillshade from the terrain Digital Elevation Model.
var Terrain = ee.Algorithms.Terrain(ee.Image(&amp;quot;USGS/SRTMGL1_003&amp;quot;));
var Terrain = Terrain.select([&#39;elevation&#39;,&#39;slope&#39;,&#39;aspect&#39;]); // Select elevation, slope and aspect

// Load NDVI 250 m collection and estimate median value per pixel
var MODIS = ee.ImageCollection(&amp;quot;MODIS/006/MOD13Q1&amp;quot;);
var MedianNDVI = MODIS.filterDate(&#39;2003-01-01&#39;, &#39;2020-12-31&#39;).select([&#39;NDVI&#39;]).median();

// Combine bands into a single image
var predictors = Terrain.addBands(MedianNDVI).clip(AOI);

// Mask ocean from predictor variables
var watermask =  Terrain.select(&#39;elevation&#39;).gt(0); //Create a water mask
var predictors = predictors.updateMask(watermask).clip(AOI);

left.addLayer(predictors.clip(AOI), {bands:[&#39;elevation&#39;], min: 900, max: 1700,  palette: [&#39;000000&#39;,&#39;006600&#39;, &#39;009900&#39;,&#39;33CC00&#39;,&#39;996600&#39;,&#39;CC9900&#39;,&#39;CC9966&#39;,&#39;FFFFFF&#39;,]}, &#39;Elevation (m)&#39;, 0);
left.addLayer(predictors.clip(AOI), {bands:[&#39;slope&#39;], min: 0, max: 45, palette:&#39;white,red&#39;}, &#39;Slope (Degrees)&#39;, 0); 
left.addLayer(predictors.clip(AOI), {bands:[&#39;aspect&#39;], min: 0, max: 350, palette:&#39;red,blue&#39;}, &#39;Aspect (Degrees)&#39;, 0); 

// Estimate correlation among predictor variables

// Extract local covariate values from multiband predictor image at training points
var DataCor = predictors.sample({scale: GrainSize, numPixels: 5000, geometries: true}); //Generate 5000 random points
var PixelVals = predictors.sampleRegions({collection: DataCor, scale: GrainSize, tileScale: 16}); //Extract covariate values

// To check all pairwise correlations we need to map the reduceColumns function across all pairwise combinations of predictors
var CorrAll = predictors.bandNames().map(function(i){
    var tmp1 = predictors.bandNames().map(function(j){
      var tmp2 = PixelVals.reduceColumns({
        reducer: ee.Reducer.spearmansCorrelation(),
        selectors: [i, j]
      });
    return tmp2.get(&#39;correlation&#39;);
    });
    return tmp1;
  });
print(&#39;Variables correlation matrix&#39;,CorrAll);

// Select bands for modeling
var bands = [&#39;elevation&#39;,&#39;slope&#39;,&#39;aspect&#39;,&#39;NDVI&#39;];
var predictors = predictors.select(bands);

///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Section 4 - Defining blocks to fold randomly for cross validation
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

// Define a function to create a grid over AOI
function makeGrid(geometry, scale) {
  // pixelLonLat returns an image with each pixel labeled with longitude and
  // latitude values.
  var lonLat = ee.Image.pixelLonLat();
  // Select the longitude and latitude bands, multiply by a large number then
  // truncate them to integers.
  var lonGrid = lonLat
    .select(&#39;longitude&#39;)
    .multiply(100000)
    .toInt();
  var latGrid = lonLat
    .select(&#39;latitude&#39;)
    .multiply(100000)
    .toInt();
  return lonGrid
    .multiply(latGrid)
    .reduceToVectors({
      geometry: geometry.buffer(100, 1000), //Buffer to expand grid and include borders
      scale: scale,
      geometryType: &#39;polygon&#39;,
    });
}
// Create grid and remove cells outside AOI
var Scale = 5000; // Set range in m to create spatial blocks
var grid = makeGrid(AOI, Scale);
var Grid = watermask.reduceRegions({collection: grid, reducer: ee.Reducer.mean()}).filter(ee.Filter.neq(&#39;mean&#39;,null));
right.addLayer(Grid, {},&#39;Grid for spatail block cross validation&#39;, 0);



//////////////////////////////////////
// Section 5 - Fitting SDM models
//////////////////////////////////////

// Define function to generate a vector of random numbers between 1 and 1000
function runif(length) {
    return Array.apply(null, Array(length)).map(function() {
        return Math.round(Math.random() * (1000 - 1) + 1);
    });
}

// Define SDM function
// Activate the desired classifier, Random Forest or Gradient Boosting. 
// Note that other algorithms are available in GEE. See ee.Classifiers on the documentation for more information.

function SDM(x) {
    var Seed = ee.Number(x);
    
    // Randomly split blocks for training and validation
    var GRID = ee.FeatureCollection(Grid).randomColumn({seed:Seed}).sort(&#39;random&#39;);
    var TrainingGrid = GRID.filter(ee.Filter.lt(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TestingGrid = GRID.filter(ee.Filter.gte(&#39;random&#39;, split));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    // Presence
    var PresencePoints = ee.FeatureCollection(Data).filter(ee.Filter.eq(&#39;PresAbs&#39;,1)); //Filter all presence points
    var TrPresencePoints = PresencePoints.filter(ee.Filter.bounds(TrainingGrid));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TePresencePoints = PresencePoints.filter(ee.Filter.bounds(TestingGrid));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    //Absence   
    var AbsPoints = ee.FeatureCollection(Data).filter(ee.Filter.eq(&#39;PresAbs&#39;,0)); //Filter all absence points
    var TrAbsencePoints = AbsPoints.filter(ee.Filter.bounds(TrainingGrid));  // Filter points with &#39;random&#39; property &amp;lt; split percentage
    var TeAbsencePoints = AbsPoints.filter(ee.Filter.bounds(TestingGrid));  // Filter points with &#39;random&#39; property &amp;gt;= split percentage

    // Merge points
    var trainingPartition = TrPresencePoints.merge(TrAbsencePoints);
    var testingPartition = TePresencePoints.merge(TeAbsencePoints);
    
    // Extract local covariate values from multiband predictor image at training points
    var trainPixelVals = predictors.sampleRegions({collection: trainingPartition, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16});

    // Classify using random forest 
    var Classifier = ee.Classifier.smileRandomForest({
       numberOfTrees: 500, //The number of decision trees to create.
       variablesPerSplit: null, //The number of variables per split. If unspecified, uses the square root of the number of variables.
       minLeafPopulation: 10,//Only create nodes whose training set contains at least this many points. Integer, default: 1
       bagFraction: 0.5,//The fraction of input to bag per tree. Default: 0.5.
       maxNodes: null,//The maximum number of leaf nodes in each tree. If unspecified, defaults to no limit.
       seed: Seed//The randomization seed.
      });
    
    // Classify using gradient boosting 
    // var ClassifierPr = ee.Classifier.smileGradientTreeBoost({
    //   numberOfTrees:500, //The number of decision trees to create.
    //   shrinkage: 0.005, //The shrinkage parameter in (0, 1) controls the learning rate of procedure. Default: 0.005
    //   samplingRate: 0.7, //The sampling rate for stochastic tree boosting. Default 0.07
    //   maxNodes: null, //The maximum number of leaf nodes in each tree. If unspecified, defaults to no limit.
    //   loss: &amp;quot;LeastAbsoluteDeviation&amp;quot;, //Loss function for regression. One of: LeastSquares, LeastAbsoluteDeviation, Huber.
    //   seed:Seed //The randomization seed.
    // });
  
    // Presence probability 
    var ClassifierPr = Classifier.setOutputMode(&#39;PROBABILITY&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    var ClassifiedImgPr = predictors.select(bands).classify(ClassifierPr);
    
    // Binary presence/absence map
    var ClassifierBin = Classifier.setOutputMode(&#39;CLASSIFICATION&#39;).train(trainPixelVals, &#39;PresAbs&#39;, bands); 
    var ClassifiedImgBin = predictors.select(bands).classify(ClassifierBin);
   
    return ee.List([ClassifiedImgPr, ClassifiedImgBin, trainingPartition, testingPartition]);
  
}


// Define partition for training and testing data
var split = 0.70;  // The proportion of the blocks used to select training data

// Define number of repetitions
var numiter = 10;

// Fit SDM 
// Create random seeds
var RanSeeds = runif(numiter);
var results = ee.List(RanSeeds).map(SDM);
// Extract results from list
var results = results.flatten();
//print(results); //Activate this line to visualize all elements

////////////////////////////////////////////////////////////////////
// Section 6 - Extracting and displaying model prediction results
////////////////////////////////////////////////////////////////////

// Habitat suitability

// Set visualization parameters
var visParams = {
  min: 0,
  max: 0.8,
  palette: [&amp;quot;#440154FF&amp;quot;,&amp;quot;#482677FF&amp;quot;,&amp;quot;#404788FF&amp;quot;,&amp;quot;#33638DFF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,
  &amp;quot;#1F968BFF&amp;quot;,&amp;quot;#29AF7FFF&amp;quot;,&amp;quot;#55C667FF&amp;quot;,&amp;quot;#95D840FF&amp;quot;,&amp;quot;#DCE319FF&amp;quot;],
};

// Extract all model predictions
var images = ee.List.sequence(0,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// You can add all the individual model predictions to the map. The number of layers to add will depend on how many iterations you selected.

// left.addLayer(ee.Image(images.get(0)), visParams, &#39;Run1&#39;);
// left.addLayer(ee.Image(image.get(1)), visParams, &#39;Run2&#39;);

// Calculate mean of all individual model runs
var ModelAverage = ee.ImageCollection.fromImages(images).mean();

// Add final habitat suitability layer and presence locations to the map
left.addLayer(ModelAverage, visParams, &#39;Habitat Suitability&#39;);
left.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);

// Create legend for habitat suitability map.
var legend = ui.Panel({style: {position: &#39;bottom-left&#39;, padding: &#39;8px 15px&#39;}});

legend.add(ui.Label({
  value: &amp;quot;Habitat suitability&amp;quot;,
  style: {fontWeight: &#39;bold&#39;, fontSize: &#39;18px&#39;, margin: &#39;0 0 4px 0&#39;, padding: &#39;0px&#39;}
}));

var colors = [&amp;quot;#DCE319FF&amp;quot;,&amp;quot;#287D8EFF&amp;quot;,&amp;quot;#440154FF&amp;quot;];
var names = [&#39;High&#39;, &#39;Medium&#39;,&#39;Low&#39;];
var entry;
for (var x = 0; x&amp;lt;3; x++){
  entry = [
    ui.Label({style:{color:colors[x],margin: &#39;0 0 4px 0&#39;}, value:&#39;██&#39;}),
    ui.Label({value: names[x],style: {margin: &#39;0 0 4px 4px&#39;}})
  ];
  legend.add(ui.Panel(entry, ui.Panel.Layout.Flow(&#39;horizontal&#39;)));
}

legend.add(ui.Panel(
  [ui.Label({value: &amp;quot;Presence locations&amp;quot;,style: {fontWeight: &#39;bold&#39;, fontSize: &#39;16px&#39;, margin: &#39;0 0 4px 0&#39;}}),
   ui.Label({style:{color:&amp;quot;red&amp;quot;,margin: &#39;0 0 0 4px&#39;}, value:&#39;◉&#39;})],
  ui.Panel.Layout.Flow(&#39;horizontal&#39;)));

left.add(legend);


// Distribution map

// Extract all model predictions
var images2 = ee.List.sequence(1,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// Calculate mean of all indivudual model runs
var DistributionMap = ee.ImageCollection.fromImages(images2).mode();

// Add final distribution map and presence locations to the map
right.addLayer(DistributionMap, 
  {palette: [&amp;quot;white&amp;quot;, &amp;quot;green&amp;quot;], min: 0, max: 1}, 
  &#39;Potential distribution&#39;);
right.addLayer(Data, {color:&#39;red&#39;}, &#39;Presence&#39;, 1);

// Create legend for distribution map
var legend2 = ui.Panel({style: {position: &#39;bottom-left&#39;,padding: &#39;8px 15px&#39;}});
legend2.add(ui.Label({
  value: &amp;quot;Potential distribution map&amp;quot;,
  style: {fontWeight: &#39;bold&#39;,fontSize: &#39;18px&#39;,margin: &#39;0 0 4px 0&#39;,padding: &#39;0px&#39;}
}));

var colors2 = [&amp;quot;green&amp;quot;,&amp;quot;white&amp;quot;];
var names2 = [&#39;Presence&#39;, &#39;Absence&#39;];
var entry2;
for (var x = 0; x&amp;lt;2; x++){
  entry2 = [
    ui.Label({style:{color:colors2[x],margin: &#39;0 0 4px 0&#39;}, value:&#39;██&#39;}),
    ui.Label({value: names2[x],style: {margin: &#39;0 0 4px 4px&#39;}})
  ];
  legend2.add(ui.Panel(entry2, ui.Panel.Layout.Flow(&#39;horizontal&#39;)));
}

legend2.add(ui.Panel(
  [ui.Label({value: &amp;quot;Presence locations&amp;quot;,style: {fontWeight: &#39;bold&#39;, fontSize: &#39;16px&#39;, margin: &#39;0 0 4px 0&#39;}}),
   ui.Label({style:{color:&amp;quot;red&amp;quot;,margin: &#39;0 0 4px 4px&#39;}, value:&#39;◉&#39;})],
  ui.Panel.Layout.Flow(&#39;horizontal&#39;)));

right.add(legend2);

/////////////////////////////////////////
// Section 7 - Accuracy assessment
/////////////////////////////////////////

// Extract testing/validation datasets
var TestingDatasets = ee.List.sequence(3,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
                      return results.get(x)});

// Double check that you have a satisfactory number of points for model validation
print(&#39;Number of presence and absence points for model validation&#39;, ee.List.sequence(0,ee.Number(numiter).subtract(1),1)
.map(function(x){
  return ee.List([ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,1)).size(),
         ee.FeatureCollection(TestingDatasets.get(x)).filter(ee.Filter.eq(&#39;PresAbs&#39;,0)).size()]);
})
);

// Define functions to estimate, sensitivity, specificity and precision.
function getAcc(img,TP){
  var Pr_Prob_Vals = img.sampleRegions({collection: TP, properties: [&#39;PresAbs&#39;], scale: GrainSize, tileScale: 16});
  var seq = ee.List.sequence({start: 0, end: 1, count: 25});
  return ee.FeatureCollection(seq.map(function(cutoff) {
  var Pres = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,1);
  // true-positive and true-positive rate, sensitivity  
  var TP =  ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var TPR = TP.divide(Pres.size());
  var Abs = Pr_Prob_Vals.filterMetadata(&#39;PresAbs&#39;,&#39;equals&#39;,0);
  // false-negative
  var FN = ee.Number(Pres.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  // true-negative and true-negative rate, specificity  
  var TN = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;less_than&#39;,cutoff).size());
  var TNR = TN.divide(Abs.size());
  // false-positive and false-positive rate
  var FP = ee.Number(Abs.filterMetadata(&#39;classification&#39;,&#39;greater_than&#39;,cutoff).size());
  var FPR = FP.divide(Abs.size());
  // precision
  var Precision = TP.divide(TP.add(FP));
  // sum of sensitivity and specificity
  var SUMSS = TPR.add(TNR);
  return ee.Feature(null,{cutoff: cutoff, TP:TP, TN:TN, FP:FP, FN:FN, TPR:TPR, TNR:TNR, FPR:FPR, Precision:Precision, SUMSS:SUMSS});
  }));
}

// Calculate AUC of the Receiver Operator Characteristic
function getAUCROC(x){
  var X = ee.Array(x.aggregate_array(&#39;FPR&#39;));
  var Y = ee.Array(x.aggregate_array(&#39;TPR&#39;)); 
  var X1 = X.slice(0,1).subtract(X.slice(0,0,-1));
  var Y1 = Y.slice(0,1).add(Y.slice(0,0,-1));
  return X1.multiply(Y1).multiply(0.5).reduce(&#39;sum&#39;,[0]).abs().toList().get(0);
}

function AUCROCaccuracy(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return getAUCROC(Acc);
}


var AUCROCs = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(AUCROCaccuracy);
print(&#39;AUC-ROC:&#39;, AUCROCs);
print(&#39;Mean AUC-ROC&#39;, AUCROCs.reduce(ee.Reducer.mean()));

/////////////////////////////////////////////////////////////////////////////////
// Section 8 - Create a custom binary distribution map based on best threshold
/////////////////////////////////////////////////////////////////////////////////

// Calculating the potential distribution map based on the threshold 
// that maximizes the sum of sensitivity and specificity is computationally intensive and 
// may need to be executed in batch mode for a large number of iterations.
// In batch mode, the final image needs to exported to Google Drive and opened in 
// another software for visualization (or imported to GEE as an asset for visualization.

// Function to extract threshold values
function getThreshold(x){
  var HSM = ee.Image(images.get(x));
  var TData = ee.FeatureCollection(TestingDatasets.get(x));
  var Acc = getAcc(HSM, TData);
  return Acc.sort({property:&#39;SUMSS&#39;,ascending:false}).first().get(&amp;quot;cutoff&amp;quot;);
}

// Extract threshold values
var Thresholds = ee.List.sequence(0,ee.Number(numiter).subtract(1),1).map(getThreshold);
var MT = ee.Number(Thresholds.reduce(ee.Reducer.mean()));
print(&#39;Mean threshold:&#39;, MT);

// Transform probability model output into a binary map using the defined threshold and set NA into -9999
var DistributionMap2 = ModelAverage.gte(MT).unmask(-9999);

// Export final model to drive
Export.image.toDrive({
  image: DistributionMap2,
  description: &#39;filename&#39;,
  scale: GrainSize,
  maxPixels: 1e10,
  region: AOI
});


///////////////////////////////////////
// Section 9 - Export outputs
///////////////////////////////////////

// Export final outputs to Google Drive
/*
Export.image.toDrive({
  image: DistributionMap, //Object to export
  description: &#39;PotentialDistribution&#39;, //Name of the file
  scale: GrainSize, //Spatial resolution of the exported raster
  maxPixels: 1e10,
  region: AOI //Area of interest
});

Export.image.toDrive({
  image: ModelAverage, //Object to export
  description: &#39;HSI&#39;, //Name of the file
  scale: GrainSize, //Spatial resolution of the exported raster
  maxPixels: 1e10,
  region: AOI //Area of interest
});

// Export training and validation data sets

// Extract training datasets
var TrainingDatasets = ee.List.sequence(1,ee.Number(numiter).multiply(4).subtract(1),4).map(function(x){
  return results.get(x)});

// If you are interested in exporting any of the training or testing data sets used for modelling,
// you need to extract the feature collections from the list and export them.
// Here an example to export the trainign and validation data sets from the first iteration. 
// For other iterations you need to change the number in the get function. In JavaScript the first element of the list is 0.

Export.table.toDrive({
  collectio : TrainingDatasets.get(0),
  description: &#39;TestingDataRun1&#39;,
  fileFormat: &#39;CSV&#39;,
});

Export.table.toDrive({
  collectio : TestingDatasets.get(0),
  description: &#39;TestingDataRun1&#39;,
  fileFormat: &#39;CSV&#39;,
});
*/
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Araújo, M. B., Anderson, R. P., Barbosa, A. M., Beale, C. M., Dormann,
C. F., Early, R., Garcia, R. A., Guisan, A., Maioran, L., Naimi, B.,
O’Hara, R. B., Zimmermann, N. E., &amp;amp; Rahbek, C. (2019). Standards for
distribution models in biodiversity assessments. Science Advances, 5(1),
1–12. &lt;a href=&#34;https://doi.org/10.1126/sciadv.aat4858&#34;&gt;https://doi.org/10.1126/sciadv.aat4858&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barbet-Massin, M., Jiguet, F., Albert, C. H., &amp;amp; Thuiller, W. (2012).
Selecting pseudo-absences for species distribution models: How, where
and how many? Methods in Ecology and Evolution, 3(2), 327–338.
&lt;a href=&#34;https://doi.org/10.1111/j.2041-210X.2011.00172.x&#34;&gt;https://doi.org/10.1111/j.2041-210X.2011.00172.x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chefaoui, R. M., &amp;amp; Lobo, J. M. (2008). Assessing the effects of
pseudo-absences on predictive distribution model performance. Ecological
Modelling, 210(4), 478–486.
&lt;a href=&#34;https://doi.org/10.1016/j.ecolmodel.2007.08.010&#34;&gt;https://doi.org/10.1016/j.ecolmodel.2007.08.010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evans, J. S., Murphy, M. A., Holden, Z. A., &amp;amp; Cushman, S. A. (2011).
Modeling Species Distribution and Change Using Random Forest. In C. A.
Drew, Y. F. Wiersma, &amp;amp; F. Huettmann (Eds.), Predictive Species and
Habitat Modeling in Landscape Ecology: Concepts and Applications
(pp. 139–159). New York, NY: Springer New York.
&lt;a href=&#34;https://doi.org/10.1007/978-1-4419-7390-0_8&#34;&gt;https://doi.org/10.1007/978-1-4419-7390-0_8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Farr, T. G., Rosen, P. A., Caro, E., Crippen, R., Duren, R., Hensley,
S., Kobrick, M., Paller, M., Rodriguez, E., Roth, L., Seal, D., Shaffer,
S., Shimada, J., Umland, J., Werner, M., Oskin, M., Burbank, D.,
&amp;amp;Alsdorf, D. (2007). The shuttle radar topography mission. Reviews of
Geophysics, 45(2), RG2004. &lt;a href=&#34;https://doi.org/10.1029/2005RG000183&#34;&gt;https://doi.org/10.1029/2005RG000183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fielding, A. H., &amp;amp; Bell, J. F. (1997). A review of methods for the
assessment of prediction errors in conservation presence/absence models.
Environmental Conservation, 24(1), 38–49.
&lt;a href=&#34;https://doi.org/10.1017/S0376892997000088&#34;&gt;https://doi.org/10.1017/S0376892997000088&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Guisan, A., Thuiller, W., &amp;amp; Zimmermann, N. (2017). Habitat Suitability
and Distribution Models: With Applications in R. Cambridge, UK.:
Cambridge University Press. &lt;a href=&#34;https://doi.org/doi:10.1017/9781139028271&#34;&gt;https://doi.org/doi:10.1017/9781139028271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hijmans, R.J., Cameron, S.E., Parra, J.L., Jones, P.G., &amp;amp; Jarvis, A.
(2005). Very High Resolution Interpolated Climate Surfaces for Global
Land Areas. International Journal of Climatology 25: 1965-1978.&lt;/p&gt;
&lt;p&gt;Hijmans, R. J., Phillips, S., Leathwick, J., &amp;amp; Elith, J. (2017). dismo:
Species distribution modeling. R package version 1.1-4.&lt;/p&gt;
&lt;p&gt;Kindt, R. (2018). Ensemble species distribution modelling with
transformed suitability values. Environmental Modelling and Software,
100, 136–145. &lt;a href=&#34;https://doi.org/10.1016/j.envsoft.2017.11.009&#34;&gt;https://doi.org/10.1016/j.envsoft.2017.11.009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Leroy, B., Delsol, R., Hugueny, B., Meynard, C. N., Barhoumi, C.,
Barbet-Massin, M., &amp;amp; Bellard, C. (2018). Without quality
presence–absence data, discrimination metrics such as TSS can be
misleading measures of model performance. Journal of Biogeography,
45(9), 1994–2002. &lt;a href=&#34;https://doi.org/10.1111/jbi.13402&#34;&gt;https://doi.org/10.1111/jbi.13402&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, C., Newell, G., &amp;amp; White, M. (2016). On the selection of thresholds
for predicting species occurrence with presence-only data. Ecology and
Evolution, 6(1), 337–348. &lt;a href=&#34;https://doi.org/10.1002/ece3.1878&#34;&gt;https://doi.org/10.1002/ece3.1878&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marmion, M., Parviainen, M., Luoto, M., Heikkinen, R. K., &amp;amp; Thuiller, W.
(2009). Evaluation of consensus methods in predictive species
distribution modelling. Diversity and Distributions, 15(1), 59–69.
&lt;a href=&#34;https://doi.org/10.1111/j.1472-4642.2008.00491.x&#34;&gt;https://doi.org/10.1111/j.1472-4642.2008.00491.x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mittermeier, R. A., Rylands, A. B., &amp;amp; Wilson, D. E., (Eds.). (2013).
Handbook of the Mammals of the World: Volume 3, Primates. Barcelona,
Spain.: Linx Ediciones.&lt;/p&gt;
&lt;p&gt;Phillips, S. J., Anderson, R. P., Dudík, M., Schapire, R. E., &amp;amp; Blair,
M. E. (2017). Opening the black box: an open-source release of Maxent.
Ecography, 40(7), 887–893. &lt;a href=&#34;https://doi.org/10.1111/ecog.03049&#34;&gt;https://doi.org/10.1111/ecog.03049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Phillips, S. J., Anderson, R. P., &amp;amp; Schapire, R. E. (2006). Maximum
entropy modeling of species geographic distributions. Ecological
Modelling, 190(3–4), 231–259.
&lt;a href=&#34;https://doi.org/10.1016/j.ecolmodel.2005.03.026&#34;&gt;https://doi.org/10.1016/j.ecolmodel.2005.03.026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Phillips, S. J., Dudík, M., Elith, J., Graham, C. H., Lehmann, A.,
Leathwick, J., &amp;amp; Ferrier, S. (2009). Sample selection bias and
presence-only distribution models: Implications for background and
pseudo-absence data. Ecological Applications, 19(1), 181–197.
&lt;a href=&#34;https://doi.org/10.1890/07-2153.1&#34;&gt;https://doi.org/10.1890/07-2153.1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Roberts, D. R., Bahn, V., Ciuti, S., Boyce, M. S., Elith, J.,
Guillera-Arroita, G., Hauenstein, S., Lahoz-Monford, J. J., Schröder,
B., Thuiller, W., Warton, D. I., Wintle, B. A., Hartig, F., &amp;amp; Dormann,
C. F. (2017). Cross-validation strategies for data with temporal,
spatial, hierarchical, or phylogenetic structure. Ecography, 40(8),
913–929. &lt;a href=&#34;https://doi.org/10.1111/ecog.02881&#34;&gt;https://doi.org/10.1111/ecog.02881&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Senay, S. D., Worner, S. P., &amp;amp; Ikeda, T. (2013). Novel Three-Step
Pseudo-Absence Selection Technique for Improved Species Distribution
Modelling. PLoS ONE, 8(8).
&lt;a href=&#34;https://doi.org/10.1371/journal.pone.0071218&#34;&gt;https://doi.org/10.1371/journal.pone.0071218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sillero, N., Arenas-Castro, S., Enriquez‐Urzelai, U., Vale, C. G.,
Sousa-Guedes, D., Martínez-Freiría, F., … Barbosa, A. M. (2021). Want to
model a species niche? A step-by-step guideline on correlative
ecological niche modelling. Ecological Modelling, 456.
&lt;a href=&#34;https://doi.org/10.1016/j.ecolmodel.2021.109671&#34;&gt;https://doi.org/10.1016/j.ecolmodel.2021.109671&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sofaer, H. R., Hoeting, J. A., &amp;amp; Jarnevich, C. S. (2019). The area under
the precision-recall curve as a performance metric for rare binary
events. Methods in Ecology and Evolution, 10(4), 565–577.
&lt;a href=&#34;https://doi.org/10.1111/2041-210X.13140&#34;&gt;https://doi.org/10.1111/2041-210X.13140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Valavi, R., Elith, J., Lahoz-Monfort, J. J., &amp;amp; Guillera-Arroita, G.
(2019). blockCV: An r package for generating spatially or
environmentally separated folds for k-fold cross-validation of species
distribution models. Methods in Ecology and Evolution, 10(2), 225–232.
&lt;a href=&#34;https://doi.org/10.1111/2041-210X.13107&#34;&gt;https://doi.org/10.1111/2041-210X.13107&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enhancing animal movement analyses - Spatiotemporal matching of animal positions with remotely sensed data using Google Earth Engine and R</title>
      <link>//localhost:4321/teaching/rgee/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>//localhost:4321/teaching/rgee/</guid>
      <description>&lt;p&gt;Ramiro D. Crego &lt;sup&gt;a,b&lt;/sup&gt;, Majaliwa M. Masolele &lt;sup&gt;c&lt;/sup&gt;, Grant Connette &lt;sup&gt;a,b&lt;/sup&gt;, and Jared A. Stabach &lt;sup&gt;a&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;a - Smithsonian National Zoo and Conservation Biology Institute,
Conservation Ecology Center, 1500 Remount Rd, Front Royal, VA 22630,
USA.&lt;/p&gt;
&lt;p&gt;b - Working Land and Seascapes, Conservation Commons, Smithsonian
Institution, Washington, DC 20013, USA.&lt;/p&gt;
&lt;p&gt;c - Boyd Orr Centre for Population and Ecosystem Health, Institute of
Biodiversity, Animal Health &amp;amp; Comparative Medicine (IBAHCM), University
of Glasgow, G12 8QQ, UK.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following tutorial describes the code workflow presented in the
manuscript “Enhancing animal movement analyses - Spatiotemporal matching
of animal positions with remotely sensed data using Google Earth Engine
and R.”&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/20/4154&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;./Figures/23.png&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;
&lt;p&gt;The code workflow allows you to find the closest image from an image
collection to the time at which each GPS location was acquired and
extract the pixel value.&lt;/p&gt;
&lt;p&gt;To use this code it is necessary to have a &lt;a href=&#34;https://earthengine.google.com/&#34;&gt;Google Earth
Engine&lt;/a&gt; account and to install the
&lt;code&gt;rgee&lt;/code&gt;
&lt;a href=&#34;https://r-spatial.github.io/rgee/#quick-start-users-guide-for-rgee&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;loading-and-preparing-tracking-data&#34;&gt;Loading and preparing tracking data&lt;/h2&gt;
&lt;p&gt;The first step is to read a csv file with the telemetry data.&lt;/p&gt;
&lt;p&gt;For this example we randomly selected 100 GPS fixes from the entire
wildebeest dataset used in the manuscript, obtained from
&lt;a href=&#34;https://www.datarepository.movebank.org/handle/10255/move.1098&#34;&gt;Movebank&lt;/a&gt;.
The data is provided in the repository within the folder Data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(sf)
library(dplyr)
trackingdata &amp;lt;- read.csv(&amp;quot;./Data/Data.csv&amp;quot;, header = T) #Load your dataset
head(trackingdata)

##             timestamp location.long location.lat
## 1 2010-08-10 15:00:00      35.21622    -1.164488
## 2 2011-06-26 12:00:00      36.83339    -1.462866
## 3 2011-10-19 03:00:00      36.90053    -1.404862
## 4 2011-11-20 12:00:00      36.91039    -1.430901
## 5 2010-08-29 14:00:00      35.30668    -1.345507
## 6 2010-12-20 05:00:00      36.91189    -1.433012
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next step, we need to convert the dataframe into an sf object. We
also need to set the date as a string with a ‘YYYY-MM-DDTHH:MM:SS’. We
will use this data to convert it into milliseconds since midnight on
January 1, 1970, a format used in Google Earth Engine to manage dates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;trackingdata$Date &amp;lt;- as.POSIXct(trackingdata$timestamp, format = &amp;quot;%Y-%m-%d %H:%M:%S&amp;quot;, tz=&amp;quot;UTC&amp;quot;) #Modify as necessary
trackingdata$Date &amp;lt;- as.factor(trackingdata$Date)
trackingdata$Date &amp;lt;- sub(&amp;quot; &amp;quot;, &amp;quot;T&amp;quot;, trackingdata$Date) #Put in a format that can be read by javascript
trackingdata$ID &amp;lt;- seq(1:nrow(trackingdata)) # Add ID to each point (optional)
datasf &amp;lt;- st_as_sf(trackingdata, coords = c(&#39;location.long&#39;,&#39;location.lat&#39;), crs = 4326) #Transform the dataframe into sf object. Make sure the name of the columns for the coordinates match. CRS needs to be in longlat WGS84.
head(datasf)

## Simple feature collection with 6 features and 3 fields
## Geometry type: POINT
## Dimension:     XY
## Bounding box:  xmin: 35.21622 ymin: -1.462866 xmax: 36.91189 ymax: -1.164488
## Geodetic CRS:  WGS 84
##             timestamp                Date ID
## 1 2010-08-10 15:00:00 2010-08-10T15:00:00  1
## 2 2011-06-26 12:00:00 2011-06-26T12:00:00  2
## 3 2011-10-19 03:00:00 2011-10-19T03:00:00  3
## 4 2011-11-20 12:00:00 2011-11-20T12:00:00  4
## 5 2010-08-29 14:00:00 2010-08-29T14:00:00  5
## 6 2010-12-20 05:00:00 2010-12-20T05:00:00  6
##                     geometry
## 1 POINT (35.21622 -1.164488)
## 2 POINT (36.83339 -1.462866)
## 3 POINT (36.90053 -1.404862)
## 4 POINT (36.91039 -1.430901)
## 5 POINT (35.30668 -1.345507)
## 6 POINT (36.91189 -1.433012)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;initialize-rgee&#34;&gt;Initialize rgee&lt;/h2&gt;
&lt;p&gt;Next, we need to load and initialize rgee.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(rgee)
ee_Initialize()
ee_check()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;define-google-earth-engine-functions&#34;&gt;Define Google Earth Engine functions&lt;/h2&gt;
&lt;p&gt;The next set of functions will be used to match images to the data and
extract pixel values.&lt;/p&gt;
&lt;p&gt;Note that you can edit the maximum temporal window allowed to find a
match.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Function to add property with time in milliseconds
add_date&amp;lt;-function(feature) {
  date &amp;lt;- ee$Date(ee$String(feature$get(&amp;quot;Date&amp;quot;)))$millis()
  feature$set(list(date_millis=date))
}

#Join Image and Points based on a maxDifference Filter within a temporal window

#Set temporal window in days for filter. This will depend on the remote sensing data used.
tempwin &amp;lt;- 16 

#Set the filter
maxDiffFilter&amp;lt;-ee$Filter$maxDifference(
  difference=tempwin*24*60*60*1000, #days * hr * min * sec * milliseconds
  leftField= &amp;quot;date_millis&amp;quot;, #Timestamp of the telemetry data
  rightField=&amp;quot;system:time_start&amp;quot; #Image date
)

# Define the join. We implement the saveBest function for the join, which finds the image that best matches the filter (i.e., the image closest in time to the particular GPS fix location). 
saveBestJoin&amp;lt;-ee$Join$saveBest(
  matchKey=&amp;quot;bestImage&amp;quot;,
  measureKey=&amp;quot;timeDiff&amp;quot;
)

#Function to add property with raster pixel value from the matched image
add_value&amp;lt;-function(feature){
  #Get the image selected by the join
  img1&amp;lt;-ee$Image(feature$get(&amp;quot;bestImage&amp;quot;))$select(band)
  #Extract geometry from the feature
  point&amp;lt;-feature$geometry()
  #Get pixel value for each point at the desired spatial resolution (argument scale)
  pixel_value&amp;lt;-img1$sample(region=point, scale=250, tileScale = 16, dropNulls = F) 
  #Return the data containing pixel value and image date.
  feature$setMulti(list(PixelVal = pixel_value$first()$get(band), DateTimeImage = img1$get(&#39;system:index&#39;)))
}

# Function to remove image property from features
removeProperty&amp;lt;- function(feature) {
  #Get the properties of the data
  properties = feature$propertyNames()
  #Select all items except images
  selectProperties = properties$filter(ee$Filter$neq(&amp;quot;item&amp;quot;, &amp;quot;bestImage&amp;quot;))
  #Return selected features
  feature$select(selectProperties)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;load-image-collection&#34;&gt;Load image collection&lt;/h2&gt;
&lt;p&gt;In this example, we are using NDVI from MODIS Terra Vegetation Indexes
16-Day Global 250m data set. However, you can use any other remote
sensing product of interest and filter to the desired dates.&lt;/p&gt;
&lt;p&gt;One of the main advantages offered by Google Earth Engine is the
enormous amount of data available to be used. The constantly growing
database consists on more than a petabyte archive of publicly available
remotely sensed imagery and other related data sets. In the study cases
of the manuscript we used MOD13Q1 and ERA5_LAND/HOURLY, but data
available includes other products from MODIS, data from other satellites
such as Landsat, National Oceanographic and Atmospheric Administration
Advanced Very High Resolution Radiometer (NOAA AVHRR), Sentinel 1, 2, 3
and 5-P, Advanced Land Observing Satellite (ALOS), and other products
such as sea surface temperature data, CHIRPS climate data, topography
data, and land cover data. The entire list of datasets is available at
this
&lt;a href=&#34;https://developers.google.com/earth-engine/datasets/catalog&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that all image collections in Earth Engine have a code that you can
extract from the link provided above and use to import into the workflow
by modifying this example.&lt;/p&gt;
&lt;p&gt;We will set the start and end days to filter the image collections.
Temporal availability depends on each dataset.&lt;/p&gt;
&lt;p&gt;We will also create an object with the name of the band we are
interested in working with. The name of the band is also specific to
each image collection.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;start&amp;lt;-&amp;quot;2010-01-01&amp;quot;
end&amp;lt;-&amp;quot;2013-01-01&amp;quot;
imagecoll&amp;lt;-ee$ImageCollection(&#39;MODIS/006/MOD13Q1&#39;)$filterDate(start,end)
band &amp;lt;- &amp;quot;NDVI&amp;quot; #Name of the band to use. You can change to EVI for instance when using MOD13Q1.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;extract-pixel-value&#34;&gt;Extract pixel value&lt;/h2&gt;
&lt;p&gt;A key function in this process is the &lt;code&gt;ee_as_sf&lt;/code&gt; which converts the
Google Earth Engine table in a sf object. This function provides three
different options to convert the table (feature collection) into a sf
object:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;getInfo: which is fast and direct but has a limit of 5000 features&lt;/li&gt;
&lt;li&gt;drive: which exports data through your Google Drive account&lt;/li&gt;
&lt;li&gt;gsc: which exports data through your Google Cloud Storage account&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can find more information about this function in the help:
?ee_as_sf&lt;/p&gt;
&lt;p&gt;We use here the &lt;code&gt;getInfo&lt;/code&gt; option given it is direct and simple. However,
this option has a limit of 5000 features to convert. For that reason, we
are going to run a loop, processing 1000 features (points) per time to
avoid errors. If memory limit errors are display, then you can reduce
the number of points to extract each time by changing the &lt;code&gt;each&lt;/code&gt;
argument on the &lt;code&gt;rep&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;In this example we only have 100 points so the loop will only run once,
but for larger datasets the loop may run multiple times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;datasf$uniq &amp;lt;- rep(1:1000, each=1000)[1:nrow(datasf)] #This is for up to 1 million points. To increase the max number of points, increase the value for max repetitions. To change the number of points to run per time, change the value in the argument each (up to 5000).

start_time &amp;lt;- Sys.time()
dataoutput &amp;lt;- data.frame()
for(x in unique(datasf$uniq)){
  data1 &amp;lt;- datasf %&amp;gt;% filter(uniq == x)
  # Send sf to GEE
  data &amp;lt;- sf_as_ee(data1)
  # Transform day into milliseconds
  data&amp;lt;-data$map(add_date)
  # Apply the join
  Data_match&amp;lt;-saveBestJoin$apply(data, imagecoll, maxDiffFilter)
  # Add pixel value to the data
  DataFinal&amp;lt;-Data_match$map(add_value)
  # Remove image property from the data
  DataFinal&amp;lt;-DataFinal$map(removeProperty)
  # Move GEE object into R
  temp&amp;lt;- ee_as_sf(DataFinal, via = &#39;getInfo&#39;)
  # Append
  dataoutput &amp;lt;- rbind(dataoutput, temp)
}
end_time &amp;lt;- Sys.time()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The time needed to run 100 points was:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;end_time - start_time

## Time difference of 3.329701 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new sf data frame with the pixel values in now stored as the
&lt;code&gt;dataoutput&lt;/code&gt; object. You can use this for further analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;names(dataoutput)[4] &amp;lt;- band
dataoutput

## Simple feature collection with 100 features and 7 fields
## Geometry type: POINT
## Dimension:     XY
## Bounding box:  xmin: 34.99598 ymin: -2.693471 xmax: 37.5994 ymax: -1.148282
## Geodetic CRS:  WGS 84
## First 10 features:
##                   Date DateTimeImage ID NDVI date_millis
## 1  2010-08-10 15:00:00    2010_08_13  1 3011  1552145792
## 2  2011-06-26 12:00:00    2011_06_26  2 3632  -875425280
## 3  2011-10-19 03:00:00    2011_10_16  3 2957   438240128
## 4  2011-11-20 12:00:00    2011_11_17  4 4952 -1059527168
## 5  2010-08-29 14:00:00    2010_08_29  5 2699 -1104821504
## 6  2010-12-20 05:00:00    2010_12_19  6 3518    36043904
## 7  2010-07-16 18:01:00    2010_07_12  7 3227  -596994208
## 8  2011-12-09 03:01:00    2011_12_03  8 4460   549732832
## 9  2012-01-01 07:00:00    2012_01_01  9 2624 -1743694464
## 10 2012-04-16 04:00:00    2012_04_22 10 5840 -1186029056
##              timestamp uniq                   geometry
## 1  2010-08-10 15:00:00    1 POINT (35.21622 -1.164488)
## 2  2011-06-26 12:00:00    1 POINT (36.83339 -1.462866)
## 3  2011-10-19 03:00:00    1 POINT (36.90053 -1.404862)
## 4  2011-11-20 12:00:00    1 POINT (36.91039 -1.430901)
## 5  2010-08-29 14:00:00    1 POINT (35.30668 -1.345507)
## 6  2010-12-20 05:00:00    1 POINT (36.91189 -1.433012)
## 7  2010-07-16 18:01:00    1 POINT (35.24742 -1.324172)
## 8  2011-12-09 03:01:00    1   POINT (37.565 -2.446763)
## 9  2012-01-01 07:00:00    1 POINT (37.03885 -2.620116)
## 10 2012-04-16 04:00:00    1 POINT (35.52517 -1.233949)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualize-locations&#34;&gt;Visualize locations&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Pres &amp;lt;- dataoutput

library(tmap)
tmap_mode(&#39;plot&#39;)
tm_shape(Pres) + tm_dots(col = &#39;blue&#39;, title = &amp;quot;Presence&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;tutorial_files/figure-markdown_strict/unnamed-chunk-11-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;session&#34;&gt;Session&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;devtools::session_info()

## ─ Session info ─────────────────────────────────────────────────
##  setting  value
##  version  R version 4.4.1 (2024-06-14)
##  os       macOS Sonoma 14.5
##  system   aarch64, darwin20
##  ui       RStudio
##  language (EN)
##  collate  en_US.UTF-8
##  ctype    en_US.UTF-8
##  tz       Europe/Dublin
##  date     2024-07-20
##  rstudio  2024.04.2+764 Chocolate Cosmos (desktop)
##  pandoc   3.1.11 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/aarch64/ (via rmarkdown)
## 
## ─ Packages ─────────────────────────────────────────────────────
##  package           * version   date (UTC) lib source
##  abind               1.4-5     2016-07-21 [1] CRAN (R 4.4.0)
##  base64enc           0.1-3     2015-07-28 [1] CRAN (R 4.4.0)
##  cachem              1.1.0     2024-05-16 [1] CRAN (R 4.4.0)
##  class               7.3-22    2023-05-03 [1] CRAN (R 4.4.1)
##  classInt            0.4-10    2023-09-05 [1] CRAN (R 4.4.0)
##  cli                 3.6.3     2024-06-21 [1] CRAN (R 4.4.0)
##  codetools           0.2-20    2024-03-31 [1] CRAN (R 4.4.1)
##  crayon              1.5.3     2024-06-20 [1] CRAN (R 4.4.0)
##  crosstalk           1.2.1     2023-11-23 [1] CRAN (R 4.4.0)
##  crul                1.5.0     2024-07-19 [1] CRAN (R 4.4.0)
##  curl                5.2.1     2024-03-01 [1] CRAN (R 4.4.0)
##  DBI                 1.2.3     2024-06-02 [1] CRAN (R 4.4.0)
##  devtools            2.4.5     2022-10-11 [1] CRAN (R 4.4.0)
##  dichromat           2.0-0.1   2022-05-02 [1] CRAN (R 4.4.0)
##  digest              0.6.36    2024-06-23 [1] CRAN (R 4.4.0)
##  dplyr             * 1.1.4     2023-11-17 [1] CRAN (R 4.4.0)
##  e1071               1.7-14    2023-12-06 [1] CRAN (R 4.4.0)
##  ellipsis            0.3.2     2021-04-29 [1] CRAN (R 4.4.0)
##  evaluate            0.24.0    2024-06-10 [1] CRAN (R 4.4.0)
##  fansi               1.0.6     2023-12-08 [1] CRAN (R 4.4.0)
##  fastmap             1.2.0     2024-05-15 [1] CRAN (R 4.4.0)
##  fs                  1.6.4     2024-04-25 [1] CRAN (R 4.4.0)
##  generics            0.1.3     2022-07-05 [1] CRAN (R 4.4.0)
##  geojson             0.3.5     2023-08-08 [1] CRAN (R 4.4.0)
##  geojsonio           0.11.3    2023-09-06 [1] CRAN (R 4.4.0)
##  geojsonsf           2.0.3     2022-05-30 [1] CRAN (R 4.4.0)
##  glue                1.7.0     2024-01-09 [1] CRAN (R 4.4.0)
##  highr               0.11      2024-05-26 [1] CRAN (R 4.4.0)
##  htmltools           0.5.8.1   2024-04-04 [1] CRAN (R 4.4.0)
##  htmlwidgets         1.6.4     2023-12-06 [1] CRAN (R 4.4.0)
##  httpcode            0.3.0     2020-04-10 [1] CRAN (R 4.4.0)
##  httpuv              1.6.15    2024-03-26 [1] CRAN (R 4.4.0)
##  jqr                 1.3.3     2023-12-04 [1] CRAN (R 4.4.0)
##  jquerylib           0.1.4     2021-04-26 [1] CRAN (R 4.4.0)
##  jsonlite            1.8.8     2023-12-04 [1] CRAN (R 4.4.0)
##  KernSmooth          2.23-24   2024-05-17 [1] CRAN (R 4.4.1)
##  knitr               1.48      2024-07-07 [1] CRAN (R 4.4.0)
##  later               1.3.2     2023-12-06 [1] CRAN (R 4.4.0)
##  lattice             0.22-6    2024-03-20 [1] CRAN (R 4.4.1)
##  lazyeval            0.2.2     2019-03-15 [1] CRAN (R 4.4.0)
##  leafem              0.2.3     2023-09-17 [1] CRAN (R 4.4.0)
##  leaflet             2.2.2     2024-03-26 [1] CRAN (R 4.4.0)
##  leaflet.providers   2.0.0     2023-10-17 [1] CRAN (R 4.4.0)
##  leafsync            0.1.0     2019-03-05 [1] CRAN (R 4.4.0)
##  lifecycle           1.0.4     2023-11-07 [1] CRAN (R 4.4.0)
##  lwgeom              0.2-14    2024-02-21 [1] CRAN (R 4.4.0)
##  magrittr            2.0.3     2022-03-30 [1] CRAN (R 4.4.0)
##  Matrix              1.7-0     2024-04-26 [1] CRAN (R 4.4.1)
##  memoise             2.0.1     2021-11-26 [1] CRAN (R 4.4.0)
##  mime                0.12      2021-09-28 [1] CRAN (R 4.4.0)
##  miniUI              0.1.1.1   2018-05-18 [1] CRAN (R 4.4.1)
##  pillar              1.9.0     2023-03-22 [1] CRAN (R 4.4.0)
##  pkgbuild            1.4.4     2024-03-17 [1] CRAN (R 4.4.0)
##  pkgconfig           2.0.3     2019-09-22 [1] CRAN (R 4.4.0)
##  pkgload             1.4.0     2024-06-28 [1] CRAN (R 4.4.0)
##  png                 0.1-8     2022-11-29 [1] CRAN (R 4.4.0)
##  processx            3.8.4     2024-03-16 [1] CRAN (R 4.4.0)
##  profvis             0.3.8     2023-05-02 [1] CRAN (R 4.4.0)
##  promises            1.3.0     2024-04-05 [1] CRAN (R 4.4.0)
##  proxy               0.4-27    2022-06-09 [1] CRAN (R 4.4.0)
##  ps                  1.7.7     2024-07-02 [1] CRAN (R 4.4.0)
##  purrr               1.0.2     2023-08-10 [1] CRAN (R 4.4.0)
##  R6                  2.5.1     2021-08-19 [1] CRAN (R 4.4.0)
##  raster              3.6-26    2023-10-14 [1] CRAN (R 4.4.0)
##  RColorBrewer        1.1-3     2022-04-03 [1] CRAN (R 4.4.0)
##  Rcpp                1.0.12    2024-01-09 [1] CRAN (R 4.4.0)
##  remotes             2.5.0     2024-03-17 [1] CRAN (R 4.4.0)
##  reticulate          1.38.0    2024-06-19 [1] CRAN (R 4.4.0)
##  rgee              * 1.1.7     2023-09-27 [1] CRAN (R 4.4.0)
##  rlang               1.1.4     2024-06-04 [1] CRAN (R 4.4.0)
##  rmarkdown         * 2.27      2024-05-17 [1] CRAN (R 4.4.0)
##  rstudioapi          0.16.0    2024-03-24 [1] CRAN (R 4.4.0)
##  s2                  1.1.7     2024-07-17 [1] CRAN (R 4.4.0)
##  sessioninfo         1.2.2     2021-12-06 [1] CRAN (R 4.4.0)
##  sf                * 1.0-16    2024-03-24 [1] CRAN (R 4.4.0)
##  shiny               1.8.1.1   2024-04-02 [1] CRAN (R 4.4.0)
##  sp                  2.1-4     2024-04-30 [1] CRAN (R 4.4.0)
##  stars               0.6-6     2024-07-16 [1] CRAN (R 4.4.0)
##  stringi             1.8.4     2024-05-06 [1] CRAN (R 4.4.0)
##  stringr             1.5.1     2023-11-14 [1] CRAN (R 4.4.0)
##  terra               1.7-78    2024-05-22 [1] CRAN (R 4.4.0)
##  tibble              3.2.1     2023-03-20 [1] CRAN (R 4.4.0)
##  tidyselect          1.2.1     2024-03-11 [1] CRAN (R 4.4.0)
##  tmap              * 3.3-4     2023-09-12 [1] CRAN (R 4.4.0)
##  tmaptools           3.1-1     2021-01-19 [1] CRAN (R 4.4.0)
##  units               0.8-5     2023-11-28 [1] CRAN (R 4.4.0)
##  urlchecker          1.0.1     2021-11-30 [1] CRAN (R 4.4.0)
##  usethis             2.2.3     2024-02-19 [1] CRAN (R 4.4.0)
##  utf8                1.2.4     2023-10-22 [1] CRAN (R 4.4.0)
##  V8                  4.4.2     2024-02-15 [1] CRAN (R 4.4.0)
##  vctrs               0.6.5     2023-12-01 [1] CRAN (R 4.4.0)
##  viridisLite         0.4.2     2023-05-02 [1] CRAN (R 4.4.0)
##  webshot             0.5.5     2023-06-26 [1] CRAN (R 4.4.0)
##  wk                  0.9.2     2024-07-09 [1] CRAN (R 4.4.0)
##  xfun                0.45      2024-06-16 [1] CRAN (R 4.4.0)
##  XML                 3.99-0.17 2024-06-25 [1] CRAN (R 4.4.0)
##  xtable              1.8-4     2019-04-21 [1] CRAN (R 4.4.0)
##  yaml                2.3.9     2024-07-05 [1] CRAN (R 4.4.0)
## 
##  [1] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library
## 
## ─ Python configuration ─────────────────────────────────────────
##  python:         /Users/ramirocrego/.virtualenvs/rgee/bin/python
##  libpython:      /Users/ramirocrego/Library/r-miniconda-arm64/lib/libpython3.10.dylib
##  pythonhome:     /Users/ramirocrego/.virtualenvs/rgee:/Users/ramirocrego/.virtualenvs/rgee
##  version:        3.10.12 (main, Jul  5 2023, 15:02:25) [Clang 14.0.6 ]
##  numpy:          /Users/ramirocrego/.virtualenvs/rgee/lib/python3.10/site-packages/numpy
##  numpy_version:  1.25.1
##  ee:             /Users/ramirocrego/.virtualenvs/rgee/lib/python3.10/site-packages/ee
##  
##  NOTE: Python version was forced by RETICULATE_PYTHON
## 
## ────────────────────────────────────────────────────────────────
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
